{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8982809,"sourceType":"datasetVersion","datasetId":5409488}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom PIL import Image\nfrom transformers import CLIPProcessor, CLIPModel\n\nclass MisogynyCls(nn.Module):\n    def __init__(self, num_linear_layers, task_a_out=1, task_b_out=4, input_dim=1024, hidden_dim=512, drop_value=0.2):\n        super().__init__()\n        self.head_task_a = nn.Linear(hidden_dim, task_a_out)\n        self.head_task_b = nn.Linear(hidden_dim, task_b_out)\n        self.sigmoid = nn.Sigmoid()\n        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # Check if CUDA is available\n        \n        # Pretrained CLIP loading...\n        self.clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n        self.clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n\n\n        self.layers = nn.ModuleList()\n\n        for i in range(num_linear_layers):\n            if i == 0:\n                self.layers.append(nn.Linear(input_dim, hidden_dim))\n            else:\n                self.layers.append(nn.Linear(hidden_dim, hidden_dim))\n                \n            self.layers.append(nn.BatchNorm1d(hidden_dim))\n            self.layers.append(nn.Dropout(drop_value))\n            self.layers.append(nn.ReLU())\n\n\n    def forward(self, text_list, image_list):\n        clip_inputs = self.clip_processor(text=text_list, images=image_list, return_tensors=\"pt\", padding=True, truncation=True)\n        clip_outputs = self.clip_model(**clip_inputs)\n        \n        x = torch.cat([clip_outputs['text_embeds'], clip_outputs['image_embeds']], dim=1).to(self.device) # model input is the concatenation of the two modalities !\n        \n        for layer in self.layers:\n            x = layer(x)\n            #print(x.shape)\n        pred_taskA = self.sigmoid(self.head_task_a(x))\n        pred_taskB = self.sigmoid(self.head_task_b(x))\n        \n        return pred_taskA, pred_taskB","metadata":{"execution":{"iopub.status.busy":"2024-07-21T18:11:52.357711Z","iopub.execute_input":"2024-07-21T18:11:52.358108Z","iopub.status.idle":"2024-07-21T18:11:52.374476Z","shell.execute_reply.started":"2024-07-21T18:11:52.358080Z","shell.execute_reply":"2024-07-21T18:11:52.373114Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"import shap\nimport torch\nimport numpy as np\nfrom PIL import Image\nimport os, copy, sys\nimport math, json\nimport random\nfrom tqdm import tqdm\nfrom PIL import Image\nfrom torchvision import transforms\nfrom nltk.tokenize import word_tokenize\n\n\n\n\n\nclass MMSHAP:\n    \n    def __init__(self,\n                 classifier):\n        self.classifier = classifier\n        self.img = None\n        self.num_txt_token = None\n        self.patch_size = None\n        \n    def custom_masker(self, mask, x):\n        masked_X = np.copy(x).reshape(1, -1) # fai controllo per vedere se effettivamente ha una shape  e.g. (1, 15)\n        mask = np.expand_dims(mask, axis=0) # same as unsqueeze(0)\n        \n        #print(f'type mask: {type(mask)}')\n        #print(f'shape mask: {mask.shape}')\n        #print(f'shape x: {x.shape}')\n        \n\n        masked_X[~mask] = \"UNK\"\n        return masked_X\n\n    def get_model_prediction(self, x): # x must be an ndarray of strings representing the couple (perturbed_txt, perturbed_img)\n        #print(x)\n        self.classifier.eval()\n        perturbed_imgs = []\n\n        with torch.no_grad():\n            # split up the input_ids and the image_token_ids from x (containing both appended)\n            masked_txt_tokens = [input_string[:self.num_txt_tokens] for input_string in x]\n            masked_image_tokens = [input_string[self.num_txt_tokens:] for input_string in x]\n            perturbed_txts = [' '.join(token_list.tolist()) for token_list in masked_txt_tokens]\n            \n            #print(perturbed_txts)\n\n            result = np.zeros(len(x))\n            row_cols = 224 // self.patch_size # 224 / 32 = 7\n\n            # call the model for each \"new image\" generated with masked features\n            for i in range(len(x)):\n                perturbed_img = copy.deepcopy(self.img)\n\n                # here the actual masking of the image is happening. The custom masker only specified which patches to mask, but no actual masking has happened\n                curr_masked_txt_tokens = copy.deepcopy(masked_txt_tokens[i])\n\n                # PATCHIFY THE IMAGE\n                for k in range(len(masked_image_tokens[i])):\n                    if masked_image_tokens[i][k] == \"UNK\":  # should be the patch we want to mask\n                        m = k // row_cols\n                        n = k % row_cols\n                        perturbed_img[:, m*self.patch_size:(m+1)*self.patch_size, n*self.patch_size:(n+1)*self.patch_size] = 0 # torch.rand(3, patch_size, patch_size)  # np.random.rand()\n\n                perturbed_imgs.append(perturbed_img)\n\n            outputs_taskA, _ = self.classifier(perturbed_txts, perturbed_imgs)\n\n        return outputs_taskA\n\n    \n    def compute_mmscore(self, num_txt_tokens, shap_values):\n        \"\"\" Compute Multimodality Score. (80% textual, 20% visual, possibly: 0% knowledge). \"\"\"\n        print(shap_values.values.shape)\n        #print(shap_values.data)\n        \n        text_contrib = np.abs(shap_values.values[0, :num_txt_tokens]).sum()\n        image_contrib = np.abs(shap_values.values[0, num_txt_tokens:]).sum()\n        text_score = text_contrib / (text_contrib + image_contrib)\n        image_score = image_contrib / (text_contrib + image_contrib) # is just 1 - text_score in the two modalities case\n        return text_score, image_score\n    \n    def wrapper_mmscore(self, txt_to_explain, img_to_explain): # specify better the types of the parameters (img must be a tensor of shape CxWxH)\n        mmscore_list = []\n        \n        for txt, img in zip(txt_to_explain, img_to_explain):\n            \n            txt_tokens = word_tokenize(txt)\n            num_txt_tokens = len(txt_tokens)\n            p = int(math.ceil(np.sqrt(num_txt_tokens)))\n            patch_size = 224 // p\n            img_tokens = [\" \" for el in range(1, p**2+1)]\n            txt_tokens = np.array(txt_tokens + img_tokens)\n\n            self.img = img\n            self.num_txt_tokens = num_txt_tokens\n            self.patch_size = patch_size\n\n            explainer = shap.Explainer(self.get_model_prediction, self.custom_masker, silent=True)\n\n            # print(txt_tokens.shape)\n            # print(type(txt_tokens))\n            txt_tokens = txt_tokens.reshape(1, -1)\n            #print(txt_tokens)\n\n            shap_values = explainer(txt_tokens)\n            text_score, image_score = self.compute_mmscore(num_txt_tokens, shap_values)\n            mmscore_list.append(text_score)\n            \n        mmscore_array = np.array(mmscore_list)\n        mmshap_mean = np.mean(mmscore_array)\n        mmshap_variance = np.var(mmscore_array)\n            \n        return mmshap_mean, mmshap_variance # image_score si ricava in automatico da text_score","metadata":{"execution":{"iopub.status.busy":"2024-07-21T18:11:52.376559Z","iopub.execute_input":"2024-07-21T18:11:52.376940Z","iopub.status.idle":"2024-07-21T18:11:52.404682Z","shell.execute_reply.started":"2024-07-21T18:11:52.376897Z","shell.execute_reply":"2024-07-21T18:11:52.403186Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"from torchvision import transforms\n\ncheckpoint = torch.load('/kaggle/input/model-params/model_3.pth', map_location=torch.device('cpu'))\nclassifier = MisogynyCls(5)\nclassifier.load_state_dict(checkpoint)\n\nmmshap_analyzer = MMSHAP(classifier)\n\ntxt = [\"mi chiamo silvio e sono bellissimo\", \"miao miao miao\", \"yo yo yu ghdhd \"]\nimage = [transforms.ToTensor()(Image.new('RGB', (224, 224), color='white')) for i in range(3)] # DA FARE UN RESIZE DI (224, 224)\n\ntext_score_mean, test_score_variance = mmshap_analyzer.wrapper_mmscore(txt, image)\n\nprint(f\"text_score: {text_score_mean} - test_score_variance: {test_score_variance}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-07-21T18:11:52.406763Z","iopub.execute_input":"2024-07-21T18:11:52.407239Z","iopub.status.idle":"2024-07-21T18:14:26.340624Z","shell.execute_reply.started":"2024-07-21T18:11:52.407173Z","shell.execute_reply":"2024-07-21T18:14:26.339067Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"(1, 15)\n(1, 7)\n(1, 8)\ntext_score: 0.9998103516728108 - test_score_variance: 1.296362378938283e-08\n","output_type":"stream"}]}]}
{"metadata":{"colab":{"provenance":[],"gpuType":"T4","include_colab_link":true},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","kaggle":{"accelerator":"none","dataSources":[{"sourceId":8814535,"sourceType":"datasetVersion","datasetId":5302239},{"sourceId":8823503,"sourceType":"datasetVersion","datasetId":5308380}],"dockerImageVersionId":30732,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install --upgrade transformers","metadata":{"id":"ClPn9DhYJM8u","colab":{"base_uri":"https://localhost:8080/"},"outputId":"fabbbf4c-a592-43fa-c00a-9ba78826e74f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch import nn\n\n\nclass MisogynyCls(nn.Module):\n    def __init__(self, num_linear_layers, task_a_out=1, task_b_out=4, input_dim=1024, hidden_dim=512, drop_value=0.2):\n        super().__init__()\n        self.head_task_a = nn.Linear(hidden_dim, task_a_out)\n        self.head_task_b = nn.Linear(hidden_dim, task_b_out)\n\n        self.layers = nn.ModuleList()\n\n        for i in range(num_linear_layers):\n            if i == 0:\n                self.layers.append(nn.Linear(input_dim, hidden_dim))\n            else:\n                self.layers.append(nn.Linear(hidden_dim, hidden_dim))\n            self.layers.append(nn.Dropout(drop_value))\n            self.layers.append(nn.ReLU())\n\n    def forward(self, x):\n        for layer in self.layers:\n            x = layer(x)\n        return self.head_task_a(x), self.head_task_b(x)","metadata":{"id":"ZRBdpGpDJqv0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## IN CASE WE NEED TO TRANSFORM A TSV/CSV INTO A JSON FORMAT","metadata":{}},{"cell_type":"code","source":"import csv\nimport json\nimport os\n\n# Leggi il file TSV, convertilo in JSON (per facilitare la lettura)\ninput_file = '/kaggle/input/mydataset-wow/train_image_text.tsv'\noutput_file = '/kaggle/working/train_image_text.json'\n\ndata = []\n\nwith open(input_file, newline='', encoding='utf-8') as tsvfile:\n    reader = csv.DictReader(tsvfile, delimiter='\\t')\n    for row in reader:\n        data.append(row)\n\n# Crea il file JSON se non esiste\nif not os.path.exists(output_file):\n    with open(output_file, 'w', encoding='utf-8') as jsonfile:\n        json.dump([], jsonfile, ensure_ascii=False, indent=4)\n    print(f\"File JSON vuoto creato come {output_file}\")\n\n    # Scrivi i dati nel file JSON\n    with open(output_file, 'w', encoding='utf-8') as jsonfile:\n        json.dump(data, jsonfile, ensure_ascii=False, indent=4)\n\n    print(f\"File JSON salvato come {output_file}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# testing the model...\nimport torch\n\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\ninputs = processor(text=[\"a photo of a cat\"], images=image, return_tensors=\"pt\", padding=True)\noutputs = model(**inputs)\n\n#print(f\"text_embeds shape: {outputs['text_embeds'].shape}\")\n#print(f\"image_embeds shape: {outputs['image_embeds'].shape}\")\n\nmodel_input = torch.cat([outputs['text_embeds'], outputs['image_embeds']], dim=1)\nmodel_to_test = MyModel(2, 4, 10)\n\nmiao, miaomiao = model_to_test(model_input)\n\nprint(f\"head_task_a: {miao}\")\nprint(f\"head_task_b: {miaomiao}\")\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FBA_cvmeYdJs","outputId":"d96d3812-3da3-46fe-f1ee-c0221512e875","jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#CLIP IMAGE PROCESSING AND EMBEDDING\n\nimport os\nimport json\nfrom tqdm import tqdm\nfrom PIL import Image\n\nwith open(\"/kaggle/working/train_image_text.json\",\"r\") as f:\n  data = json.load(f)\n\n  text_list=[]\n  image_list=[]\n  i=0\n    \n  for item in tqdm(data):\n    if i>5000:\n      break\n    file_path = f\"/kaggle/input/mydataset-wow/MAMI DATASET-20240614T064502Z-001/MAMI DATASET/training/TRAINING/{item['file_name']}\"\n    image_text = item[\"text\"]\n    text_list.append(image_text)\n    image_list.append(Image.open(f\"{file_path}\"))\n    i+=1\n\n  print(f\"len image_list: {len(image_list)}\")\n  print(f\"len text_list: {len(text_list)}\")\n\n  inputs = processor(text=text_list, images=image_list, return_tensors=\"pt\", padding=True)\n  outputs = model(**inputs)\n","metadata":{"id":"BI3BOah8mmaN","outputId":"44210de1-21a7-452d-d6b2-600612626ea2","colab":{"base_uri":"https://localhost:8080/"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import Dataset\nimport os\n\nclass MultimodalDataset(Dataset): # Dataset for handling multimodal data\n    def __init__(self, images_dir, json_file_path): # dir_path -> directory path where images are stored / json_file_path -> file path for metadata (including labels)   \n        file_paths, text_list, labels_misogyny, shaming_label_list, stereotype_label_list, objectification_label_list, violence_label_list = load_json_file(json_file_path)\n        \n        #print(f\"len image_list: {len(file_paths)}\") # dovrebbe essere +/- 9k\n        #print(f\"len text_list: {len(text_list)}\") # dovrebbe essere +/- 9k\n        \n        self.file_paths = file_paths\n        self.images_dir = images_dir\n        self.text_list = text_list\n        self.labels_misogyny = labels_misogyny\n        self.shaming_label_list = shaming_label_list\n        self.stereotype_label_list = stereotype_label_list\n        self.objectification_label_list = objectification_label_list\n        self.violence_label_list = violence_label_list\n        \n    def __len__(self):\n        return len(self.file_paths)\n    \n    def __getitem__(self, idx):\n        return self.file_paths[idx], self.text_list[idx], self.labels_misogyny[idx], self.shaming_label_list[idx], self.stereotype_label_list[idx], self.objectification_label_list[idx], self.violence_label_list[idx]\n    \ndef load_json_file(json_file_path):\n    with open(json_file_path,\"r\") as f:\n        data = json.load(f)\n        text_list = [] # list of the text related to each image\n        image_list = [] # list of images path\n        labels_misogyny = [] # list of TASK A labels (misogyny classification)\n\n        ### list of TASK B labels ###\n        shaming_label_list = [] \n        stereotype_label_list = []\n        objectification_label_list = []\n        violence_label_list = []\n\n\n        for item in tqdm(data): \n            image_list.append(item['file_name'])\n            text_list.append(item[\"text\"])\n            labels_misogyny.append(float(item[\"label\"]))\n            shaming_label_list.append(float(item[\"shaming\"]))\n            stereotype_label_list.append(float(item[\"stereotype\"]))\n            objectification_label_list.append(float(item[\"objectification\"]))\n            violence_label_list.append(float(item[\"violence\"]))\n\n        #print(f\"{type(labels_misogyny)}\")\n        return image_list, text_list, torch.tensor(labels_misogyny, dtype=torch.float32), torch.tensor(shaming_label_list, dtype=torch.float32), torch.tensor(stereotype_label_list, dtype=torch.float32), torch.tensor(objectification_label_list, dtype=torch.float32), torch.tensor(violence_label_list, dtype=torch.float32)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def accuracy(out, preds):\n    total = out.shape[0]\n    #print(f\"out shape: {out.shape}\")\n    #print(f\"preds shape: {preds.shape}\")\n\n    correct = (preds == out).sum().item()\n    return correct/total","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## LOADING THE DATASET AND TRAINING THE NETWORK","metadata":{}},{"cell_type":"code","source":"\n# DA NOTARE CHE HO TAGLIATO A 77 IL NUMERO DI TOKEN NEL TESTO !!! \n# ALTERNATIVA E' QUELLA DI PROCESSARE SOLO LE COPPIE CHE HANNO UN PROMPT DELLA GIUSTA LUNGHEZZA (< 77)\n\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import CLIPProcessor, CLIPModel\nfrom PIL import Image # per debug\nimport os\nimport json\nfrom tqdm import tqdm\nfrom PIL import Image\nimport torch.nn.functional as F\nimport torch.optim as optim\n\n\n\nclass Trainer():\n    def __init__(self, images_dir, json_file_path)\n    # Check if CUDA is available\n    if torch.cuda.is_available(images_dir):\n        # Set the device to the first available CUDA device\n        self.device = torch.device(\"cuda:0\")\n        print(f\"Using device: {device}\")\n    else:\n        # If CUDA is not available, use the CPU\n        self.device = torch.device(\"cpu\")\n        print(\"CUDA is not available. Using CPU.\")\n\n    # Loading the Dataset\n    self.images_dir = images_dir\n    self.json_file_path = json_file_path\n    \n    # Defining the Dataset\n    train_data = MultimodalDataset(images_dir, json_file_path)\n    self.train_dataloader = DataLoader(train_data, batch_size=64, shuffle=True)\n\n    # Defining the Model\n    self.classifier = MisogynyCls(5).to(device)\n    self.optimizer = optim.Adam(classifier.parameters(), lr=0.001)\n    self.loss_taskA = nn.CrossEntropyLoss()\n    self.loss_taskB = nn.CrossEntropyLoss()\n    \n    # Pretrained CLIP loading...\n    self.clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n    self.clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n    \n    def train_model(self, num_epochs):\n        for epoch in range(num_epochs):\n            loss_taskA, loss_taskB, accuracy_taskA, accuracy_taskB = self.train_epoch()\n            print(f'Epoch [{epoch+1}/{num_epochs}], Loss task A: {loss_taskA: .4f}, Loss task B: {loss_taskB: .4f}')\n            print(f'Accuracy task A: {accuracy_taskA}, Accuracy task B: {accuracy_taskB}')\n            \n            \n\n    def train_epoch(self)\n        # Poi durante il training andrÃ² a creare l'embedding per entrambi\n        for batch in tqdm(self.train_dataloader):\n            self.optimizer.zero_grad() # ZEROING OUT THE GRADIENTS\n\n            # CREATING THE CLIP EMBEDDINGS\n            image_list, text_list, labels_misogyny, shaming_labels, stereotype_labels, objectification_labels, violence_labels = batch\n            labels_misogyny = labels_misogyny.to(device)\n            labels_taskB = torch.stack([shaming_labels, stereotype_labels, objectification_labels, violence_labels],  dim=1).to(device)\n\n            image_list = [Image.open(f\"{os.path.join(images_dir, img)}\") for img in image_list] # per poterlo usare poi con CLIP\n            text_list = [text[:77] for text in text_list] # per poterlo usare poi con CLIP, altrimenti fa i capricci :(\n\n            inputs = self.clip_processor(text=text_list, images=image_list, return_tensors=\"pt\", padding=True)\n            outputs = self.clip_model(**inputs)\n\n\n            # GETTING THE PREDICTIONS...\n            model_input = torch.cat([outputs['text_embeds'], outputs['image_embeds']], dim=1).to(device)\n            pred_taskA, pred_taskB = self.classifier(model_input)\n\n            loss_A = self.loss_taskA(pred_taskA.squeeze(1), labels_misogyny)\n            loss_B = self.loss_taskB(pred_taskB, labels_taskB)\n            loss = loss_A + loss_B\n            \n            loss.backward()\n            \n            accuracy_taskA = accuracy(pred_taskA, labels_misogyny)\n            accuracy_taskB = accuracy(pred_taskB, labels_taskB)\n\n            self.optimizer.step()\n\n\n            return loss_A.item(), loss_B.item(), accuracy_taskA, accuracy_taskB\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_trainer = Trainer(\"/kaggle/input/mydataset-wow/MAMI DATASET-20240614T064502Z-001/MAMI DATASET/training/TRAINING/\", # images_dir\n                        \"/kaggle/working/train_image_text.json\") # json_file as data source\n)\n\nmodel_trainer.train_model()","metadata":{},"execution_count":null,"outputs":[]}]}
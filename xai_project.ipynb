{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ClPn9DhYJM8u","outputId":"fabbbf4c-a592-43fa-c00a-9ba78826e74f","trusted":true},"outputs":[],"source":["!pip install --upgrade transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZRBdpGpDJqv0","trusted":true},"outputs":[],"source":["from torch import nn\n","\n","\n","class MisogynyCls(nn.Module):\n","    def __init__(self, num_linear_layers, task_a_out=1, task_b_out=4, input_dim=1024, hidden_dim=512, drop_value=0.2):\n","        super().__init__()\n","        self.head_task_a = nn.Linear(hidden_dim, task_a_out)\n","        self.head_task_b = nn.Linear(hidden_dim, task_b_out)\n","\n","        self.layers = nn.ModuleList()\n","\n","        for i in range(num_linear_layers):\n","            if i == 0:\n","                self.layers.append(nn.Linear(input_dim, hidden_dim))\n","            else:\n","                self.layers.append(nn.Linear(hidden_dim, hidden_dim))\n","            self.layers.append(nn.Dropout(drop_value))\n","            self.layers.append(nn.ReLU())\n","\n","    def forward(self, x):\n","        for layer in self.layers:\n","            x = layer(x)\n","        return self.head_task_a(x), self.head_task_b(x)"]},{"cell_type":"markdown","metadata":{},"source":["## IN CASE WE NEED TO TRANSFORM A TSV/CSV INTO A JSON FORMAT (last two cells are only for testing purposese... the first instead is the one needed for the conversion from tsv to json)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import csv\n","import json\n","import os\n","\n","# Leggi il file TSV, convertilo in JSON (per facilitare la lettura)\n","input_file = '/kaggle/input/mydataset-wow/train_image_text.tsv'\n","output_file = '/kaggle/working/train_image_text.json'\n","\n","data = []\n","\n","with open(input_file, newline='', encoding='utf-8') as tsvfile:\n","    reader = csv.DictReader(tsvfile, delimiter='\\t')\n","    for row in reader:\n","        data.append(row)\n","\n","# Crea il file JSON se non esiste\n","if not os.path.exists(output_file):\n","    with open(output_file, 'w', encoding='utf-8') as jsonfile:\n","        json.dump([], jsonfile, ensure_ascii=False, indent=4)\n","    print(f\"File JSON vuoto creato come {output_file}\")\n","\n","    # Scrivi i dati nel file JSON\n","    with open(output_file, 'w', encoding='utf-8') as jsonfile:\n","        json.dump(data, jsonfile, ensure_ascii=False, indent=4)\n","\n","    print(f\"File JSON salvato come {output_file}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FBA_cvmeYdJs","jupyter":{"source_hidden":true},"outputId":"d96d3812-3da3-46fe-f1ee-c0221512e875","trusted":true},"outputs":[],"source":["# testing the model...\n","import torch\n","\n","url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n","image = Image.open(requests.get(url, stream=True).raw)\n","\n","inputs = processor(text=[\"a photo of a cat\"], images=image, return_tensors=\"pt\", padding=True)\n","outputs = model(**inputs)\n","\n","#print(f\"text_embeds shape: {outputs['text_embeds'].shape}\")\n","#print(f\"image_embeds shape: {outputs['image_embeds'].shape}\")\n","\n","model_input = torch.cat([outputs['text_embeds'], outputs['image_embeds']], dim=1)\n","model_to_test = MyModel(2, 4, 10)\n","\n","miao, miaomiao = model_to_test(model_input)\n","\n","print(f\"head_task_a: {miao}\")\n","print(f\"head_task_b: {miaomiao}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BI3BOah8mmaN","outputId":"44210de1-21a7-452d-d6b2-600612626ea2","trusted":true},"outputs":[],"source":["#CLIP IMAGE PROCESSING AND EMBEDDING\n","\n","import os\n","import json\n","from tqdm import tqdm\n","from PIL import Image\n","\n","with open(\"/kaggle/working/train_image_text.json\",\"r\") as f:\n","  data = json.load(f)\n","\n","  text_list=[]\n","  image_list=[]\n","  i=0\n","    \n","  for item in tqdm(data):\n","    if i>5000:\n","      break\n","    file_path = f\"/kaggle/input/mydataset-wow/MAMI DATASET-20240614T064502Z-001/MAMI DATASET/training/TRAINING/{item['file_name']}\"\n","    image_text = item[\"text\"]\n","    text_list.append(image_text)\n","    image_list.append(Image.open(f\"{file_path}\"))\n","    i+=1\n","\n","  print(f\"len image_list: {len(image_list)}\")\n","  print(f\"len text_list: {len(text_list)}\")\n","\n","  inputs = processor(text=text_list, images=image_list, return_tensors=\"pt\", padding=True)\n","  outputs = model(**inputs)\n"]},{"cell_type":"markdown","metadata":{},"source":["## Multimodal Dataset definition"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from torch.utils.data import Dataset\n","import os\n","\n","class MultimodalDataset(Dataset): # Dataset for handling multimodal data\n","    def __init__(self, images_dir, json_file_path): # dir_path -> directory path where images are stored / json_file_path -> file path for metadata (including labels)   \n","        file_paths, text_list, labels_misogyny, shaming_label_list, stereotype_label_list, objectification_label_list, violence_label_list = load_json_file(json_file_path)\n","        \n","        #print(f\"len image_list: {len(file_paths)}\") # dovrebbe essere +/- 9k\n","        #print(f\"len text_list: {len(text_list)}\") # dovrebbe essere +/- 9k\n","        \n","        self.file_paths = file_paths\n","        self.images_dir = images_dir\n","        self.text_list = text_list\n","        self.labels_misogyny = labels_misogyny\n","        self.shaming_label_list = shaming_label_list\n","        self.stereotype_label_list = stereotype_label_list\n","        self.objectification_label_list = objectification_label_list\n","        self.violence_label_list = violence_label_list\n","        \n","    def __len__(self):\n","        return len(self.file_paths)\n","    \n","    def __getitem__(self, idx):\n","        return self.file_paths[idx], self.text_list[idx], self.labels_misogyny[idx], self.shaming_label_list[idx], self.stereotype_label_list[idx], self.objectification_label_list[idx], self.violence_label_list[idx]\n","    \n","def load_json_file(json_file_path):\n","    with open(json_file_path,\"r\") as f:\n","        data = json.load(f)\n","        text_list = [] # list of the text related to each image\n","        image_list = [] # list of images path\n","        labels_misogyny = [] # list of TASK A labels (misogyny classification)\n","\n","        ### list of TASK B labels ###\n","        shaming_label_list = [] \n","        stereotype_label_list = []\n","        objectification_label_list = []\n","        violence_label_list = []\n","\n","\n","        for item in tqdm(data): \n","            image_list.append(item['file_name'])\n","            text_list.append(item[\"text\"])\n","            labels_misogyny.append(float(item[\"label\"]))\n","            shaming_label_list.append(float(item[\"shaming\"]))\n","            stereotype_label_list.append(float(item[\"stereotype\"]))\n","            objectification_label_list.append(float(item[\"objectification\"]))\n","            violence_label_list.append(float(item[\"violence\"]))\n","\n","        #print(f\"{type(labels_misogyny)}\")\n","        return image_list, text_list, torch.tensor(labels_misogyny, dtype=torch.float32), torch.tensor(shaming_label_list, dtype=torch.float32), torch.tensor(stereotype_label_list, dtype=torch.float32), torch.tensor(objectification_label_list, dtype=torch.float32), torch.tensor(violence_label_list, dtype=torch.float32)\n","    \n","def accuracy(out, preds):\n","    total = out.shape[0]\n","    #print(f\"out shape: {out.shape}\")\n","    #print(f\"preds shape: {preds.shape}\")\n","\n","    correct = (preds == out).sum().item()\n","    return correct/total\n"]},{"cell_type":"markdown","metadata":{},"source":["## LOADING THE DATASET AND TRAINING THE NETWORK\n","# Thanks to a class used for handling the argument of the network training (still room for modifying it !)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","# DA NOTARE CHE HO TAGLIATO A 77 IL NUMERO DI TOKEN NEL TESTO !!! \n","# ALTERNATIVA E' QUELLA DI PROCESSARE SOLO LE COPPIE CHE HANNO UN PROMPT DELLA GIUSTA LUNGHEZZA (< 77)\n","\n","import torch\n","from torch import nn\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import CLIPProcessor, CLIPModel\n","from PIL import Image # per debug\n","import os\n","import json\n","from tqdm import tqdm\n","from PIL import Image\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","\n","\n","class Trainer():\n","    def __init__(self, images_dir, json_file_path)\n","    # Check if CUDA is available\n","    if torch.cuda.is_available(images_dir):\n","        # Set the device to the first available CUDA device\n","        self.device = torch.device(\"cuda:0\")\n","        print(f\"Using device: {device}\")\n","    else:\n","        # If CUDA is not available, use the CPU\n","        self.device = torch.device(\"cpu\")\n","        print(\"CUDA is not available. Using CPU.\")\n","\n","    # Loading the Dataset\n","    self.images_dir = images_dir\n","    self.json_file_path = json_file_path\n","    \n","    # Defining the Dataset\n","    train_data = MultimodalDataset(images_dir, json_file_path)\n","    self.train_dataloader = DataLoader(train_data, batch_size=64, shuffle=True)\n","\n","    # Defining the Model\n","    self.classifier = MisogynyCls(5).to(device)\n","    self.optimizer = optim.Adam(classifier.parameters(), lr=0.001)\n","    self.loss_taskA = nn.CrossEntropyLoss()\n","    self.loss_taskB = nn.CrossEntropyLoss()\n","    \n","    # Pretrained CLIP loading...\n","    self.clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n","    self.clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n","    \n","    def train_model(self, num_epochs):\n","        for epoch in range(num_epochs):\n","            loss_taskA, loss_taskB, accuracy_taskA, accuracy_taskB = self.train_epoch()\n","            print(f'Epoch [{epoch+1}/{num_epochs}], Loss task A: {loss_taskA: .4f}, Loss task B: {loss_taskB: .4f}')\n","            print(f'Accuracy task A: {accuracy_taskA}, Accuracy task B: {accuracy_taskB}')\n","            \n","            \n","\n","    def train_epoch(self)\n","        # Poi durante il training andrÃ² a creare l'embedding per entrambi\n","        for batch in tqdm(self.train_dataloader):\n","            self.optimizer.zero_grad() # ZEROING OUT THE GRADIENTS\n","\n","            # CREATING THE CLIP EMBEDDINGS\n","            image_list, text_list, labels_misogyny, shaming_labels, stereotype_labels, objectification_labels, violence_labels = batch\n","            labels_misogyny = labels_misogyny.to(device)\n","            labels_taskB = torch.stack([shaming_labels, stereotype_labels, objectification_labels, violence_labels],  dim=1).to(device)\n","\n","            image_list = [Image.open(f\"{os.path.join(images_dir, img)}\") for img in image_list] # per poterlo usare poi con CLIP\n","            text_list = [text[:77] for text in text_list] # per poterlo usare poi con CLIP, altrimenti fa i capricci :(\n","\n","            inputs = self.clip_processor(text=text_list, images=image_list, return_tensors=\"pt\", padding=True)\n","            outputs = self.clip_model(**inputs)\n","\n","\n","            # GETTING THE PREDICTIONS...\n","            model_input = torch.cat([outputs['text_embeds'], outputs['image_embeds']], dim=1).to(device)\n","            pred_taskA, pred_taskB = self.classifier(model_input)\n","\n","            loss_A = self.loss_taskA(pred_taskA.squeeze(1), labels_misogyny)\n","            loss_B = self.loss_taskB(pred_taskB, labels_taskB)\n","            loss = loss_A + loss_B\n","            \n","            loss.backward()\n","            \n","            accuracy_taskA = accuracy(pred_taskA, labels_misogyny)\n","            accuracy_taskB = accuracy(pred_taskB, labels_taskB)\n","\n","            self.optimizer.step()\n","\n","\n","            return loss_A.item(), loss_B.item(), accuracy_taskA, accuracy_taskB\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model_trainer = Trainer(\"/kaggle/input/mydataset-wow/MAMI DATASET-20240614T064502Z-001/MAMI DATASET/training/TRAINING/\", # images_dir\n","                        \"/kaggle/working/train_image_text.json\") # json_file as data source\n","\n","\n","model_trainer.train_model()"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","include_colab_link":true,"provenance":[]},"kaggle":{"accelerator":"none","dataSources":[{"datasetId":5302239,"sourceId":8814535,"sourceType":"datasetVersion"},{"datasetId":5308380,"sourceId":8823503,"sourceType":"datasetVersion"}],"dockerImageVersionId":30732,"isGpuEnabled":false,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}

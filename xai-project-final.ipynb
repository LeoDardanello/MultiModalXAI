{"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","include_colab_link":true,"provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8814535,"sourceType":"datasetVersion","datasetId":5302239},{"sourceId":8823503,"sourceType":"datasetVersion","datasetId":5308380},{"sourceId":8842298,"sourceType":"datasetVersion","datasetId":5321593}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install --upgrade transformers","metadata":{"id":"ClPn9DhYJM8u","outputId":"fabbbf4c-a592-43fa-c00a-9ba78826e74f","execution":{"iopub.status.busy":"2024-07-02T15:15:38.537480Z","iopub.execute_input":"2024-07-02T15:15:38.537831Z","iopub.status.idle":"2024-07-02T15:15:54.463775Z","shell.execute_reply.started":"2024-07-02T15:15:38.537803Z","shell.execute_reply":"2024-07-02T15:15:54.462626Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.41.2)\nCollecting transformers\n  Downloading transformers-4.42.3-py3-none-any.whl.metadata (43 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.23.2)\nRequirement already satisfied: numpy<2.0,>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.3)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.19.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.3.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\nDownloading transformers-4.42.3-py3-none-any.whl (9.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.3/9.3 MB\u001b[0m \u001b[31m78.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hInstalling collected packages: transformers\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.41.2\n    Uninstalling transformers-4.41.2:\n^C\n\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"!pip install wandb\n","metadata":{"execution":{"iopub.status.busy":"2024-07-02T15:15:54.465802Z","iopub.execute_input":"2024-07-02T15:15:54.466111Z","iopub.status.idle":"2024-07-02T15:15:59.123346Z","shell.execute_reply.started":"2024-07-02T15:15:54.466081Z","shell.execute_reply":"2024-07-02T15:15:59.122268Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: wandb in /opt/conda/lib/python3.10/site-packages (0.17.0)\nRequirement already satisfied: click!=8.0.0,>=7.1 in /opt/conda/lib/python3.10/site-packages (from wandb) (8.1.7)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (0.4.0)\nRequirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.1.41)\nRequirement already satisfied: platformdirs in /opt/conda/lib/python3.10/site-packages (from wandb) (3.11.0)\nRequirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.20.3)\nRequirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from wandb) (6.0.1)\nRequirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.32.3)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.3.1)\nRequirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb) (1.3.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb) (69.0.3)\nRequirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n^C\n\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"!wandb login\n","metadata":{"execution":{"iopub.status.busy":"2024-07-02T15:15:59.124701Z","iopub.execute_input":"2024-07-02T15:15:59.125000Z","iopub.status.idle":"2024-07-02T15:16:02.666878Z","shell.execute_reply.started":"2024-07-02T15:15:59.124969Z","shell.execute_reply":"2024-07-02T15:16:02.665931Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \nAborted!\n","output_type":"stream"}]},{"cell_type":"code","source":"!rm -rf /kaggle/working/*","metadata":{"execution":{"iopub.status.busy":"2024-07-02T15:16:11.048880Z","iopub.execute_input":"2024-07-02T15:16:11.049284Z","iopub.status.idle":"2024-07-02T15:16:11.988204Z","shell.execute_reply.started":"2024-07-02T15:16:11.049245Z","shell.execute_reply":"2024-07-02T15:16:11.986994Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"from torch import nn\n\n\nclass MisogynyCls(nn.Module):\n    def __init__(self, num_linear_layers, task_a_out=1, task_b_out=4, input_dim=1024, hidden_dim=512, drop_value=0.2):\n        super().__init__()\n        self.head_task_a = nn.Linear(hidden_dim, task_a_out)\n        self.head_task_b = nn.Linear(hidden_dim, task_b_out)\n        self.sigmoid = nn.Sigmoid()\n\n\n        self.layers = nn.ModuleList()\n\n        for i in range(num_linear_layers):\n            if i == 0:\n                self.layers.append(nn.Linear(input_dim, hidden_dim))\n            else:\n                self.layers.append(nn.Linear(hidden_dim, hidden_dim))\n                \n            self.layers.append(nn.Dropout(drop_value))\n            self.layers.append(nn.ReLU())\n\n    def forward(self, x):\n        for layer in self.layers:\n            x = layer(x)\n        pred_taskA = self.sigmoid(self.head_task_a(x))\n        pred_taskB = self.sigmoid(self.head_task_b(x))\n        \n        return pred_taskA.squeeze(1), pred_taskB","metadata":{"id":"ZRBdpGpDJqv0","execution":{"iopub.status.busy":"2024-07-02T15:16:14.175470Z","iopub.execute_input":"2024-07-02T15:16:14.175847Z","iopub.status.idle":"2024-07-02T15:16:17.889134Z","shell.execute_reply.started":"2024-07-02T15:16:14.175812Z","shell.execute_reply":"2024-07-02T15:16:17.888389Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"## IN CASE WE NEED TO TRANSFORM A TSV/CSV INTO A JSON FORMAT (last two cells are only for testing purposese... the first instead is the one needed for the conversion from tsv to json)","metadata":{}},{"cell_type":"code","source":"import csv\nimport json\nimport os\n\nfile_and_dest = [('/kaggle/input/dataset-wow/train_image_text.tsv','/kaggle/working/train_image_text.json'),\n                    ('/kaggle/input/dataset-wow/test_image_text.tsv','/kaggle/working/test_image_text.json')]\n\nfor file in file_and_dest: \n    data = []\n\n    with open(file[0], newline='', encoding='utf-8') as tsvfile:\n        reader = csv.DictReader(tsvfile, delimiter='\\t')\n        for row in reader:\n            data.append(row)\n\n    if not os.path.exists(file[1]):\n        with open(file[1], 'w', encoding='utf-8') as jsonfile:\n            json.dump([], jsonfile, ensure_ascii=False, indent=4)\n        print(f\"File JSON vuoto creato come {file[1]}\")\n\n        with open(file[1], 'w', encoding='utf-8') as jsonfile:\n            json.dump(data, jsonfile, ensure_ascii=False, indent=4)\n\n        print(f\"File JSON salvato come {file[1]}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-07-02T15:17:08.064252Z","iopub.execute_input":"2024-07-02T15:17:08.065158Z","iopub.status.idle":"2024-07-02T15:17:08.122065Z","shell.execute_reply.started":"2024-07-02T15:17:08.065127Z","shell.execute_reply":"2024-07-02T15:17:08.121301Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"## Multimodal Dataset definition","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import Dataset\nimport os\nimport wandb\n\nclass MultimodalDataset(Dataset): # Dataset for handling multimodal data\n    def __init__(self, images_dir, json_file_path): # dir_path -> directory path where images are stored / json_file_path -> file path for metadata (including labels)   \n        file_paths, text_list, labels_misogyny, shaming_label_list, stereotype_label_list, objectification_label_list, violence_label_list = load_json_file(json_file_path)\n        \n        #print(f\"len image_list: {len(file_paths)}\") # dovrebbe essere +/- 9k\n        #print(f\"len text_list: {len(text_list)}\") # dovrebbe essere +/- 9k\n        \n        self.file_paths = file_paths\n        self.images_dir = images_dir\n        self.text_list = text_list\n        self.labels_misogyny = labels_misogyny\n        self.shaming_label_list = shaming_label_list\n        self.stereotype_label_list = stereotype_label_list\n        self.objectification_label_list = objectification_label_list\n        self.violence_label_list = violence_label_list\n        \n    def __len__(self):\n        return len(self.file_paths)\n    \n    def __getitem__(self, idx):\n        return self.file_paths[idx], self.text_list[idx], self.labels_misogyny[idx], self.shaming_label_list[idx], self.stereotype_label_list[idx], self.objectification_label_list[idx], self.violence_label_list[idx]\n    \ndef load_json_file(json_file_path):\n    with open(json_file_path,\"r\") as f:\n        data = json.load(f)\n        text_list = [] # list of the text related to each image\n        image_list = [] # list of images path\n        labels_misogyny = [] # list of TASK A labels (misogyny classification)\n\n        ### list of TASK B labels ###\n        shaming_label_list = [] \n        stereotype_label_list = []\n        objectification_label_list = []\n        violence_label_list = []\n\n\n        for item in tqdm(data): \n            image_list.append(item['file_name'])\n            text_list.append(item[\"text\"])\n            labels_misogyny.append(float(item[\"label\"]))\n            shaming_label_list.append(float(item[\"shaming\"]))\n            stereotype_label_list.append(float(item[\"stereotype\"]))\n            objectification_label_list.append(float(item[\"objectification\"]))\n            violence_label_list.append(float(item[\"violence\"]))\n\n        #print(f\"{type(labels_misogyny)}\")\n        return image_list, text_list, torch.tensor(labels_misogyny, dtype=torch.float32), torch.tensor(shaming_label_list, dtype=torch.float32), torch.tensor(stereotype_label_list, dtype=torch.float32), torch.tensor(objectification_label_list, dtype=torch.float32), torch.tensor(violence_label_list, dtype=torch.float32)\n    \ndef accuracy(preds, labels, thresh):\n    num_samples = labels.shape[0]\n    preds = preds > thresh\n    matching_rows = torch.eq(labels.bool(), preds)\n    \n    # in case we're dealing with the prediction of task B/task A (they've different number of dimensions)\n    num_correct = matching_rows.all(dim=1).sum().item() if preds.ndim!=1 else matching_rows.sum().item()\n    return 100*(num_correct/num_samples)\n\n\ndef split_json_dataset(split_train, dataset_path, train_data_path, test_data_path): #per adesso li metto sempre in kaggle/working/... da cambiare\n\n    # Split the dataset in train/test set\n    with open(dataset_path) as f:\n        data = json.load(f)\n\n    total_objects = len(data)\n    num_train_objects = int(total_objects * split_train)\n    train_data = data[:num_train_objects]\n    test_data = data[num_train_objects:]\n\n    with open(train_data_path, 'w', encoding='utf-8') as jsonfile:\n        json.dump(train_data, jsonfile, ensure_ascii=False, indent=4)\n\n    with open(test_data_path, 'w', encoding='utf-8') as jsonfile:\n        json.dump(test_data, jsonfile, ensure_ascii=False, indent=4)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-02T14:44:56.083140Z","iopub.execute_input":"2024-07-02T14:44:56.083535Z","iopub.status.idle":"2024-07-02T14:44:56.585075Z","shell.execute_reply.started":"2024-07-02T14:44:56.083506Z","shell.execute_reply":"2024-07-02T14:44:56.584151Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"## LOADING THE DATASET AND TRAINING THE NETWORK\n# Thanks to a class used for handling the argument of the network training (still room for modifying it !)","metadata":{}},{"cell_type":"code","source":"### import torch\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import CLIPProcessor, CLIPModel, CLIPTokenizer\nfrom PIL import Image # per debug\nimport os\nimport json\nfrom tqdm import tqdm\nfrom PIL import Image\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport wandb\n\n\n\nclass Trainer():\n    def __init__(self, images_dir, \n                       json_dataset_path,\n                       json_train_path,\n                       json_test_path,\n                       train_data_split=0.8,\n                       batch_size=256, \n                       lr=0.001, \n                       num_epochs=10,\n                       threshold=0.5,\n                       weight_taskA=0.7,\n                       weight_taskB=0.3):\n        \n        # Check if CUDA is available\n        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n        print(f\"Training on: {self.device}\")\n        \n        # Loading the Dataset\n        self.images_dir = images_dir\n        self.json_file_path = json_dataset_path\n        self.num_epochs = num_epochs\n        self.threshold = threshold\n        self.weight_taskA = weight_taskA\n        self.weight_taskB = weight_taskB\n        \n        # Defining the Dataset\n        split_json_dataset(train_data_split, json_dataset_path, json_train_path, json_test_path) # splitta il dataset in train/test\n        train_data = MultimodalDataset(images_dir, json_train_path)\n        test_data = MultimodalDataset(images_dir, json_test_path)\n        self.train_dataloader = DataLoader(train_data, batch_size, shuffle=True)\n        self.test_dataloader = DataLoader(test_data, batch_size, shuffle=True)\n\n        # Defining the Model\n        self.classifier = MisogynyCls(5).to(self.device)\n        self.optimizer = optim.Adam(self.classifier.parameters(), lr)\n        self.loss_taskA = F.binary_cross_entropy #nn.CrossEntropyLoss()\n        self.loss_taskB = F.binary_cross_entropy #nn.CrossEntropyLoss()\n\n        # Pretrained CLIP loading...\n        self.clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n        self.clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n    \n        '''\n        wandb.init(  # set the wandb project where this run will be logged\n                    project=\"Multimodal_xai\", \n                    # track hyperparameters and run metadata   \n                    config={\n                        #\"architecture\": args.model,    \"optimizer\": \"SGD\" if args.use_sgd else \"ADAM\",\n                        #\"scheduler\" : args.scheduler,    \"learning_rate\": args.lr * 100 if args.use_sgd else args.lr,\n                        #\"min_learning_rate\": args.lrmin,    \"epochs\": args.epochs,\n                        #\"filename\": args.filename,    \"batch\": args.batch_size,\n                        #\"momentum\": args.momentum,    \"weightdecay\": args.wd,\n                        #\"tanh\": args.tanh,    \"invert_lr\": args.invert_lr,\n                        #\"max_learning_rate\": args.lrmax\n                    })\n        '''\n        \n    def train_model(self):\n        for epoch in range(self.num_epochs):\n            print(f'Epoch [{epoch+1}/{self.num_epochs}]')\n            train_loss_list, test_loss_list, train_acc_taskA_list, train_acc_taskB_list,  test_acc_taskA_list, test_acc_taskB_list= self.train_epoch()\n            \n            print(f'Average Train Loss: {sum(train_loss_list) / len(train_loss_list): .4f}, Average Test Loss: {sum(test_loss_list) / len(test_loss_list): .4f}')\n            print(f'Average Accuracy Train (task A): {sum(train_acc_taskA_list) / len(train_acc_taskA_list): .4f}%, Average Accuracy Test (task A): {sum(test_acc_taskA_list) / len(test_acc_taskA_list): .4f}%')\n            print(f'Average Accuracy Train (task B): {sum(train_acc_taskB_list) / len(train_acc_taskB_list): .4f}%, Average Accuracy Test (task B): {sum(test_acc_taskB_list) / len(test_acc_taskB_list): .4f}%')\n\n            model_path = '/kaggle/working/model_' + (epoch+1) + '.pth'\n            torch.save(self.classifier.state_dict(), model_path);\n            \n            \n\n    def train_epoch(self):\n        train_loss_list = []\n        test_loss_list = []\n        train_acc_taskA_list = []\n        train_acc_taskB_list = []\n        test_acc_taskA_list = []\n        test_acc_taskB_list = []\n\n\n        \n        for batch in tqdm(self.train_dataloader):\n            self.optimizer.zero_grad() # ZEROING OUT THE GRADIENTS\n            self.classifier.train() # TRAINING MODE\n            \n            # CREATING THE CLIP EMBEDDINGS\n            image_list, text_list, labels_misogyny, shaming_labels, stereotype_labels, objectification_labels, violence_labels = batch\n            \n            image_list = [Image.open(f\"{os.path.join(self.images_dir, img)}\") for img in image_list] # per poterlo usare poi con CLIP\n            labels_misogyny = labels_misogyny.to(self.device)\n            labels_taskB = torch.stack([shaming_labels, stereotype_labels, objectification_labels, violence_labels],  dim=1).to(self.device)\n\n            inputs = self.clip_processor(text=text_list, images=image_list, return_tensors=\"pt\", padding=True, truncation=True)\n            outputs = self.clip_model(**inputs)\n\n            # GETTING THE PREDICTIONS...\n            model_input = torch.cat([outputs['text_embeds'], outputs['image_embeds']], dim=1).to(self.device)\n            pred_taskA, pred_taskB = self.classifier(model_input)\n\n            loss_A = self.loss_taskA(pred_taskA, labels_misogyny)\n            loss_B = self.loss_taskB(pred_taskB, labels_taskB, reduction='mean')\n            loss = (self.weight_taskA * loss_A) + (self.weight_taskB * loss_B)\n            train_loss_list.append(loss)\n            \n            loss.backward()\n            self.optimizer.step()\n\n            accuracy_taskA = accuracy(pred_taskA, labels_misogyny, self.threshold)\n            accuracy_taskB = accuracy(pred_taskB, labels_taskB, self.threshold)\n            \n            train_acc_taskA_list.append(accuracy_taskA)\n            train_acc_taskB_list.append(accuracy_taskB)\n            \n            \n            \n        with torch.no_grad():\n            self.classifier.eval()\n\n            for batch in self.test_dataloader:\n                # CREATING THE CLIP EMBEDDINGS\n                image_list, text_list, labels_misogyny, shaming_labels, stereotype_labels, objectification_labels, violence_labels = batch\n\n                image_list = [Image.open(f\"{os.path.join(self.images_dir, img)}\") for img in image_list] # per poterlo usare poi con CLIP\n                labels_misogyny = labels_misogyny.to(self.device)\n                labels_taskB = torch.stack([shaming_labels, stereotype_labels, objectification_labels, violence_labels],  dim=1).to(self.device)\n\n                inputs = self.clip_processor(text=text_list, images=image_list, return_tensors=\"pt\", padding=True, truncation=True)\n                outputs = self.clip_model(**inputs)\n\n                # GETTING THE PREDICTIONS...\n                model_input = torch.cat([outputs['text_embeds'], outputs['image_embeds']], dim=1).to(self.device)\n                pred_taskA, pred_taskB = self.classifier(model_input)\n\n                loss_A = self.loss_taskA(pred_taskA, labels_misogyny)\n                loss_B = self.loss_taskB(pred_taskB, labels_taskB, reduction='mean')\n                loss = (self.weight_taskA * loss_A) + (self.weight_taskB * loss_B)\n                test_loss_list.append(loss)\n\n                accuracy_taskA = accuracy(pred_taskA, labels_misogyny, self.threshold)\n                accuracy_taskB = accuracy(pred_taskB, labels_taskB, self.threshold)\n\n                test_acc_taskA_list.append(accuracy_taskA)\n                test_acc_taskB_list.append(accuracy_taskB)\n\n            \n            \n            '''\n            wandb.log({\"accuracy_taskA\": accuracy_taskA,\n                       \"accuracy_taskB\": accuracy_taskB,\n                       \"loss_A\": loss_A.item(),\n                       \"loss_B\": loss_B.item(),\n                       \"learning_rate\": self.optimizer.param_groups[0]['lr']})\n            \n            '''\n            \n        return train_loss_list, test_loss_list, train_acc_taskA_list, train_acc_taskB_list, test_acc_taskA_list, test_acc_taskB_list\n\n            ","metadata":{"execution":{"iopub.status.busy":"2024-07-02T14:50:43.507225Z","iopub.execute_input":"2024-07-02T14:50:43.507607Z","iopub.status.idle":"2024-07-02T14:50:43.995838Z","shell.execute_reply.started":"2024-07-02T14:50:43.507578Z","shell.execute_reply":"2024-07-02T14:50:43.994834Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"\nmodel_trainer = Trainer(\"/kaggle/input/dataset-wow/MAMI DATASET-20240614T064502Z-001 (1)/MAMI DATASET/training/TRAINING\", # images_dir\n                        \"/kaggle/working/train_image_text.json\",\n                        \"/kaggle/working/train.json\",\n                        \"/kaggle/working/test.json\",\n                        batch_size=64,\n                        weight_taskA=0.8,\n                        weight_taskB=0.2) # json_file as data source\n\n\nmodel_trainer.train_model()","metadata":{"execution":{"iopub.status.busy":"2024-07-02T14:50:50.744014Z","iopub.execute_input":"2024-07-02T14:50:50.744681Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Training on: cuda:0\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 7200/7200 [00:00<00:00, 444573.50it/s]\n100%|██████████| 1800/1800 [00:00<00:00, 430503.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/10]\n","output_type":"stream"},{"name":"stderr","text":" 35%|███▍      | 39/113 [11:22<21:40, 17.57s/it]","output_type":"stream"}]},{"cell_type":"code","source":"!nvidia-smi","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}
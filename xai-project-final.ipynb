{"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","include_colab_link":true,"provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8823503,"sourceType":"datasetVersion","datasetId":5308380},{"sourceId":8853110,"sourceType":"datasetVersion","datasetId":5321593}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#!pip install --upgrade transformers","metadata":{"id":"ClPn9DhYJM8u","outputId":"fabbbf4c-a592-43fa-c00a-9ba78826e74f","execution":{"iopub.status.busy":"2024-07-02T23:53:30.674350Z","iopub.execute_input":"2024-07-02T23:53:30.674712Z","iopub.status.idle":"2024-07-02T23:53:30.679399Z","shell.execute_reply.started":"2024-07-02T23:53:30.674679Z","shell.execute_reply":"2024-07-02T23:53:30.678500Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"#!pip install wandb\n","metadata":{"execution":{"iopub.status.busy":"2024-07-02T23:53:30.680511Z","iopub.execute_input":"2024-07-02T23:53:30.680783Z","iopub.status.idle":"2024-07-02T23:53:30.690960Z","shell.execute_reply.started":"2024-07-02T23:53:30.680752Z","shell.execute_reply":"2024-07-02T23:53:30.690072Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"#!wandb login\n","metadata":{"execution":{"iopub.status.busy":"2024-07-02T23:53:30.692088Z","iopub.execute_input":"2024-07-02T23:53:30.692351Z","iopub.status.idle":"2024-07-02T23:53:30.700520Z","shell.execute_reply.started":"2024-07-02T23:53:30.692328Z","shell.execute_reply":"2024-07-02T23:53:30.699680Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"!rm -rf /kaggle/working/*","metadata":{"execution":{"iopub.status.busy":"2024-07-17T11:59:13.683513Z","iopub.execute_input":"2024-07-17T11:59:13.684055Z","iopub.status.idle":"2024-07-17T11:59:14.690986Z","shell.execute_reply.started":"2024-07-17T11:59:13.684027Z","shell.execute_reply":"2024-07-17T11:59:14.689682Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"from torch import nn\n\n\nclass MisogynyCls(nn.Module):\n    def __init__(self, num_linear_layers, single_task_mode=True, task_a_out=1, task_b_out=4, input_dim=1024, hidden_dim=512, drop_value=0.2):\n        super().__init__()\n        self.single_task_mode=single_task_mode\n        self.head_task_a = nn.Linear(hidden_dim, task_a_out)\n        if not self.single_task_mode:\n            self.head_task_b = nn.Linear(hidden_dim, task_b_out)\n        self.sigmoid = nn.Sigmoid()\n\n\n        self.layers = nn.ModuleList()\n\n        for i in range(num_linear_layers):\n            if i == 0:\n                self.layers.append(nn.Linear(input_dim, hidden_dim))\n            else:\n                self.layers.append(nn.Linear(hidden_dim, hidden_dim))\n                \n            self.layers.append(nn.Dropout(drop_value))\n            self.layers.append(nn.ReLU())\n\n    def forward(self, x):\n        for layer in self.layers:\n            x = layer(x)\n        pred_taskA = self.sigmoid(self.head_task_a(x))\n        if not self.single_task_mode:\n            pred_taskB = self.sigmoid(self.head_task_b(x))\n            return pred_taskA.squeeze(1), pred_taskB\n        else:\n            return pred_taskA.squeeze(1)","metadata":{"id":"ZRBdpGpDJqv0","execution":{"iopub.status.busy":"2024-07-17T11:59:14.693091Z","iopub.execute_input":"2024-07-17T11:59:14.693715Z","iopub.status.idle":"2024-07-17T11:59:20.846632Z","shell.execute_reply.started":"2024-07-17T11:59:14.693682Z","shell.execute_reply":"2024-07-17T11:59:20.845635Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## tsv/csv to json","metadata":{}},{"cell_type":"code","source":"import csv\nimport json\nimport os\n\nimages_path = \"/kaggle/input/dataset-wow/MAMI DATASET/MAMI DATASET/training/TRAINING\"\n\nfile_and_dest = [('/kaggle/input/dataset-wow/train_image_text.tsv','/kaggle/working/train_image_text.json'),\n                    ('/kaggle/input/dataset-wow/test_image_text.tsv','/kaggle/working/test_image_text.json')]\n\n\nfor file in file_and_dest: \n    data = []\n\n    with open(file[0], newline='', encoding='utf-8') as tsvfile:\n        reader = csv.DictReader(tsvfile, delimiter='\\t')\n        \n        for row in reader:\n            data.append(row)\n                \n    if not os.path.exists(file[1]):\n        with open(file[1], 'w', encoding='utf-8') as jsonfile:\n            json.dump([], jsonfile, ensure_ascii=False, indent=4)\n        print(f\"File JSON vuoto creato come {file[1]}\")\n\n        with open(file[1], 'w', encoding='utf-8') as jsonfile:\n            json.dump(data, jsonfile, ensure_ascii=False, indent=4)\n\n        print(f\"File JSON salvato come {file[1]}\")\n\n","metadata":{"execution":{"iopub.status.busy":"2024-07-17T11:59:20.851988Z","iopub.execute_input":"2024-07-17T11:59:20.852313Z","iopub.status.idle":"2024-07-17T11:59:21.121051Z","shell.execute_reply.started":"2024-07-17T11:59:20.852283Z","shell.execute_reply":"2024-07-17T11:59:21.120074Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"File JSON vuoto creato come /kaggle/working/train_image_text.json\nFile JSON salvato come /kaggle/working/train_image_text.json\nFile JSON vuoto creato come /kaggle/working/test_image_text.json\nFile JSON salvato come /kaggle/working/test_image_text.json\n","output_type":"stream"}]},{"cell_type":"code","source":"'''\nimport csv\nimport json\nimport os\n\nimages_path = \"/kaggle/input/dataset-wow/MAMI DATASET/MAMI DATASET/training/TRAINING\"\n\nfile_source = '/kaggle/input/dataset-wow/train_image_text.tsv'\nfile_train = '/kaggle/working/train_image_text.json'\nfile_test = '/kaggle/working/test_image_text.json'\ndata = []\n\nwith open(file_source, newline='', encoding='utf-8') as tsvfile:\n    reader = csv.DictReader(tsvfile, delimiter='\\t')\n    \n\n    for row in reader:\n        file_path = os.path.join(images_path, row[\"file_name\"])\n\n        if os.path.exists(file_path):\n            data.append(row)\n    \n    split = int(len(data)*0.8)\n\n    with open(file_train, 'w', encoding='utf-8') as jsonfile:\n        json.dump([], jsonfile, ensure_ascii=False, indent=4)\n    print(f\"File JSON vuoto creato come {file_train}\")\n\n    with open(file_test, 'w', encoding='utf-8') as jsonfile:\n        json.dump([], jsonfile, ensure_ascii=False, indent=4)\n    print(f\"File JSON vuoto creato come {file_test}\")\n    \n    with open(file_train, 'w', encoding='utf-8') as jsonfile:\n        json.dump(data[:split], jsonfile, ensure_ascii=False, indent=4)\n    print(f\"File JSON salvato come {file_train}\")\n        \n    with open(file_test, 'w', encoding='utf-8') as jsonfile:\n        json.dump(data[split:], jsonfile, ensure_ascii=False, indent=4)\n    print(f\"File JSON salvato come {file_test}\")\n'''","metadata":{"execution":{"iopub.status.busy":"2024-07-17T07:52:07.335311Z","iopub.execute_input":"2024-07-17T07:52:07.336185Z","iopub.status.idle":"2024-07-17T07:52:07.344347Z","shell.execute_reply.started":"2024-07-17T07:52:07.336151Z","shell.execute_reply":"2024-07-17T07:52:07.343431Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"'\\nimport csv\\nimport json\\nimport os\\n\\nimages_path = \"/kaggle/input/dataset-wow/MAMI DATASET/MAMI DATASET/training/TRAINING\"\\n\\nfile_source = \\'/kaggle/input/dataset-wow/train_image_text.tsv\\'\\nfile_train = \\'/kaggle/working/train_image_text.json\\'\\nfile_test = \\'/kaggle/working/test_image_text.json\\'\\ndata = []\\n\\nwith open(file_source, newline=\\'\\', encoding=\\'utf-8\\') as tsvfile:\\n    reader = csv.DictReader(tsvfile, delimiter=\\'\\t\\')\\n    \\n\\n    for row in reader:\\n        file_path = os.path.join(images_path, row[\"file_name\"])\\n\\n        if os.path.exists(file_path):\\n            data.append(row)\\n    \\n    split = int(len(data)*0.8)\\n\\n    with open(file_train, \\'w\\', encoding=\\'utf-8\\') as jsonfile:\\n        json.dump([], jsonfile, ensure_ascii=False, indent=4)\\n    print(f\"File JSON vuoto creato come {file_train}\")\\n\\n    with open(file_test, \\'w\\', encoding=\\'utf-8\\') as jsonfile:\\n        json.dump([], jsonfile, ensure_ascii=False, indent=4)\\n    print(f\"File JSON vuoto creato come {file_test}\")\\n    \\n    with open(file_train, \\'w\\', encoding=\\'utf-8\\') as jsonfile:\\n        json.dump(data[:split], jsonfile, ensure_ascii=False, indent=4)\\n    print(f\"File JSON salvato come {file_train}\")\\n        \\n    with open(file_test, \\'w\\', encoding=\\'utf-8\\') as jsonfile:\\n        json.dump(data[split:], jsonfile, ensure_ascii=False, indent=4)\\n    print(f\"File JSON salvato come {file_test}\")\\n'"},"metadata":{}}]},{"cell_type":"markdown","source":"## Multimodal Dataset definition","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import Dataset\nimport os\nimport wandb\n\nclass MultimodalDataset(Dataset): # Dataset for handling multimodal data\n    def __init__(self, images_dir, json_file_path): # dir_path -> directory path where images are stored / json_file_path -> file path for metadata (including labels)   \n        file_paths, text_list, labels_misogyny, shaming_label_list, stereotype_label_list, objectification_label_list, violence_label_list = load_json_file(json_file_path)\n   \n        self.file_paths = file_paths\n        self.images_dir = images_dir\n        self.text_list = text_list\n        self.labels_misogyny = labels_misogyny\n        self.shaming_label_list = shaming_label_list\n        self.stereotype_label_list = stereotype_label_list\n        self.objectification_label_list = objectification_label_list\n        self.violence_label_list = violence_label_list\n        \n    def __len__(self):\n        return len(self.file_paths)\n    \n    def __getitem__(self, idx):\n        return self.file_paths[idx], self.text_list[idx], self.labels_misogyny[idx], self.shaming_label_list[idx], self.stereotype_label_list[idx], self.objectification_label_list[idx], self.violence_label_list[idx]\n    \ndef load_json_file(json_file_path):\n    with open(json_file_path,\"r\") as f:\n        data = json.load(f)\n        text_list = [] # list of the text related to each image\n        image_list = [] # list of images path\n        labels_misogyny = [] # list of TASK A labels (misogyny classification)\n\n        ### list of TASK B labels ###\n        shaming_label_list = [] \n        stereotype_label_list = []\n        objectification_label_list = []\n        violence_label_list = []\n\n\n        for item in tqdm(data): \n            image_list.append(item['file_name'])\n            text_list.append(item[\"text\"])\n            labels_misogyny.append(float(item[\"label\"]))\n            shaming_label_list.append(float(item[\"shaming\"]))\n            stereotype_label_list.append(float(item[\"stereotype\"]))\n            objectification_label_list.append(float(item[\"objectification\"]))\n            violence_label_list.append(float(item[\"violence\"]))\n\n        #print(f\"{type(labels_misogyny)}\")\n        return image_list, text_list, torch.tensor(labels_misogyny, dtype=torch.float32), torch.tensor(shaming_label_list, dtype=torch.float32), torch.tensor(stereotype_label_list, dtype=torch.float32), torch.tensor(objectification_label_list, dtype=torch.float32), torch.tensor(violence_label_list, dtype=torch.float32)\n    \ndef accuracy(preds, labels, thresh):\n    num_samples = labels.shape[0]\n    preds = preds > thresh\n    matching_rows = torch.eq(labels.bool(), preds)\n    \n    # in case we're dealing with the prediction of task B/task A (they've different number of dimensions)\n    num_correct = matching_rows.all(dim=1).sum().item() if preds.ndim!=1 else matching_rows.sum().item()\n    return 100*(num_correct/num_samples)","metadata":{"execution":{"iopub.status.busy":"2024-07-17T11:59:21.122458Z","iopub.execute_input":"2024-07-17T11:59:21.122741Z","iopub.status.idle":"2024-07-17T11:59:22.650413Z","shell.execute_reply.started":"2024-07-17T11:59:21.122716Z","shell.execute_reply":"2024-07-17T11:59:22.649575Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## LOADING THE DATASET AND TRAINING THE NETWORK","metadata":{}},{"cell_type":"code","source":"### import torch\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import CLIPProcessor, CLIPModel, CLIPTokenizer\nfrom PIL import Image # per debug\nimport os\nimport json\nfrom tqdm import tqdm\nfrom PIL import Image\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport wandb\n\n\n\nclass Trainer():\n    def __init__(self, \n                model,     \n                train_images_dir,\n                test_image_dir,\n                json_train_path,\n                json_test_path,\n                single_task_mode=True,\n                train_data_split=0.8,\n                batch_size=256, \n                lr=0.001, \n                num_epochs=15,\n                threshold=0.5,\n                weight_taskA=0.7,\n                weight_taskB=0.3):\n        \n        # Check if CUDA is available\n        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n        print(f\"Training on: {self.device}\")\n        \n        # Loading the Dataset\n        self.train_images_dir = train_images_dir\n        self.test_image_dir = test_image_dir\n        self.num_epochs = num_epochs\n        self.threshold = threshold\n        self.weight_taskA = weight_taskA\n        self.single_task_mode=single_task_mode\n        if not self.single_task_mode:\n            self.weight_taskB = weight_taskB\n        \n        train_data = MultimodalDataset(train_images_dir, json_train_path)\n        test_data = MultimodalDataset(test_image_dir, json_test_path)\n        \n        print(f\"training on samples:{test_data.__len__()}\")\n        print(f\"testing on samples:{test_data.__len__()}\")\n        if not self.single_task_mode:\n            print(f\"training model in single task mode: Misoginy Identification\")\n        else:\n            print(f\"training model in dual task mode: Misoginy Identification and Misoginy Classification\")\n\n    \n        self.train_dataloader = DataLoader(train_data, batch_size, shuffle=True, pin_memory=True)\n        self.test_dataloader = DataLoader(test_data, batch_size, shuffle=True, pin_memory=True)\n\n        # Defining the Model\n        self.classifier = model.to(self.device)\n        self.optimizer = optim.Adam(self.classifier.parameters(), lr)\n        self.loss_taskA = F.binary_cross_entropy \n        if not self.single_task_mode:\n            self.loss_taskB = F.binary_cross_entropy\n\n        # Pretrained CLIP loading...\n        self.clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\", device_map='cuda')\n        self.clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n        \n        \n    def get_random_test_sample(self):\n        if not self.single_task_mode:\n            batch = next(iter(test_loader))\n            image,image_text,_,_,_,_,_ = batch\n            return images\n        \n    def train_model(self):\n        for epoch in range(self.num_epochs):\n            print(f'Epoch [{epoch+1}/{self.num_epochs}]')\n            if not self.single_task_mode:\n                train_loss_list, test_loss_list, train_acc_taskA_list, train_acc_taskB_list,  test_acc_taskA_list, test_acc_taskB_list= self.train_epoch()\n            else:\n                train_loss_list, test_loss_list, train_acc_taskA_list, test_acc_taskA_list= self.train_epoch()\n            print(f'Average Train Loss: {sum(train_loss_list) / len(train_loss_list): .4f}, Average Test Loss: {sum(test_loss_list) / len(test_loss_list): .4f}')\n            print(f'Average Accuracy Train (task A): {sum(train_acc_taskA_list) / len(train_acc_taskA_list): .4f}%, Average Accuracy Test (task A): {sum(test_acc_taskA_list) / len(test_acc_taskA_list): .4f}%')\n            if not self.single_task_mode:\n                print(f'Average Accuracy Train (task B): {sum(train_acc_taskB_list) / len(train_acc_taskB_list): .4f}%, Average Accuracy Test (task B): {sum(test_acc_taskB_list) / len(test_acc_taskB_list): .4f}%')\n\n            model_path = '/kaggle/working/model_' + str(epoch+1) + '.pth'\n            torch.save(self.classifier.state_dict(), model_path);\n            \n            \n\n    def train_epoch(self):\n        train_loss_list = []\n        test_loss_list = []\n        train_acc_taskA_list = []\n        train_acc_taskB_list = []\n        test_acc_taskA_list = []\n        test_acc_taskB_list = []\n        \n        for batch in tqdm(self.train_dataloader):\n            self.optimizer.zero_grad() # ZEROING OUT THE GRADIENTS\n            self.classifier.train() # TRAINING MODE\n            \n            # CREATING THE CLIP EMBEDDINGS\n            image_list, text_list, labels_misogyny, shaming_labels, stereotype_labels, objectification_labels, violence_labels = batch\n            \n            image_list = [Image.open(f\"{os.path.join(self.train_images_dir, img)}\") for img in image_list] # per poterlo usare poi con CLIP\n            labels_misogyny = labels_misogyny.to(self.device)\n            labels_taskB = torch.stack([shaming_labels, stereotype_labels, objectification_labels, violence_labels],  dim=1).to(self.device)\n\n            clip_inputs = self.clip_processor(text=text_list, images=image_list, return_tensors=\"pt\", padding=True, truncation=True)\n            clip_inputs['input_ids'] = clip_inputs['input_ids'].to(self.device)\n            clip_inputs['attention_mask'] = clip_inputs['attention_mask'].to(self.device)\n            clip_inputs['pixel_values'] = clip_inputs['pixel_values'].to(self.device)\n    \n            clip_outputs = self.clip_model(**clip_inputs)\n            model_input = torch.cat([clip_outputs['text_embeds'], clip_outputs['image_embeds']], dim=1).to(self.device)\n    \n    \n            if not self.single_task_mode:\n                pred_taskA, pred_taskB = self.classifier(model_input)\n            else:\n                pred_taskA = self.classifier(model_input)\n\n            loss_A = self.loss_taskA(pred_taskA, labels_misogyny)\n            if not self.single_task_mode:\n                loss_B = self.loss_taskB(pred_taskB, labels_taskB, reduction='mean')\n                loss = (self.weight_taskA * loss_A) + (self.weight_taskB * loss_B)\n            else:\n                loss= loss_A\n            train_loss_list.append(loss)\n            \n            loss.backward()\n            self.optimizer.step()\n\n            accuracy_taskA = accuracy(pred_taskA, labels_misogyny, self.threshold)\n            if not self.single_task_mode:\n                accuracy_taskB = accuracy(pred_taskB, labels_taskB, self.threshold)\n            \n            train_acc_taskA_list.append(accuracy_taskA)\n            if not self.single_task_mode:\n                train_acc_taskB_list.append(accuracy_taskB)\n        \n        \n        with torch.no_grad():\n            self.classifier.eval()\n\n            for batch in tqdm(self.test_dataloader):\n                # CREATING THE CLIP EMBEDDINGS\n                image_list, text_list, labels_misogyny, shaming_labels, stereotype_labels, objectification_labels, violence_labels = batch\n\n                image_list = [Image.open(f\"{os.path.join(self.test_image_dir, img)}\") for img in image_list] # per poterlo usare poi con CLIP\n                labels_misogyny = labels_misogyny.to(self.device)\n                labels_taskB = torch.stack([shaming_labels, stereotype_labels, objectification_labels, violence_labels],  dim=1).to(self.device)\n\n                clip_inputs = self.clip_processor(text=text_list, images=image_list, return_tensors=\"pt\", padding=True, truncation=True)\n                clip_inputs['input_ids'] = clip_inputs['input_ids'].to(self.device)\n                clip_inputs['attention_mask'] = clip_inputs['attention_mask'].to(self.device)\n                clip_inputs['pixel_values'] = clip_inputs['pixel_values'].to(self.device)\n\n                clip_outputs = self.clip_model(**clip_inputs)\n                model_input = torch.cat([clip_outputs['text_embeds'], clip_outputs['image_embeds']], dim=1).to(self.device)\n                \n                if not self.single_task_mode:\n                    pred_taskA, pred_taskB = self.classifier(model_input)\n                else:\n                    pred_taskA = self.classifier(model_input)\n\n                loss_A = self.loss_taskA(pred_taskA, labels_misogyny)\n                if not self.single_task_mode:\n                    loss_B = self.loss_taskB(pred_taskB, labels_taskB, reduction='mean')\n                    loss = (self.weight_taskA * loss_A) + (self.weight_taskB * loss_B)\n                else:\n                    loss=loss_A\n                test_loss_list.append(loss)\n\n                accuracy_taskA = accuracy(pred_taskA, labels_misogyny, self.threshold)\n                if not self.single_task_mode:\n                    accuracy_taskB = accuracy(pred_taskB, labels_taskB, self.threshold)\n\n                test_acc_taskA_list.append(accuracy_taskA)\n                \n                if not self.single_task_mode:\n                    test_acc_taskB_list.append(accuracy_taskB)\n\n        if not self.single_task_mode:\n            return train_loss_list, test_loss_list, train_acc_taskA_list, train_acc_taskB_list, test_acc_taskA_list, test_acc_taskB_list\n        else:\n            return train_loss_list, test_loss_list, train_acc_taskA_list,  test_acc_taskA_list, \n            ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = MisogynyCls(5)","metadata":{"execution":{"iopub.status.busy":"2024-07-17T12:00:50.658527Z","iopub.execute_input":"2024-07-17T12:00:50.659067Z","iopub.status.idle":"2024-07-17T12:00:50.726587Z","shell.execute_reply.started":"2024-07-17T12:00:50.659037Z","shell.execute_reply":"2024-07-17T12:00:50.725769Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"\nmodel_trainer = Trainer(model, # model \n                        \"/kaggle/input/dataset-wow/MAMI DATASET/MAMI DATASET/training/TRAINING\",# train_images_dir\n                        \"/kaggle/input/dataset-wow/MAMI DATASET/MAMI DATASET/test\", #test_images_dir\n                        \"/kaggle/working/train_image_text.json\",\n                        \"/kaggle/working/test_image_text.json\",\n                        batch_size=64,\n                        weight_taskA=1,\n                        weight_taskB=1) # json_file as data source\n\n\nmodel_trainer.train_model()","metadata":{"execution":{"iopub.status.busy":"2024-07-17T12:00:53.142433Z","iopub.execute_input":"2024-07-17T12:00:53.142788Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Training on: cuda:0\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 9000/9000 [00:00<00:00, 468323.36it/s]\n100%|██████████| 1000/1000 [00:00<00:00, 402331.32it/s]","output_type":"stream"},{"name":"stdout","text":"training on samples:1000\ntesting on samples:1000\ntraining model in dual task mode: Misoginy Identification and Misoginy Classification\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/4.19k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e6b29f4bc27e4120ad9fcb56c54b55e1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/605M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d06d233ac09b41ff8fc7ea1c74e3ad74"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\n2024-07-17 12:01:26.652263: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-17 12:01:26.652372: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-17 12:01:26.950636: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"222798f3c72d430eaad64fde798aafef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/592 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ec0e012b72147f09392e296b339081e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/862k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce1c683aeb054a2e9983c9adf999886b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/525k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"904906967a284063ae8c60ee065c28de"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.22M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed0d5cc09f8142f5ad4d021e85745222"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab9a902a05c844edb22fa4b4000471e3"}},"metadata":{}},{"name":"stdout","text":"Epoch [1/15]\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 141/141 [04:48<00:00,  2.04s/it]\n100%|██████████| 16/16 [00:21<00:00,  1.37s/it]\n","output_type":"stream"},{"name":"stdout","text":"Average Train Loss:  0.4225, Average Test Loss:  0.7149\nAverage Accuracy Train (task A):  80.6117%, Average Accuracy Test (task A):  72.7930%\nEpoch [2/15]\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 141/141 [04:00<00:00,  1.70s/it]\n100%|██████████| 16/16 [00:16<00:00,  1.04s/it]\n","output_type":"stream"},{"name":"stdout","text":"Average Train Loss:  0.3208, Average Test Loss:  0.5943\nAverage Accuracy Train (task A):  86.0926%, Average Accuracy Test (task A):  72.7734%\nEpoch [3/15]\n","output_type":"stream"},{"name":"stderr","text":" 90%|█████████ | 127/141 [03:37<00:24,  1.72s/it]","output_type":"stream"}]},{"cell_type":"markdown","source":"Shap Single Modality","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
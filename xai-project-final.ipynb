{"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","include_colab_link":true,"provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8823503,"sourceType":"datasetVersion","datasetId":5308380},{"sourceId":8853110,"sourceType":"datasetVersion","datasetId":5321593}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#!pip install --upgrade transformers","metadata":{"id":"ClPn9DhYJM8u","outputId":"fabbbf4c-a592-43fa-c00a-9ba78826e74f","execution":{"iopub.status.busy":"2024-07-02T23:53:30.674350Z","iopub.execute_input":"2024-07-02T23:53:30.674712Z","iopub.status.idle":"2024-07-02T23:53:30.679399Z","shell.execute_reply.started":"2024-07-02T23:53:30.674679Z","shell.execute_reply":"2024-07-02T23:53:30.678500Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"#!pip install wandb\n","metadata":{"execution":{"iopub.status.busy":"2024-07-02T23:53:30.680511Z","iopub.execute_input":"2024-07-02T23:53:30.680783Z","iopub.status.idle":"2024-07-02T23:53:30.690960Z","shell.execute_reply.started":"2024-07-02T23:53:30.680752Z","shell.execute_reply":"2024-07-02T23:53:30.690072Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"#!wandb login\n","metadata":{"execution":{"iopub.status.busy":"2024-07-02T23:53:30.692088Z","iopub.execute_input":"2024-07-02T23:53:30.692351Z","iopub.status.idle":"2024-07-02T23:53:30.700520Z","shell.execute_reply.started":"2024-07-02T23:53:30.692328Z","shell.execute_reply":"2024-07-02T23:53:30.699680Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"!rm -rf /kaggle/working/*","metadata":{"execution":{"iopub.status.busy":"2024-07-03T23:02:01.910388Z","iopub.execute_input":"2024-07-03T23:02:01.910780Z","iopub.status.idle":"2024-07-03T23:02:02.897623Z","shell.execute_reply.started":"2024-07-03T23:02:01.910751Z","shell.execute_reply":"2024-07-03T23:02:02.896337Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"from torch import nn\n\n\nclass MisogynyCls(nn.Module):\n    def __init__(self, num_linear_layers, task_a_out=1, task_b_out=4, input_dim=1024, hidden_dim=512, drop_value=0.2):\n        super().__init__()\n        self.head_task_a = nn.Linear(hidden_dim, task_a_out)\n        self.head_task_b = nn.Linear(hidden_dim, task_b_out)\n        self.sigmoid = nn.Sigmoid()\n\n\n        self.layers = nn.ModuleList()\n\n        for i in range(num_linear_layers):\n            if i == 0:\n                self.layers.append(nn.Linear(input_dim, hidden_dim))\n            else:\n                self.layers.append(nn.Linear(hidden_dim, hidden_dim))\n                \n            self.layers.append(nn.Dropout(drop_value))\n            self.layers.append(nn.ReLU())\n\n    def forward(self, x):\n        for layer in self.layers:\n            x = layer(x)\n        pred_taskA = self.sigmoid(self.head_task_a(x))\n        pred_taskB = self.sigmoid(self.head_task_b(x))\n        \n        return pred_taskA.squeeze(1), pred_taskB","metadata":{"id":"ZRBdpGpDJqv0","execution":{"iopub.status.busy":"2024-07-03T23:02:18.510806Z","iopub.execute_input":"2024-07-03T23:02:18.511217Z","iopub.status.idle":"2024-07-03T23:02:18.520851Z","shell.execute_reply.started":"2024-07-03T23:02:18.511182Z","shell.execute_reply":"2024-07-03T23:02:18.519960Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"## IN CASE WE NEED TO TRANSFORM A TSV/CSV INTO A JSON FORMAT (last two cells are only for testing purposese... the first instead is the one needed for the conversion from tsv to json)","metadata":{}},{"cell_type":"code","source":"import csv\nimport json\nimport os\n\nimages_path = \"/kaggle/input/dataset-wow/MAMI DATASET/MAMI DATASET/training/TRAINING\"\n\nfile_and_dest = [('/kaggle/input/dataset-wow/train_image_text.tsv','/kaggle/working/train_image_text.json'),\n                    ('/kaggle/input/dataset-wow/test_image_text.tsv','/kaggle/working/test_image_text.json')]\n\n\nfor file in file_and_dest: \n    data = []\n\n    with open(file[0], newline='', encoding='utf-8') as tsvfile:\n        reader = csv.DictReader(tsvfile, delimiter='\\t')\n        \n        for row in reader:\n            data.append(row)\n                \n    if not os.path.exists(file[1]):\n        with open(file[1], 'w', encoding='utf-8') as jsonfile:\n            json.dump([], jsonfile, ensure_ascii=False, indent=4)\n        print(f\"File JSON vuoto creato come {file[1]}\")\n\n        with open(file[1], 'w', encoding='utf-8') as jsonfile:\n            json.dump(data, jsonfile, ensure_ascii=False, indent=4)\n\n        print(f\"File JSON salvato come {file[1]}\")\n\n","metadata":{"execution":{"iopub.status.busy":"2024-07-03T23:02:30.856249Z","iopub.execute_input":"2024-07-03T23:02:30.857087Z","iopub.status.idle":"2024-07-03T23:02:31.101755Z","shell.execute_reply.started":"2024-07-03T23:02:30.857052Z","shell.execute_reply":"2024-07-03T23:02:31.100860Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"File JSON vuoto creato come /kaggle/working/train_image_text.json\nFile JSON salvato come /kaggle/working/train_image_text.json\nFile JSON vuoto creato come /kaggle/working/test_image_text.json\nFile JSON salvato come /kaggle/working/test_image_text.json\n","output_type":"stream"}]},{"cell_type":"code","source":"'''\nimport csv\nimport json\nimport os\n\nimages_path = \"/kaggle/input/dataset-wow/MAMI DATASET/MAMI DATASET/training/TRAINING\"\n\nfile_source = '/kaggle/input/dataset-wow/train_image_text.tsv'\nfile_train = '/kaggle/working/train_image_text.json'\nfile_test = '/kaggle/working/test_image_text.json'\ndata = []\n\nwith open(file_source, newline='', encoding='utf-8') as tsvfile:\n    reader = csv.DictReader(tsvfile, delimiter='\\t')\n    \n\n    for row in reader:\n        file_path = os.path.join(images_path, row[\"file_name\"])\n\n        if os.path.exists(file_path):\n            data.append(row)\n    \n    split = int(len(data)*0.8)\n\n    with open(file_train, 'w', encoding='utf-8') as jsonfile:\n        json.dump([], jsonfile, ensure_ascii=False, indent=4)\n    print(f\"File JSON vuoto creato come {file_train}\")\n\n    with open(file_test, 'w', encoding='utf-8') as jsonfile:\n        json.dump([], jsonfile, ensure_ascii=False, indent=4)\n    print(f\"File JSON vuoto creato come {file_test}\")\n    \n    with open(file_train, 'w', encoding='utf-8') as jsonfile:\n        json.dump(data[:split], jsonfile, ensure_ascii=False, indent=4)\n    print(f\"File JSON salvato come {file_train}\")\n        \n    with open(file_test, 'w', encoding='utf-8') as jsonfile:\n        json.dump(data[split:], jsonfile, ensure_ascii=False, indent=4)\n    print(f\"File JSON salvato come {file_test}\")\n'''","metadata":{"execution":{"iopub.status.busy":"2024-07-03T21:31:17.552963Z","iopub.execute_input":"2024-07-03T21:31:17.553608Z","iopub.status.idle":"2024-07-03T21:31:47.271621Z","shell.execute_reply.started":"2024-07-03T21:31:17.553577Z","shell.execute_reply":"2024-07-03T21:31:47.270575Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"File JSON vuoto creato come /kaggle/working/train_image_text.json\nFile JSON vuoto creato come /kaggle/working/test_image_text.json\nFile JSON salvato come /kaggle/working/train_image_text.json\nFile JSON salvato come /kaggle/working/test_image_text.json\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Multimodal Dataset definition","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import Dataset\nimport os\nimport wandb\n\nclass MultimodalDataset(Dataset): # Dataset for handling multimodal data\n    def __init__(self, images_dir, json_file_path): # dir_path -> directory path where images are stored / json_file_path -> file path for metadata (including labels)   \n        file_paths, text_list, labels_misogyny, shaming_label_list, stereotype_label_list, objectification_label_list, violence_label_list = load_json_file(json_file_path)\n   \n        self.file_paths = file_paths\n        self.images_dir = images_dir\n        self.text_list = text_list\n        self.labels_misogyny = labels_misogyny\n        self.shaming_label_list = shaming_label_list\n        self.stereotype_label_list = stereotype_label_list\n        self.objectification_label_list = objectification_label_list\n        self.violence_label_list = violence_label_list\n        \n    def __len__(self):\n        return len(self.file_paths)\n    \n    def __getitem__(self, idx):\n        return self.file_paths[idx], self.text_list[idx], self.labels_misogyny[idx], self.shaming_label_list[idx], self.stereotype_label_list[idx], self.objectification_label_list[idx], self.violence_label_list[idx]\n    \ndef load_json_file(json_file_path):\n    with open(json_file_path,\"r\") as f:\n        data = json.load(f)\n        text_list = [] # list of the text related to each image\n        image_list = [] # list of images path\n        labels_misogyny = [] # list of TASK A labels (misogyny classification)\n\n        ### list of TASK B labels ###\n        shaming_label_list = [] \n        stereotype_label_list = []\n        objectification_label_list = []\n        violence_label_list = []\n\n\n        for item in tqdm(data): \n            image_list.append(item['file_name'])\n            text_list.append(item[\"text\"])\n            labels_misogyny.append(float(item[\"label\"]))\n            shaming_label_list.append(float(item[\"shaming\"]))\n            stereotype_label_list.append(float(item[\"stereotype\"]))\n            objectification_label_list.append(float(item[\"objectification\"]))\n            violence_label_list.append(float(item[\"violence\"]))\n\n        #print(f\"{type(labels_misogyny)}\")\n        return image_list, text_list, torch.tensor(labels_misogyny, dtype=torch.float32), torch.tensor(shaming_label_list, dtype=torch.float32), torch.tensor(stereotype_label_list, dtype=torch.float32), torch.tensor(objectification_label_list, dtype=torch.float32), torch.tensor(violence_label_list, dtype=torch.float32)\n    \ndef accuracy(preds, labels, thresh):\n    num_samples = labels.shape[0]\n    preds = preds > thresh\n    matching_rows = torch.eq(labels.bool(), preds)\n    \n    # in case we're dealing with the prediction of task B/task A (they've different number of dimensions)\n    num_correct = matching_rows.all(dim=1).sum().item() if preds.ndim!=1 else matching_rows.sum().item()\n    return 100*(num_correct/num_samples)","metadata":{"execution":{"iopub.status.busy":"2024-07-03T23:03:25.903442Z","iopub.execute_input":"2024-07-03T23:03:25.904138Z","iopub.status.idle":"2024-07-03T23:03:26.883569Z","shell.execute_reply.started":"2024-07-03T23:03:25.904101Z","shell.execute_reply":"2024-07-03T23:03:26.882593Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"## LOADING THE DATASET AND TRAINING THE NETWORK\n# Thanks to a class used for handling the argument of the network training (still room for modifying it !)","metadata":{}},{"cell_type":"code","source":"### import torch\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import CLIPProcessor, CLIPModel, CLIPTokenizer\nfrom PIL import Image # per debug\nimport os\nimport json\nfrom tqdm import tqdm\nfrom PIL import Image\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport wandb\n\n\n\nclass Trainer():\n    def __init__(self, train_images_dir,\n                       test_image_dir,\n                       json_train_path,\n                       json_test_path,\n                       train_data_split=0.8,\n                       batch_size=256, \n                       lr=0.001, \n                       num_epochs=15,\n                       threshold=0.5,\n                       weight_taskA=0.7,\n                       weight_taskB=0.3):\n        \n        # Check if CUDA is available\n        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n        print(f\"Training on: {self.device}\")\n        \n        # Loading the Dataset\n        self.train_images_dir = train_images_dir\n        self.test_image_dir = test_image_dir\n        self.num_epochs = num_epochs\n        self.threshold = threshold\n        self.weight_taskA = weight_taskA\n        self.weight_taskB = weight_taskB\n        \n        train_data = MultimodalDataset(train_images_dir, json_train_path)\n        test_data = MultimodalDataset(test_image_dir, json_test_path)\n\n        self.train_dataloader = DataLoader(train_data, batch_size, shuffle=True, pin_memory=True)\n        self.test_dataloader = DataLoader(test_data, batch_size, shuffle=True, pin_memory=True)\n\n        # Defining the Model\n        self.classifier = MisogynyCls(5).to(self.device)\n        self.optimizer = optim.Adam(self.classifier.parameters(), lr)\n        self.loss_taskA = F.binary_cross_entropy \n        self.loss_taskB = F.binary_cross_entropy\n\n        # Pretrained CLIP loading...\n        self.clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\", device_map='cuda')\n        self.clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n    \n        '''\n        wandb.init(  # set the wandb project where this run will be logged\n                    project=\"Multimodal_xai\", \n                    # track hyperparameters and run metadata   \n                    config={\n                        #\"architecture\": args.model,    \"optimizer\": \"SGD\" if args.use_sgd else \"ADAM\",\n                        #\"scheduler\" : args.scheduler,    \"learning_rate\": args.lr * 100 if args.use_sgd else args.lr,\n                        #\"min_learning_rate\": args.lrmin,    \"epochs\": args.epochs,\n                        #\"filename\": args.filename,    \"batch\": args.batch_size,\n                        #\"momentum\": args.momentum,    \"weightdecay\": args.wd,\n                        #\"tanh\": args.tanh,    \"invert_lr\": args.invert_lr,\n                        #\"max_learning_rate\": args.lrmax\n                    })\n        '''\n        \n    def train_model(self):\n        for epoch in range(self.num_epochs):\n            print(f'Epoch [{epoch+1}/{self.num_epochs}]')\n            train_loss_list, test_loss_list, train_acc_taskA_list, train_acc_taskB_list,  test_acc_taskA_list, test_acc_taskB_list= self.train_epoch()\n            \n            print(f'Average Train Loss: {sum(train_loss_list) / len(train_loss_list): .4f}, Average Test Loss: {sum(test_loss_list) / len(test_loss_list): .4f}')\n            print(f'Average Accuracy Train (task A): {sum(train_acc_taskA_list) / len(train_acc_taskA_list): .4f}%, Average Accuracy Test (task A): {sum(test_acc_taskA_list) / len(test_acc_taskA_list): .4f}%')\n            print(f'Average Accuracy Train (task B): {sum(train_acc_taskB_list) / len(train_acc_taskB_list): .4f}%, Average Accuracy Test (task B): {sum(test_acc_taskB_list) / len(test_acc_taskB_list): .4f}%')\n\n            model_path = '/kaggle/working/model_' + str(epoch+1) + '.pth'\n            torch.save(self.classifier.state_dict(), model_path);\n            \n            \n\n    def train_epoch(self):\n        train_loss_list = []\n        test_loss_list = []\n        train_acc_taskA_list = []\n        train_acc_taskB_list = []\n        test_acc_taskA_list = []\n        test_acc_taskB_list = []\n        \n        for batch in tqdm(self.train_dataloader):\n            self.optimizer.zero_grad() # ZEROING OUT THE GRADIENTS\n            self.classifier.train() # TRAINING MODE\n            \n            # CREATING THE CLIP EMBEDDINGS\n            image_list, text_list, labels_misogyny, shaming_labels, stereotype_labels, objectification_labels, violence_labels = batch\n            \n            image_list = [Image.open(f\"{os.path.join(self.train_images_dir, img)}\") for img in image_list] # per poterlo usare poi con CLIP\n            labels_misogyny = labels_misogyny.to(self.device)\n            labels_taskB = torch.stack([shaming_labels, stereotype_labels, objectification_labels, violence_labels],  dim=1).to(self.device)\n\n            clip_inputs = self.clip_processor(text=text_list, images=image_list, return_tensors=\"pt\", padding=True, truncation=True)\n            clip_inputs['input_ids'] = clip_inputs['input_ids'].to(self.device)\n            clip_inputs['attention_mask'] = clip_inputs['attention_mask'].to(self.device)\n            clip_inputs['pixel_values'] = clip_inputs['pixel_values'].to(self.device)\n    \n            clip_outputs = self.clip_model(**clip_inputs)\n            model_input = torch.cat([clip_outputs['text_embeds'], clip_outputs['image_embeds']], dim=1).to(self.device)\n    \n            pred_taskA, pred_taskB = self.classifier(model_input)\n\n            loss_A = self.loss_taskA(pred_taskA, labels_misogyny)\n            loss_B = self.loss_taskB(pred_taskB, labels_taskB, reduction='mean')\n            loss = (self.weight_taskA * loss_A) + (self.weight_taskB * loss_B)\n            train_loss_list.append(loss)\n            \n            loss.backward()\n            self.optimizer.step()\n\n            accuracy_taskA = accuracy(pred_taskA, labels_misogyny, self.threshold)\n            accuracy_taskB = accuracy(pred_taskB, labels_taskB, self.threshold)\n            \n            train_acc_taskA_list.append(accuracy_taskA)\n            train_acc_taskB_list.append(accuracy_taskB)\n        \n        \n        with torch.no_grad():\n            self.classifier.eval()\n\n            for batch in tqdm(self.test_dataloader):\n                # CREATING THE CLIP EMBEDDINGS\n                image_list, text_list, labels_misogyny, shaming_labels, stereotype_labels, objectification_labels, violence_labels = batch\n\n                image_list = [Image.open(f\"{os.path.join(self.test_images_dir, img)}\") for img in image_list] # per poterlo usare poi con CLIP\n                labels_misogyny = labels_misogyny.to(self.device)\n                labels_taskB = torch.stack([shaming_labels, stereotype_labels, objectification_labels, violence_labels],  dim=1).to(self.device)\n\n                clip_inputs = self.clip_processor(text=text_list, images=image_list, return_tensors=\"pt\", padding=True, truncation=True)\n                clip_inputs['input_ids'] = clip_inputs['input_ids'].to(self.device)\n                clip_inputs['attention_mask'] = clip_inputs['attention_mask'].to(self.device)\n                clip_inputs['pixel_values'] = clip_inputs['pixel_values'].to(self.device)\n\n                clip_outputs = self.clip_model(**clip_inputs)\n                model_input = torch.cat([clip_outputs['text_embeds'], clip_outputs['image_embeds']], dim=1).to(self.device)\n\n                pred_taskA, pred_taskB = self.classifier(model_input)\n\n                loss_A = self.loss_taskA(pred_taskA, labels_misogyny)\n                loss_B = self.loss_taskB(pred_taskB, labels_taskB, reduction='mean')\n                loss = (self.weight_taskA * loss_A) + (self.weight_taskB * loss_B)\n                test_loss_list.append(loss)\n\n                accuracy_taskA = accuracy(pred_taskA, labels_misogyny, self.threshold)\n                accuracy_taskB = accuracy(pred_taskB, labels_taskB, self.threshold)\n\n                test_acc_taskA_list.append(accuracy_taskA)\n                test_acc_taskB_list.append(accuracy_taskB)\n\n            \n            '''\n            wandb.log({\"accuracy_taskA\": accuracy_taskA,\n                       \"accuracy_taskB\": accuracy_taskB,\n                       \"loss_A\": loss_A.item(),\n                       \"loss_B\": loss_B.item(),\n                       \"learning_rate\": self.optimizer.param_groups[0]['lr']})\n            \n            '''\n            \n        return train_loss_list, test_loss_list, train_acc_taskA_list, train_acc_taskB_list, test_acc_taskA_list, test_acc_taskB_list\n\n            ","metadata":{"execution":{"iopub.status.busy":"2024-07-03T23:08:48.581284Z","iopub.execute_input":"2024-07-03T23:08:48.581847Z","iopub.status.idle":"2024-07-03T23:08:48.612115Z","shell.execute_reply.started":"2024-07-03T23:08:48.581811Z","shell.execute_reply":"2024-07-03T23:08:48.611119Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"\nmodel_trainer = Trainer(\"/kaggle/input/dataset-wow/MAMI DATASET/MAMI DATASET/training/TRAINING\",# train_images_dir\n                        \"/kaggle/input/dataset-wow/MAMI DATASET/MAMI DATASET/test\", #test_images_dir\n                        \"/kaggle/working/train_image_text.json\",\n                        \"/kaggle/working/test_image_text.json\",\n                        batch_size=64,\n                        weight_taskA=1,\n                        weight_taskB=1) # json_file as data source\n\n\nmodel_trainer.train_model()","metadata":{"execution":{"iopub.status.busy":"2024-07-03T23:09:41.670808Z","iopub.execute_input":"2024-07-03T23:09:41.671196Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Training on: cuda:0\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 9000/9000 [00:00<00:00, 439439.55it/s]\n100%|██████████| 1000/1000 [00:00<00:00, 345039.82it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/15]\n","output_type":"stream"},{"name":"stderr","text":" 48%|████▊     | 67/141 [01:58<02:11,  1.78s/it]","output_type":"stream"}]}]}
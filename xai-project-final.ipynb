{"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","include_colab_link":true,"provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8823503,"sourceType":"datasetVersion","datasetId":5308380},{"sourceId":8853110,"sourceType":"datasetVersion","datasetId":5321593}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#!pip install --upgrade transformers","metadata":{"id":"ClPn9DhYJM8u","outputId":"fabbbf4c-a592-43fa-c00a-9ba78826e74f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!pip install wandb\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import wandb\n\nwandb.login()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!rm -rf /kaggle/working/*","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch import nn\n\n\nclass MisogynyCls(nn.Module):\n    def __init__(self, num_linear_layers, task_a_out=1, task_b_out=4, input_dim=1024, hidden_dim=512, drop_value=0.2):\n        super().__init__()\n        self.head_task_a = nn.Linear(hidden_dim, task_a_out)\n        self.head_task_b = nn.Linear(hidden_dim, task_b_out)\n        self.sigmoid = nn.Sigmoid()\n        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # Check if CUDA is available\n        \n        # Pretrained CLIP loading...\n        self.clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\", device_map='cuda')\n        self.clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n\n\n        self.layers = nn.ModuleList()\n\n        for i in range(num_linear_layers):\n            if i == 0:\n                self.layers.append(nn.Linear(input_dim, hidden_dim))\n            else:\n                self.layers.append(nn.Linear(hidden_dim, hidden_dim))\n                \n            self.layers.append(nn.BatchNorm1d(hidden_dim))\n            self.layers.append(nn.Dropout(drop_value))\n            self.layers.append(nn.ReLU())\n\n    def forward(self, text_list, image_list):\n        print(f'type of the image: {type(image_list[0])}')\n        print(f'size of the image: {image_list[0].shape}')\n        \n        \n        clip_inputs = self.clip_processor(text=text_list, images=image_list, return_tensors=\"pt\", padding=True, truncation=True)\n        clip_inputs['input_ids'] = clip_inputs['input_ids'].to(self.device)\n        clip_inputs['attention_mask'] = clip_inputs['attention_mask'].to(self.device)\n        clip_inputs['pixel_values'] = clip_inputs['pixel_values'].to(self.device)\n        clip_outputs = self.clip_model(**clip_inputs)\n        \n        x = torch.cat([clip_outputs['text_embeds'], clip_outputs['image_embeds']], dim=1).to(self.device) # model input is the concatenation of the two modalities !\n            \n        for layer in self.layers:\n            x = layer(x)\n            \n        pred_taskA = self.sigmoid(self.head_task_a(x))\n        pred_taskB = self.sigmoid(self.head_task_b(x))\n        \n        return pred_taskA.squeeze(1), pred_taskB","metadata":{"id":"ZRBdpGpDJqv0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## IN CASE WE NEED TO TRANSFORM A TSV/CSV INTO A JSON FORMAT (last two cells are only for testing purposese... the first instead is the one needed for the conversion from tsv to json)","metadata":{}},{"cell_type":"code","source":"import csv\nimport json\nimport os\n\nimages_path = \"/kaggle/input/dataset-wow/MAMI DATASET/MAMI DATASET/training/TRAINING\"\n\nfile_and_dest = [('/kaggle/input/dataset-wow/train_image_text.tsv','/kaggle/working/train_image_text.json'),\n                    ('/kaggle/input/dataset-wow/test_image_text.tsv','/kaggle/working/test_image_text.json')]\n\n\nfor file in file_and_dest: \n    counter = 0\n    data = []\n\n    with open(file[0], newline='', encoding='utf-8') as tsvfile:\n        reader = csv.DictReader(tsvfile, delimiter='\\t')\n        \n        for row in reader:\n            counter += 1\n            data.append(row)\n        print(f\"counter: {counter}\")\n    if not os.path.exists(file[1]):\n        with open(file[1], 'w', encoding='utf-8') as jsonfile:\n            json.dump([], jsonfile, ensure_ascii=False, indent=4)\n        print(f\"File JSON vuoto creato come {file[1]}\")\n\n        with open(file[1], 'w', encoding='utf-8') as jsonfile:\n            json.dump(data, jsonfile, ensure_ascii=False, indent=4)\n\n        print(f\"File JSON salvato come {file[1]}\")\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nimport csv\nimport json\nimport os\n\nimages_path = \"/kaggle/input/dataset-wow/MAMI DATASET/MAMI DATASET/training/TRAINING\"\n\nfile_source = '/kaggle/input/dataset-wow/train_image_text.tsv'\nfile_train = '/kaggle/working/train_image_text.json'\nfile_test = '/kaggle/working/test_image_text.json'\ndata = []\n\nwith open(file_source, newline='', encoding='utf-8') as tsvfile:\n    reader = csv.DictReader(tsvfile, delimiter='\\t')\n    \n\n    for row in reader:\n        file_path = os.path.join(images_path, row[\"file_name\"])\n\n        if os.path.exists(file_path):\n            data.append(row)\n    \n    split = int(len(data)*0.8)\n\n    with open(file_train, 'w', encoding='utf-8') as jsonfile:\n        json.dump([], jsonfile, ensure_ascii=False, indent=4)\n    print(f\"File JSON vuoto creato come {file_train}\")\n\n    with open(file_test, 'w', encoding='utf-8') as jsonfile:\n        json.dump([], jsonfile, ensure_ascii=False, indent=4)\n    print(f\"File JSON vuoto creato come {file_test}\")\n    \n    with open(file_train, 'w', encoding='utf-8') as jsonfile:\n        json.dump(data[:split], jsonfile, ensure_ascii=False, indent=4)\n    print(f\"File JSON salvato come {file_train}\")\n        \n    with open(file_test, 'w', encoding='utf-8') as jsonfile:\n        json.dump(data[split:], jsonfile, ensure_ascii=False, indent=4)\n    print(f\"File JSON salvato come {file_test}\")\n'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Multimodal Dataset definition","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import Dataset\nimport os\nimport wandb\n\nclass MultimodalDataset(Dataset): # Dataset for handling multimodal data\n    def __init__(self, images_dir, json_file_path): # dir_path -> directory path where images are stored / json_file_path -> file path for metadata (including labels)   \n        file_paths, text_list, labels_misogyny, shaming_label_list, stereotype_label_list, objectification_label_list, violence_label_list = load_json_file(json_file_path)\n   \n        self.file_paths = file_paths\n        self.images_dir = images_dir\n        self.text_list = text_list\n        self.labels_misogyny = labels_misogyny\n        self.shaming_label_list = shaming_label_list\n        self.stereotype_label_list = stereotype_label_list\n        self.objectification_label_list = objectification_label_list\n        self.violence_label_list = violence_label_list\n        \n    def __len__(self):\n        return len(self.file_paths)\n    \n    def __getitem__(self, idx):\n        return self.file_paths[idx], self.text_list[idx], self.labels_misogyny[idx], self.shaming_label_list[idx], self.stereotype_label_list[idx], self.objectification_label_list[idx], self.violence_label_list[idx]\n    \ndef load_json_file(json_file_path):\n    with open(json_file_path,\"r\") as f:\n        data = json.load(f)\n        text_list = [] # list of the text related to each image\n        image_list = [] # list of images path\n        labels_misogyny = [] # list of TASK A labels (misogyny classification)\n\n        ### list of TASK B labels ###\n        shaming_label_list = [] \n        stereotype_label_list = []\n        objectification_label_list = []\n        violence_label_list = []\n\n\n        for item in tqdm(data): \n            image_list.append(item['file_name'])\n            text_list.append(item[\"text\"])\n            labels_misogyny.append(float(item[\"label\"]))\n            shaming_label_list.append(float(item[\"shaming\"]))\n            stereotype_label_list.append(float(item[\"stereotype\"]))\n            objectification_label_list.append(float(item[\"objectification\"]))\n            violence_label_list.append(float(item[\"violence\"]))\n\n        #print(f\"{type(labels_misogyny)}\")\n        return image_list, text_list, torch.tensor(labels_misogyny, dtype=torch.float32), torch.tensor(shaming_label_list, dtype=torch.float32), torch.tensor(stereotype_label_list, dtype=torch.float32), torch.tensor(objectification_label_list, dtype=torch.float32), torch.tensor(violence_label_list, dtype=torch.float32)\n    \ndef accuracy(preds, labels, thresh):\n    num_samples = labels.shape[0]\n    preds = preds > thresh\n    matching_rows = torch.eq(labels.bool(), preds)\n    \n    # in case we're dealing with the prediction of task B/task A (they've different number of dimensions)\n    num_correct = matching_rows.all(dim=1).sum().item() if preds.ndim!=1 else matching_rows.sum().item()\n    return 100*(num_correct/num_samples)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## LOADING THE DATASET AND TRAINING THE NETWORK\n# Thanks to a class used for handling the argument of the network training (still room for modifying it !)","metadata":{}},{"cell_type":"code","source":"### import torch\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import CLIPProcessor, CLIPModel, CLIPTokenizer\nfrom PIL import Image # per debug\nimport os\nimport json\nfrom tqdm import tqdm\nfrom PIL import Image\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport wandb\n\n\n\nclass Trainer():\n    def __init__(self, train_images_dir,\n                       test_image_dir,\n                       json_train_path,\n                       json_test_path,\n                       num_linear_layers=5,\n                       drop_value=0.2,\n                       train_data_split=0.8,\n                       batch_size=256, \n                       lr=0.001, \n                       num_epochs=10,\n                       threshold=0.5,\n                       weight_taskA=1,\n                       weight_taskB=1):\n        \n        # Check if CUDA is available\n        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n        print(f\"Training on: {self.device}\")\n        \n        # Loading the Dataset\n        self.train_images_dir = train_images_dir\n        self.test_image_dir = test_image_dir\n        self.num_epochs = num_epochs\n        self.threshold = threshold\n        self.weight_taskA = weight_taskA\n        self.weight_taskB = weight_taskB\n        \n        train_data = MultimodalDataset(train_images_dir, json_train_path)\n        test_data = MultimodalDataset(test_image_dir, json_test_path)\n        \n        print(f\"training on samples:{train_data.__len__()}\")\n        print(f\"testing on samples:{test_data.__len__()}\")\n    \n        self.train_dataloader = DataLoader(train_data, batch_size, shuffle=True, pin_memory=True)\n        self.test_dataloader = DataLoader(test_data, batch_size, shuffle=True, pin_memory=True)\n\n        # Defining the Model\n        self.classifier = MisogynyCls(num_linear_layers=num_linear_layers, drop_value=drop_value).to(self.device)\n        self.optimizer = optim.Adam(self.classifier.parameters(), lr)\n        self.loss_taskA = F.binary_cross_entropy \n        self.loss_taskB = F.binary_cross_entropy\n\n        # Pretrained CLIP loading...\n        #self.clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\", device_map='cuda')\n        #self.clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n        \n        \n    def train_model(self):\n        for epoch in range(self.num_epochs):\n            print(f'Epoch [{epoch+1}/{self.num_epochs}]')\n            train_loss_list, test_loss_list, train_acc_taskA_list, train_acc_taskB_list,  test_acc_taskA_list, test_acc_taskB_list= self.train_epoch()\n            \n            train_loss_avg = sum(train_loss_list) / len(train_loss_list)\n            test_loss_avg = sum(test_loss_list) / len(test_loss_list)\n            train_acc_taskA_avg = sum(train_acc_taskA_list) / len(train_acc_taskA_list)\n            train_acc_taskB_avg = sum(test_acc_taskA_list) / len(test_acc_taskA_list)\n            test_acc_taskA_avg = sum(train_acc_taskB_list) / len(train_acc_taskB_list)\n            test_acc_taskB_avg = sum(test_acc_taskB_list) / len(test_acc_taskB_list)\n            \n            print(f'Average Train Loss: {train_loss_avg: .4f}, Average Test Loss: {test_loss_avg: .4f}')\n            print(f'Average Accuracy Train (task A): {train_acc_taskA_avg: .4f}%, Average Accuracy Test (task A): {train_acc_taskB_avg: .4f}%')\n            print(f'Average Accuracy Train (task B): {test_acc_taskA_avg: .4f}%, Average Accuracy Test (task B): {test_acc_taskB_avg: .4f}%')\n            \n            wandb.log({\"train_loss\": train_loss_avg, \n                       \"test_loss\": test_loss_avg,\n                       \"train_accuracy_taskA\": train_acc_taskA_avg,\n                       \"train_accuracy_taskB\": train_acc_taskB_avg,\n                       \"test_accuracy_taskA\": test_acc_taskA_avg,\n                       \"test_accuracy_taskB\": test_acc_taskB_avg})\n            \n            \n            # DA RIATTIVARE DOPO IL SALVATAGGIO DEL MODELLO\n            \n            #model_path = '/kaggle/working/model_' + str(epoch+1) + '.pth'\n            #torch.save(self.classifier.state_dict(), model_path);\n            \n            \n\n    def train_epoch(self):\n        train_loss_list = []\n        test_loss_list = []\n        train_acc_taskA_list = []\n        train_acc_taskB_list = []\n        test_acc_taskA_list = []\n        test_acc_taskB_list = []\n        \n        for batch in tqdm(self.train_dataloader):\n            self.optimizer.zero_grad() # ZEROING OUT THE GRADIENTS\n            self.classifier.train() # TRAINING MODE\n            \n            # CREATING THE CLIP EMBEDDINGS\n            image_list, text_list, labels_misogyny, shaming_labels, stereotype_labels, objectification_labels, violence_labels = batch\n            image_list = [Image.open(f\"{os.path.join(self.train_images_dir, img)}\") for img in image_list] # per poterlo usare poi con CLIP\n\n            labels_misogyny = labels_misogyny.to(self.device)\n            labels_taskB = torch.stack([shaming_labels, stereotype_labels, objectification_labels, violence_labels],  dim=1).to(self.device)\n    \n            pred_taskA, pred_taskB = self.classifier(text_list, image_list)\n\n            loss_A = self.loss_taskA(pred_taskA, labels_misogyny)\n            loss_B = self.loss_taskB(pred_taskB, labels_taskB, reduction='mean')\n            loss = (self.weight_taskA * loss_A) + (self.weight_taskB * loss_B)\n            train_loss_list.append(loss)\n            \n            loss.backward()\n            self.optimizer.step()\n\n            accuracy_taskA = accuracy(pred_taskA, labels_misogyny, self.threshold)\n            accuracy_taskB = accuracy(pred_taskB, labels_taskB, self.threshold)\n            \n            train_acc_taskA_list.append(accuracy_taskA)\n            train_acc_taskB_list.append(accuracy_taskB)\n        \n        \n        with torch.no_grad():\n            self.classifier.eval()\n\n            for batch in tqdm(self.test_dataloader):\n                # CREATING THE CLIP EMBEDDINGS\n                image_list, text_list, labels_misogyny, shaming_labels, stereotype_labels, objectification_labels, violence_labels = batch\n\n                image_list = [Image.open(f\"{os.path.join(self.test_image_dir, img)}\") for img in image_list] # per poterlo usare poi con CLIP\n                labels_misogyny = labels_misogyny.to(self.device)\n                labels_taskB = torch.stack([shaming_labels, stereotype_labels, objectification_labels, violence_labels],  dim=1).to(self.device)\n                \n                pred_taskA, pred_taskB = self.classifier(text_list, image_list)\n\n                loss_A = self.loss_taskA(pred_taskA, labels_misogyny)\n                loss_B = self.loss_taskB(pred_taskB, labels_taskB, reduction='mean')\n                loss = (self.weight_taskA * loss_A) + (self.weight_taskB * loss_B)\n                test_loss_list.append(loss)\n\n                accuracy_taskA = accuracy(pred_taskA, labels_misogyny, self.threshold)\n                accuracy_taskB = accuracy(pred_taskB, labels_taskB, self.threshold)\n\n                test_acc_taskA_list.append(accuracy_taskA)\n                test_acc_taskB_list.append(accuracy_taskB)\n            \n            \n            \n        return train_loss_list, test_loss_list, train_acc_taskA_list, train_acc_taskB_list, test_acc_taskA_list, test_acc_taskB_list\n\n            ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training WITHOUT wandb","metadata":{}},{"cell_type":"code","source":"\nmodel_trainer = Trainer(\"/kaggle/input/dataset-wow/MAMI DATASET/MAMI DATASET/training/TRAINING\",# train_images_dir\n                        \"/kaggle/input/dataset-wow/MAMI DATASET/MAMI DATASET/test\", #test_images_dir\n                        \"/kaggle/working/train_image_text.json\",\n                        \"/kaggle/working/test_image_text.json\",\n                        batch_size=64,\n                        num_epochs=3) # json_file as data source\n\n\nmodel_trainer.train_model()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training with wandb","metadata":{}},{"cell_type":"code","source":"import wandb\n\n\ndef train(config=None):\n    with wandb.init(config=config):\n    \n        config = wandb.config\n        model_trainer = Trainer(\"/kaggle/input/dataset-wow/MAMI DATASET/MAMI DATASET/training/TRAINING\",# train_images_dir\n                            \"/kaggle/input/dataset-wow/MAMI DATASET/MAMI DATASET/test\", #test_images_dir\n                            \"/kaggle/working/train_image_text.json\",\n                            \"/kaggle/working/test_image_text.json\",\n                            batch_size=64,\n                            num_linear_layers=config.num_layers,\n                            lr=config.learning_rate,\n                            threshold=config.threshold,\n                            drop_value=config.dropout) # json_file as data source\n\n        model_trainer.train_model()\n    \n    \n# Log in to W&B using the API key\nsweep_config = {\n    'method': 'random'\n}\nparameters_dict = {\n    'learning_rate': {\n        'values': [0.1, 0.01, 0.001, 0.0001]\n        },\n    'threshold': {\n          'values': [0.5, 0.6, 0.7, 0.8]\n        },\n    'dropout': {\n          'values': [0.2, 0.3, 0.5]\n        },\n    'num_layers': {\n          'values': [5, 7, 10]\n        },\n    }\n\nsweep_config['parameters'] = parameters_dict\nsweep_id = wandb.sweep(sweep_config, project=\"Multimodal_xai\")   \nwandb.agent(sweep_id, train, count=20)\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}
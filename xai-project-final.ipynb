{"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","include_colab_link":true,"provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8823503,"sourceType":"datasetVersion","datasetId":5308380},{"sourceId":8853110,"sourceType":"datasetVersion","datasetId":5321593}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#!pip install --upgrade transformers","metadata":{"id":"ClPn9DhYJM8u","outputId":"fabbbf4c-a592-43fa-c00a-9ba78826e74f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!pip install wandb\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import wandb\n\nwandb.login()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!rm -rf /kaggle/working/*","metadata":{"execution":{"iopub.status.busy":"2024-07-15T16:10:18.827499Z","iopub.execute_input":"2024-07-15T16:10:18.828418Z","iopub.status.idle":"2024-07-15T16:10:19.886210Z","shell.execute_reply.started":"2024-07-15T16:10:18.828382Z","shell.execute_reply":"2024-07-15T16:10:19.885060Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"}]},{"cell_type":"code","source":"from torch import nn\n\n\nclass MisogynyCls(nn.Module):\n    def __init__(self, num_linear_layers, task_a_out=1, task_b_out=4, input_dim=1024, hidden_dim=512, drop_value=0.2):\n        super().__init__()\n        self.head_task_a = nn.Linear(hidden_dim, task_a_out)\n        self.head_task_b = nn.Linear(hidden_dim, task_b_out)\n        self.sigmoid = nn.Sigmoid()\n        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # Check if CUDA is available\n        \n        # Pretrained CLIP loading...\n        self.clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\", device_map='cuda')\n        self.clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n\n\n        self.layers = nn.ModuleList()\n\n        for i in range(num_linear_layers):\n            if i == 0:\n                self.layers.append(nn.Linear(input_dim, hidden_dim))\n            else:\n                self.layers.append(nn.Linear(hidden_dim, hidden_dim))\n                \n            self.layers.append(nn.BatchNorm1d(hidden_dim))\n            self.layers.append(nn.Dropout(drop_value))\n            self.layers.append(nn.ReLU())\n\n    def forward(self, text_list, image_list):\n        clip_inputs = self.clip_processor(text=text_list, images=image_list, return_tensors=\"pt\", padding=True, truncation=True)\n        clip_inputs['input_ids'] = clip_inputs['input_ids'].to(self.device)\n        clip_inputs['attention_mask'] = clip_inputs['attention_mask'].to(self.device)\n        clip_inputs['pixel_values'] = clip_inputs['pixel_values'].to(self.device)\n        clip_outputs = self.clip_model(**clip_inputs)\n        \n        x = torch.cat([clip_outputs['text_embeds'], clip_outputs['image_embeds']], dim=1).to(self.device) # model input is the concatenation of the two modalities !\n            \n        for layer in self.layers:\n            x = layer(x)\n            \n        pred_taskA = self.sigmoid(self.head_task_a(x))\n        pred_taskB = self.sigmoid(self.head_task_b(x))\n        \n        return pred_taskA.squeeze(1), pred_taskB","metadata":{"id":"ZRBdpGpDJqv0","execution":{"iopub.status.busy":"2024-07-15T16:10:27.751679Z","iopub.execute_input":"2024-07-15T16:10:27.752563Z","iopub.status.idle":"2024-07-15T16:10:27.764790Z","shell.execute_reply.started":"2024-07-15T16:10:27.752530Z","shell.execute_reply":"2024-07-15T16:10:27.763918Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"## IN CASE WE NEED TO TRANSFORM A TSV/CSV INTO A JSON FORMAT (last two cells are only for testing purposese... the first instead is the one needed for the conversion from tsv to json)","metadata":{}},{"cell_type":"code","source":"import csv\nimport json\nimport os\n\nimages_path = \"/kaggle/input/dataset-wow/MAMI DATASET/MAMI DATASET/training/TRAINING\"\n\nfile_and_dest = [('/kaggle/input/dataset-wow/train_image_text.tsv','/kaggle/working/train_image_text.json'),\n                    ('/kaggle/input/dataset-wow/test_image_text.tsv','/kaggle/working/test_image_text.json')]\n\n\nfor file in file_and_dest: \n    counter = 0\n    data = []\n\n    with open(file[0], newline='', encoding='utf-8') as tsvfile:\n        reader = csv.DictReader(tsvfile, delimiter='\\t')\n        \n        for row in reader:\n            counter += 1\n            data.append(row)\n        print(f\"counter: {counter}\")\n    if not os.path.exists(file[1]):\n        with open(file[1], 'w', encoding='utf-8') as jsonfile:\n            json.dump([], jsonfile, ensure_ascii=False, indent=4)\n        print(f\"File JSON vuoto creato come {file[1]}\")\n\n        with open(file[1], 'w', encoding='utf-8') as jsonfile:\n            json.dump(data, jsonfile, ensure_ascii=False, indent=4)\n\n        print(f\"File JSON salvato come {file[1]}\")\n\n","metadata":{"execution":{"iopub.status.busy":"2024-07-15T16:10:31.231047Z","iopub.execute_input":"2024-07-15T16:10:31.231419Z","iopub.status.idle":"2024-07-15T16:10:31.472049Z","shell.execute_reply.started":"2024-07-15T16:10:31.231388Z","shell.execute_reply":"2024-07-15T16:10:31.471067Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"counter: 9000\nFile JSON vuoto creato come /kaggle/working/train_image_text.json\nFile JSON salvato come /kaggle/working/train_image_text.json\ncounter: 1000\nFile JSON vuoto creato come /kaggle/working/test_image_text.json\nFile JSON salvato come /kaggle/working/test_image_text.json\n","output_type":"stream"}]},{"cell_type":"code","source":"'''\nimport csv\nimport json\nimport os\n\nimages_path = \"/kaggle/input/dataset-wow/MAMI DATASET/MAMI DATASET/training/TRAINING\"\n\nfile_source = '/kaggle/input/dataset-wow/train_image_text.tsv'\nfile_train = '/kaggle/working/train_image_text.json'\nfile_test = '/kaggle/working/test_image_text.json'\ndata = []\n\nwith open(file_source, newline='', encoding='utf-8') as tsvfile:\n    reader = csv.DictReader(tsvfile, delimiter='\\t')\n    \n\n    for row in reader:\n        file_path = os.path.join(images_path, row[\"file_name\"])\n\n        if os.path.exists(file_path):\n            data.append(row)\n    \n    split = int(len(data)*0.8)\n\n    with open(file_train, 'w', encoding='utf-8') as jsonfile:\n        json.dump([], jsonfile, ensure_ascii=False, indent=4)\n    print(f\"File JSON vuoto creato come {file_train}\")\n\n    with open(file_test, 'w', encoding='utf-8') as jsonfile:\n        json.dump([], jsonfile, ensure_ascii=False, indent=4)\n    print(f\"File JSON vuoto creato come {file_test}\")\n    \n    with open(file_train, 'w', encoding='utf-8') as jsonfile:\n        json.dump(data[:split], jsonfile, ensure_ascii=False, indent=4)\n    print(f\"File JSON salvato come {file_train}\")\n        \n    with open(file_test, 'w', encoding='utf-8') as jsonfile:\n        json.dump(data[split:], jsonfile, ensure_ascii=False, indent=4)\n    print(f\"File JSON salvato come {file_test}\")\n'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Multimodal Dataset definition","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import Dataset\nimport os\nimport wandb\n\nclass MultimodalDataset(Dataset): # Dataset for handling multimodal data\n    def __init__(self, images_dir, json_file_path): # dir_path -> directory path where images are stored / json_file_path -> file path for metadata (including labels)   \n        file_paths, text_list, labels_misogyny, shaming_label_list, stereotype_label_list, objectification_label_list, violence_label_list = load_json_file(json_file_path)\n   \n        self.file_paths = file_paths\n        self.images_dir = images_dir\n        self.text_list = text_list\n        self.labels_misogyny = labels_misogyny\n        self.shaming_label_list = shaming_label_list\n        self.stereotype_label_list = stereotype_label_list\n        self.objectification_label_list = objectification_label_list\n        self.violence_label_list = violence_label_list\n        \n    def __len__(self):\n        return len(self.file_paths)\n    \n    def __getitem__(self, idx):\n        return self.file_paths[idx], self.text_list[idx], self.labels_misogyny[idx], self.shaming_label_list[idx], self.stereotype_label_list[idx], self.objectification_label_list[idx], self.violence_label_list[idx]\n    \ndef load_json_file(json_file_path):\n    with open(json_file_path,\"r\") as f:\n        data = json.load(f)\n        text_list = [] # list of the text related to each image\n        image_list = [] # list of images path\n        labels_misogyny = [] # list of TASK A labels (misogyny classification)\n\n        ### list of TASK B labels ###\n        shaming_label_list = [] \n        stereotype_label_list = []\n        objectification_label_list = []\n        violence_label_list = []\n\n\n        for item in tqdm(data): \n            image_list.append(item['file_name'])\n            text_list.append(item[\"text\"])\n            labels_misogyny.append(float(item[\"label\"]))\n            shaming_label_list.append(float(item[\"shaming\"]))\n            stereotype_label_list.append(float(item[\"stereotype\"]))\n            objectification_label_list.append(float(item[\"objectification\"]))\n            violence_label_list.append(float(item[\"violence\"]))\n\n        #print(f\"{type(labels_misogyny)}\")\n        return image_list, text_list, torch.tensor(labels_misogyny, dtype=torch.float32), torch.tensor(shaming_label_list, dtype=torch.float32), torch.tensor(stereotype_label_list, dtype=torch.float32), torch.tensor(objectification_label_list, dtype=torch.float32), torch.tensor(violence_label_list, dtype=torch.float32)\n    \ndef accuracy(preds, labels, thresh):\n    num_samples = labels.shape[0]\n    preds = preds > thresh\n    matching_rows = torch.eq(labels.bool(), preds)\n    \n    # in case we're dealing with the prediction of task B/task A (they've different number of dimensions)\n    num_correct = matching_rows.all(dim=1).sum().item() if preds.ndim!=1 else matching_rows.sum().item()\n    return 100*(num_correct/num_samples)","metadata":{"execution":{"iopub.status.busy":"2024-07-15T16:10:34.823902Z","iopub.execute_input":"2024-07-15T16:10:34.824518Z","iopub.status.idle":"2024-07-15T16:10:34.839618Z","shell.execute_reply.started":"2024-07-15T16:10:34.824489Z","shell.execute_reply":"2024-07-15T16:10:34.838745Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"## LOADING THE DATASET AND TRAINING THE NETWORK\n# Thanks to a class used for handling the argument of the network training (still room for modifying it !)","metadata":{}},{"cell_type":"code","source":"### import torch\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import CLIPProcessor, CLIPModel, CLIPTokenizer\nfrom PIL import Image # per debug\nimport os\nimport json\nfrom tqdm import tqdm\nfrom PIL import Image\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport wandb\n\n\n\nclass Trainer():\n    def __init__(self, train_images_dir,\n                       test_image_dir,\n                       json_train_path,\n                       json_test_path,\n                       num_linear_layers=5,\n                       drop_value=0.2,\n                       train_data_split=0.8,\n                       batch_size=256, \n                       lr=0.001, \n                       num_epochs=10,\n                       threshold=0.5,\n                       weight_taskA=1,\n                       weight_taskB=1):\n        \n        # Check if CUDA is available\n        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n        print(f\"Training on: {self.device}\")\n        \n        # Loading the Dataset\n        self.train_images_dir = train_images_dir\n        self.test_image_dir = test_image_dir\n        self.num_epochs = num_epochs\n        self.threshold = threshold\n        self.weight_taskA = weight_taskA\n        self.weight_taskB = weight_taskB\n        \n        train_data = MultimodalDataset(train_images_dir, json_train_path)\n        test_data = MultimodalDataset(test_image_dir, json_test_path)\n        \n        print(f\"training on samples:{train_data.__len__()}\")\n        print(f\"testing on samples:{test_data.__len__()}\")\n    \n        self.train_dataloader = DataLoader(train_data, batch_size, shuffle=True, pin_memory=True)\n        self.test_dataloader = DataLoader(test_data, batch_size, shuffle=True, pin_memory=True)\n\n        # Defining the Model\n        self.classifier = MisogynyCls(num_linear_layers=num_linear_layers, drop_value=drop_value).to(self.device)\n        self.optimizer = optim.Adam(self.classifier.parameters(), lr)\n        self.loss_taskA = F.binary_cross_entropy \n        self.loss_taskB = F.binary_cross_entropy\n\n        # Pretrained CLIP loading...\n        #self.clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\", device_map='cuda')\n        #self.clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n        \n    def get_test_dataset(self,size=100):\n        batch = next(iter(self.test_dataloader))\n        images, _,_,_,_,_,_ = batch\n        return images[:size]\n    \n    def get_model(self):\n        return self.classifier\n    \n        \n    def train_model(self):\n        for epoch in range(self.num_epochs):\n            print(f'Epoch [{epoch+1}/{self.num_epochs}]')\n            train_loss_list, test_loss_list, train_acc_taskA_list, train_acc_taskB_list,  test_acc_taskA_list, test_acc_taskB_list= self.train_epoch()\n            \n            train_loss_avg = sum(train_loss_list) / len(train_loss_list)\n            test_loss_avg = sum(test_loss_list) / len(test_loss_list)\n            train_acc_taskA_avg = sum(train_acc_taskA_list) / len(train_acc_taskA_list)\n            train_acc_taskB_avg = sum(test_acc_taskA_list) / len(test_acc_taskA_list)\n            test_acc_taskA_avg = sum(train_acc_taskB_list) / len(train_acc_taskB_list)\n            test_acc_taskB_avg = sum(test_acc_taskB_list) / len(test_acc_taskB_list)\n            \n            print(f'Average Train Loss: {train_loss_avg: .4f}, Average Test Loss: {test_loss_avg: .4f}')\n            print(f'Average Accuracy Train (task A): {train_acc_taskA_avg: .4f}%, Average Accuracy Test (task A): {train_acc_taskB_avg: .4f}%')\n            print(f'Average Accuracy Train (task B): {test_acc_taskA_avg: .4f}%, Average Accuracy Test (task B): {test_acc_taskB_avg: .4f}%')\n                    \n    def train_epoch(self):\n        train_loss_list = []\n        test_loss_list = []\n        train_acc_taskA_list = []\n        train_acc_taskB_list = []\n        test_acc_taskA_list = []\n        test_acc_taskB_list = []\n        \n        for batch in tqdm(self.train_dataloader):\n            self.optimizer.zero_grad() # ZEROING OUT THE GRADIENTS\n            self.classifier.train() # TRAINING MODE\n            \n            # CREATING THE CLIP EMBEDDINGS\n            image_list, text_list, labels_misogyny, shaming_labels, stereotype_labels, objectification_labels, violence_labels = batch\n            image_list = [Image.open(f\"{os.path.join(self.train_images_dir, img)}\") for img in image_list] # per poterlo usare poi con CLIP\n\n            labels_misogyny = labels_misogyny.to(self.device)\n            labels_taskB = torch.stack([shaming_labels, stereotype_labels, objectification_labels, violence_labels],  dim=1).to(self.device)\n    \n            pred_taskA, pred_taskB = self.classifier(text_list, image_list)\n\n            loss_A = self.loss_taskA(pred_taskA, labels_misogyny)\n            loss_B = self.loss_taskB(pred_taskB, labels_taskB, reduction='mean')\n            loss = (self.weight_taskA * loss_A) + (self.weight_taskB * loss_B)\n            train_loss_list.append(loss)\n            \n            loss.backward()\n            self.optimizer.step()\n\n            accuracy_taskA = accuracy(pred_taskA, labels_misogyny, self.threshold)\n            accuracy_taskB = accuracy(pred_taskB, labels_taskB, self.threshold)\n            \n            train_acc_taskA_list.append(accuracy_taskA)\n            train_acc_taskB_list.append(accuracy_taskB)\n        \n        \n        with torch.no_grad():\n            self.classifier.eval()\n\n            for batch in tqdm(self.test_dataloader):\n                # CREATING THE CLIP EMBEDDINGS\n                image_list, text_list, labels_misogyny, shaming_labels, stereotype_labels, objectification_labels, violence_labels = batch\n\n                image_list = [Image.open(f\"{os.path.join(self.test_image_dir, img)}\") for img in image_list] # per poterlo usare poi con CLIP\n                labels_misogyny = labels_misogyny.to(self.device)\n                labels_taskB = torch.stack([shaming_labels, stereotype_labels, objectification_labels, violence_labels],  dim=1).to(self.device)\n                \n                pred_taskA, pred_taskB = self.classifier(text_list, image_list)\n\n                loss_A = self.loss_taskA(pred_taskA, labels_misogyny)\n                loss_B = self.loss_taskB(pred_taskB, labels_taskB, reduction='mean')\n                loss = (self.weight_taskA * loss_A) + (self.weight_taskB * loss_B)\n                test_loss_list.append(loss)\n\n                accuracy_taskA = accuracy(pred_taskA, labels_misogyny, self.threshold)\n                accuracy_taskB = accuracy(pred_taskB, labels_taskB, self.threshold)\n\n                test_acc_taskA_list.append(accuracy_taskA)\n                test_acc_taskB_list.append(accuracy_taskB)\n            \n            \n            \n        return train_loss_list, test_loss_list, train_acc_taskA_list, train_acc_taskB_list, test_acc_taskA_list, test_acc_taskB_list\n\n            ","metadata":{"execution":{"iopub.status.busy":"2024-07-15T16:48:51.236531Z","iopub.execute_input":"2024-07-15T16:48:51.236812Z","iopub.status.idle":"2024-07-15T16:48:51.261906Z","shell.execute_reply.started":"2024-07-15T16:48:51.236790Z","shell.execute_reply":"2024-07-15T16:48:51.261086Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"markdown","source":"## Training WITHOUT wandb","metadata":{}},{"cell_type":"code","source":"model_trainer = Trainer(\"/kaggle/input/dataset-wow/MAMI DATASET/MAMI DATASET/training/TRAINING\",# train_images_dir\n                        \"/kaggle/input/dataset-wow/MAMI DATASET/MAMI DATASET/test\", #test_images_dir\n                        \"/kaggle/working/train_image_text.json\",\n                        \"/kaggle/working/test_image_text.json\",\n                        batch_size=64,\n                        num_epochs=3) # json_file as data source\n","metadata":{"execution":{"iopub.status.busy":"2024-07-15T16:48:55.245725Z","iopub.execute_input":"2024-07-15T16:48:55.246591Z","iopub.status.idle":"2024-07-15T16:48:56.978282Z","shell.execute_reply.started":"2024-07-15T16:48:55.246560Z","shell.execute_reply":"2024-07-15T16:48:56.977194Z"},"trusted":true},"execution_count":70,"outputs":[{"name":"stdout","text":"Training on: cuda:0\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 9000/9000 [00:00<00:00, 457466.17it/s]\n100%|██████████| 1000/1000 [00:00<00:00, 375027.18it/s]","output_type":"stream"},{"name":"stdout","text":"training on samples:9000\ntesting on samples:1000\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"model_trainer.train_model()","metadata":{"execution":{"iopub.status.busy":"2024-07-15T16:26:14.753534Z","iopub.execute_input":"2024-07-15T16:26:14.754208Z","iopub.status.idle":"2024-07-15T16:26:25.605277Z","shell.execute_reply.started":"2024-07-15T16:26:14.754176Z","shell.execute_reply":"2024-07-15T16:26:25.603929Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"Training on: cuda:0\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 9000/9000 [00:00<00:00, 454393.45it/s]\n100%|██████████| 1000/1000 [00:00<00:00, 306019.55it/s]","output_type":"stream"},{"name":"stdout","text":"training on samples:9000\ntesting on samples:1000\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/3]\n","output_type":"stream"},{"name":"stderr","text":"  3%|▎         | 4/141 [00:08<04:58,  2.18s/it]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[26], line 9\u001b[0m\n\u001b[1;32m      1\u001b[0m model_trainer \u001b[38;5;241m=\u001b[39m Trainer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/kaggle/input/dataset-wow/MAMI DATASET/MAMI DATASET/training/TRAINING\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;66;03m# train_images_dir\u001b[39;00m\n\u001b[1;32m      2\u001b[0m                         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/kaggle/input/dataset-wow/MAMI DATASET/MAMI DATASET/test\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;66;03m#test_images_dir\u001b[39;00m\n\u001b[1;32m      3\u001b[0m                         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/kaggle/working/train_image_text.json\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m                         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/kaggle/working/test_image_text.json\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m                         batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m,\n\u001b[1;32m      6\u001b[0m                         num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m) \u001b[38;5;66;03m# json_file as data source\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[43mmodel_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[23], line 72\u001b[0m, in \u001b[0;36mTrainer.train_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_epochs):\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 72\u001b[0m     train_loss_list, test_loss_list, train_acc_taskA_list, train_acc_taskB_list,  test_acc_taskA_list, test_acc_taskB_list\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m     train_loss_avg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(train_loss_list) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loss_list)\n\u001b[1;32m     75\u001b[0m     test_loss_avg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(test_loss_list) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(test_loss_list)\n","Cell \u001b[0;32mIn[23], line 129\u001b[0m, in \u001b[0;36mTrainer.train_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    126\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m--> 129\u001b[0m accuracy_taskA \u001b[38;5;241m=\u001b[39m \u001b[43maccuracy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred_taskA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels_misogyny\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthreshold\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m accuracy_taskB \u001b[38;5;241m=\u001b[39m accuracy(pred_taskB, labels_taskB, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mthreshold)\n\u001b[1;32m    132\u001b[0m train_acc_taskA_list\u001b[38;5;241m.\u001b[39mappend(accuracy_taskA)\n","Cell \u001b[0;32mIn[11], line 56\u001b[0m, in \u001b[0;36maccuracy\u001b[0;34m(preds, labels, thresh)\u001b[0m\n\u001b[1;32m     53\u001b[0m matching_rows \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meq(labels\u001b[38;5;241m.\u001b[39mbool(), preds)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# in case we're dealing with the prediction of task B/task A (they've different number of dimensions)\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m num_correct \u001b[38;5;241m=\u001b[39m matching_rows\u001b[38;5;241m.\u001b[39mall(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mif\u001b[39;00m preds\u001b[38;5;241m.\u001b[39mndim\u001b[38;5;241m!=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mmatching_rows\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m100\u001b[39m\u001b[38;5;241m*\u001b[39m(num_correct\u001b[38;5;241m/\u001b[39mnum_samples)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"markdown","source":"## Training with wandb","metadata":{}},{"cell_type":"code","source":"import wandb\n\n\ndef train(config=None):\n    with wandb.init(config=config):\n    \n        config = wandb.config\n        model_trainer = Trainer(\"/kaggle/input/dataset-wow/MAMI DATASET/MAMI DATASET/training/TRAINING\",# train_images_dir\n                            \"/kaggle/input/dataset-wow/MAMI DATASET/MAMI DATASET/test\", #test_images_dir\n                            \"/kaggle/working/train_image_text.json\",\n                            \"/kaggle/working/test_image_text.json\",\n                            batch_size=64,\n                            num_linear_layers=config.num_layers,\n                            lr=config.learning_rate,\n                            threshold=config.threshold,\n                            drop_value=config.dropout) # json_file as data source\n\n        model_trainer.train_model()\n    \n    \n# Log in to W&B using the API key\nsweep_config = {\n    'method': 'random'\n}\nparameters_dict = {\n    'learning_rate': {\n        'values': [0.1, 0.01, 0.001, 0.0001]\n        },\n    'threshold': {\n          'values': [0.5, 0.6, 0.7, 0.8]\n        },\n    'dropout': {\n          'values': [0.2, 0.3, 0.5]\n        },\n    'num_layers': {\n          'values': [5, 7, 10]\n        },\n    }\n\nsweep_config['parameters'] = parameters_dict\nsweep_id = wandb.sweep(sweep_config, project=\"Multimodal_xai\")   \nwandb.agent(sweep_id, train, count=20)\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"ShapSingle Modalities Explaination","metadata":{}},{"cell_type":"code","source":"test_img=model_trainer.get_test_dataset(100) \nbackground_dataset= test_img[:100]\n\nimage_to_explain = test_img[100:103]\n\n","metadata":{"execution":{"iopub.status.busy":"2024-07-15T16:49:09.834190Z","iopub.execute_input":"2024-07-15T16:49:09.834867Z","iopub.status.idle":"2024-07-15T16:49:09.841974Z","shell.execute_reply.started":"2024-07-15T16:49:09.834820Z","shell.execute_reply":"2024-07-15T16:49:09.841034Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"code","source":"import shap\ne = shap.DeepExplainer(model_trainer.get_model(), background_dataset)\nshap_values = e.shap_values(image_to_explain)","metadata":{"execution":{"iopub.status.busy":"2024-07-15T16:49:13.341046Z","iopub.execute_input":"2024-07-15T16:49:13.341416Z","iopub.status.idle":"2024-07-15T16:49:13.512608Z","shell.execute_reply.started":"2024-07-15T16:49:13.341386Z","shell.execute_reply":"2024-07-15T16:49:13.511294Z"},"trusted":true},"execution_count":72,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[72], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mshap\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m e \u001b[38;5;241m=\u001b[39m \u001b[43mshap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDeepExplainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackground_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m shap_values \u001b[38;5;241m=\u001b[39m e\u001b[38;5;241m.\u001b[39mshap_values(image_to_explain)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/shap/explainers/_deep/__init__.py:86\u001b[0m, in \u001b[0;36mDeepExplainer.__init__\u001b[0;34m(self, model, data, session, learning_phase_flags)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexplainer \u001b[38;5;241m=\u001b[39m TFDeep(model, data, session, learning_phase_flags)\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m framework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpytorch\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 86\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexplainer \u001b[38;5;241m=\u001b[39m \u001b[43mPyTorchDeep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexpected_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexplainer\u001b[38;5;241m.\u001b[39mexpected_value\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexplainer\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m=\u001b[39m framework\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/shap/explainers/_deep/deep_pytorch.py:58\u001b[0m, in \u001b[0;36mPyTorchDeep.__init__\u001b[0;34m(self, model, data)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 58\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;66;03m# also get the device everything is running on\u001b[39;00m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mdevice\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","\u001b[0;31mTypeError\u001b[0m: MisogynyCls.forward() takes 3 positional arguments but 65 were given"],"ename":"TypeError","evalue":"MisogynyCls.forward() takes 3 positional arguments but 65 were given","output_type":"error"}]}]}
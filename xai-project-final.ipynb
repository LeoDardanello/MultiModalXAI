{"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","include_colab_link":true,"provenance":[]},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8823503,"sourceType":"datasetVersion","datasetId":5308380},{"sourceId":8842298,"sourceType":"datasetVersion","datasetId":5321593}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#!pip install --upgrade transformers","metadata":{"id":"ClPn9DhYJM8u","outputId":"fabbbf4c-a592-43fa-c00a-9ba78826e74f","execution":{"iopub.status.busy":"2024-07-02T23:53:30.674350Z","iopub.execute_input":"2024-07-02T23:53:30.674712Z","iopub.status.idle":"2024-07-02T23:53:30.679399Z","shell.execute_reply.started":"2024-07-02T23:53:30.674679Z","shell.execute_reply":"2024-07-02T23:53:30.678500Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"#!pip install wandb\n","metadata":{"execution":{"iopub.status.busy":"2024-07-02T23:53:30.680511Z","iopub.execute_input":"2024-07-02T23:53:30.680783Z","iopub.status.idle":"2024-07-02T23:53:30.690960Z","shell.execute_reply.started":"2024-07-02T23:53:30.680752Z","shell.execute_reply":"2024-07-02T23:53:30.690072Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"#!wandb login\n","metadata":{"execution":{"iopub.status.busy":"2024-07-02T23:53:30.692088Z","iopub.execute_input":"2024-07-02T23:53:30.692351Z","iopub.status.idle":"2024-07-02T23:53:30.700520Z","shell.execute_reply.started":"2024-07-02T23:53:30.692328Z","shell.execute_reply":"2024-07-02T23:53:30.699680Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"!rm -rf /kaggle/working/*","metadata":{"execution":{"iopub.status.busy":"2024-07-02T23:53:30.703016Z","iopub.execute_input":"2024-07-02T23:53:30.703322Z","iopub.status.idle":"2024-07-02T23:53:31.680080Z","shell.execute_reply.started":"2024-07-02T23:53:30.703294Z","shell.execute_reply":"2024-07-02T23:53:31.679009Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"from torch import nn\n\n\nclass MisogynyCls(nn.Module):\n    def __init__(self, num_linear_layers, task_a_out=1, task_b_out=4, input_dim=1024, hidden_dim=512, drop_value=0.2):\n        super().__init__()\n        self.head_task_a = nn.Linear(hidden_dim, task_a_out)\n        self.head_task_b = nn.Linear(hidden_dim, task_b_out)\n        self.sigmoid = nn.Sigmoid()\n\n\n        self.layers = nn.ModuleList()\n\n        for i in range(num_linear_layers):\n            if i == 0:\n                self.layers.append(nn.Linear(input_dim, hidden_dim))\n            else:\n                self.layers.append(nn.Linear(hidden_dim, hidden_dim))\n                \n            self.layers.append(nn.Dropout(drop_value))\n            self.layers.append(nn.ReLU())\n\n    def forward(self, x):\n        for layer in self.layers:\n            x = layer(x)\n        pred_taskA = self.sigmoid(self.head_task_a(x))\n        pred_taskB = self.sigmoid(self.head_task_b(x))\n        \n        return pred_taskA.squeeze(1), pred_taskB","metadata":{"id":"ZRBdpGpDJqv0","execution":{"iopub.status.busy":"2024-07-02T23:53:31.681471Z","iopub.execute_input":"2024-07-02T23:53:31.681748Z","iopub.status.idle":"2024-07-02T23:53:33.354369Z","shell.execute_reply.started":"2024-07-02T23:53:31.681723Z","shell.execute_reply":"2024-07-02T23:53:33.353569Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## IN CASE WE NEED TO TRANSFORM A TSV/CSV INTO A JSON FORMAT (last two cells are only for testing purposese... the first instead is the one needed for the conversion from tsv to json)","metadata":{}},{"cell_type":"code","source":"'''\nimport csv\nimport json\nimport os\n\nimages_path = \"/kaggle/input/dataset-wow/MAMI DATASET/MAMI DATASET/training/TRAINING\"\n\nfile_and_dest = [('/kaggle/input/dataset-wow/train_image_text.tsv','/kaggle/working/train_image_text.json'),\n                    ('/kaggle/input/dataset-wow/test_image_text.tsv','/kaggle/working/test_image_text.json')]\n\n\nfor file in file_and_dest: \n    data = []\n\n    with open(file[0], newline='', encoding='utf-8') as tsvfile:\n        reader = csv.DictReader(tsvfile, delimiter='\\t')\n        \n        for row in reader:\n            file_path = os.path.join(images_path, row[\"file_name\"])\n            \n            if os.path.exists(file_path):\n                data.append(row)\n                \n    if not os.path.exists(file[1]):\n        with open(file[1], 'w', encoding='utf-8') as jsonfile:\n            json.dump([], jsonfile, ensure_ascii=False, indent=4)\n        print(f\"File JSON vuoto creato come {file[1]}\")\n\n        with open(file[1], 'w', encoding='utf-8') as jsonfile:\n            json.dump(data, jsonfile, ensure_ascii=False, indent=4)\n\n        print(f\"File JSON salvato come {file[1]}\")\n'''\n","metadata":{"execution":{"iopub.status.busy":"2024-07-02T23:53:33.355681Z","iopub.execute_input":"2024-07-02T23:53:33.356650Z","iopub.status.idle":"2024-07-02T23:53:33.365583Z","shell.execute_reply.started":"2024-07-02T23:53:33.356611Z","shell.execute_reply":"2024-07-02T23:53:33.364557Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"'\\nimport csv\\nimport json\\nimport os\\n\\nimages_path = \"/kaggle/input/dataset-wow/MAMI DATASET/MAMI DATASET/training/TRAINING\"\\n\\nfile_and_dest = [(\\'/kaggle/input/dataset-wow/train_image_text.tsv\\',\\'/kaggle/working/train_image_text.json\\'),\\n                    (\\'/kaggle/input/dataset-wow/test_image_text.tsv\\',\\'/kaggle/working/test_image_text.json\\')]\\n\\n\\nfor file in file_and_dest: \\n    data = []\\n\\n    with open(file[0], newline=\\'\\', encoding=\\'utf-8\\') as tsvfile:\\n        reader = csv.DictReader(tsvfile, delimiter=\\'\\t\\')\\n        \\n        for row in reader:\\n            file_path = os.path.join(images_path, row[\"file_name\"])\\n            \\n            if os.path.exists(file_path):\\n                data.append(row)\\n                \\n    if not os.path.exists(file[1]):\\n        with open(file[1], \\'w\\', encoding=\\'utf-8\\') as jsonfile:\\n            json.dump([], jsonfile, ensure_ascii=False, indent=4)\\n        print(f\"File JSON vuoto creato come {file[1]}\")\\n\\n        with open(file[1], \\'w\\', encoding=\\'utf-8\\') as jsonfile:\\n            json.dump(data, jsonfile, ensure_ascii=False, indent=4)\\n\\n        print(f\"File JSON salvato come {file[1]}\")\\n'"},"metadata":{}}]},{"cell_type":"code","source":"import csv\nimport json\nimport os\n\nimages_path = \"/kaggle/input/dataset-wow/MAMI DATASET/MAMI DATASET/training/TRAINING\"\n\nfile_source = '/kaggle/input/dataset-wow/train_image_text.tsv'\nfile_train = '/kaggle/working/train_image_text.json'\nfile_test = '/kaggle/working/test_image_text.json'\ndata = []\n\nwith open(file_source, newline='', encoding='utf-8') as tsvfile:\n    reader = csv.DictReader(tsvfile, delimiter='\\t')\n    \n\n    for row in reader:\n        file_path = os.path.join(images_path, row[\"file_name\"])\n\n        if os.path.exists(file_path):\n            data.append(row)\n    \n    split = int(len(data)*0.8)\n\n    with open(file_train, 'w', encoding='utf-8') as jsonfile:\n        json.dump([], jsonfile, ensure_ascii=False, indent=4)\n    print(f\"File JSON vuoto creato come {file_train}\")\n\n    with open(file_test, 'w', encoding='utf-8') as jsonfile:\n        json.dump([], jsonfile, ensure_ascii=False, indent=4)\n    print(f\"File JSON vuoto creato come {file_test}\")\n    \n    with open(file_train, 'w', encoding='utf-8') as jsonfile:\n        json.dump(data[:split], jsonfile, ensure_ascii=False, indent=4)\n    print(f\"File JSON salvato come {file_train}\")\n        \n    with open(file_test, 'w', encoding='utf-8') as jsonfile:\n        json.dump(data[split:], jsonfile, ensure_ascii=False, indent=4)\n    print(f\"File JSON salvato come {file_test}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-07-02T23:53:33.367026Z","iopub.execute_input":"2024-07-02T23:53:33.367753Z","iopub.status.idle":"2024-07-02T23:53:37.247955Z","shell.execute_reply.started":"2024-07-02T23:53:33.367718Z","shell.execute_reply":"2024-07-02T23:53:37.247024Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"File JSON vuoto creato come /kaggle/working/train_image_text.json\nFile JSON vuoto creato come /kaggle/working/test_image_text.json\nFile JSON salvato come /kaggle/working/train_image_text.json\nFile JSON salvato come /kaggle/working/test_image_text.json\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Multimodal Dataset definition","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import Dataset\nimport os\nimport wandb\n\nclass MultimodalDataset(Dataset): # Dataset for handling multimodal data\n    def __init__(self, images_dir, json_file_path): # dir_path -> directory path where images are stored / json_file_path -> file path for metadata (including labels)   \n        file_paths, text_list, labels_misogyny, shaming_label_list, stereotype_label_list, objectification_label_list, violence_label_list = load_json_file(json_file_path)\n   \n        self.file_paths = file_paths\n        self.images_dir = images_dir\n        self.text_list = text_list\n        self.labels_misogyny = labels_misogyny\n        self.shaming_label_list = shaming_label_list\n        self.stereotype_label_list = stereotype_label_list\n        self.objectification_label_list = objectification_label_list\n        self.violence_label_list = violence_label_list\n        \n    def __len__(self):\n        return len(self.file_paths)\n    \n    def __getitem__(self, idx):\n        return self.file_paths[idx], self.text_list[idx], self.labels_misogyny[idx], self.shaming_label_list[idx], self.stereotype_label_list[idx], self.objectification_label_list[idx], self.violence_label_list[idx]\n    \ndef load_json_file(json_file_path):\n    with open(json_file_path,\"r\") as f:\n        data = json.load(f)\n        text_list = [] # list of the text related to each image\n        image_list = [] # list of images path\n        labels_misogyny = [] # list of TASK A labels (misogyny classification)\n\n        ### list of TASK B labels ###\n        shaming_label_list = [] \n        stereotype_label_list = []\n        objectification_label_list = []\n        violence_label_list = []\n\n\n        for item in tqdm(data): \n            image_list.append(item['file_name'])\n            text_list.append(item[\"text\"])\n            labels_misogyny.append(float(item[\"label\"]))\n            shaming_label_list.append(float(item[\"shaming\"]))\n            stereotype_label_list.append(float(item[\"stereotype\"]))\n            objectification_label_list.append(float(item[\"objectification\"]))\n            violence_label_list.append(float(item[\"violence\"]))\n\n        #print(f\"{type(labels_misogyny)}\")\n        return image_list, text_list, torch.tensor(labels_misogyny, dtype=torch.float32), torch.tensor(shaming_label_list, dtype=torch.float32), torch.tensor(stereotype_label_list, dtype=torch.float32), torch.tensor(objectification_label_list, dtype=torch.float32), torch.tensor(violence_label_list, dtype=torch.float32)\n    \ndef accuracy(preds, labels, thresh):\n    num_samples = labels.shape[0]\n    preds = preds > thresh\n    matching_rows = torch.eq(labels.bool(), preds)\n    \n    # in case we're dealing with the prediction of task B/task A (they've different number of dimensions)\n    num_correct = matching_rows.all(dim=1).sum().item() if preds.ndim!=1 else matching_rows.sum().item()\n    return 100*(num_correct/num_samples)","metadata":{"execution":{"iopub.status.busy":"2024-07-02T23:53:37.249050Z","iopub.execute_input":"2024-07-02T23:53:37.249304Z","iopub.status.idle":"2024-07-02T23:53:37.819270Z","shell.execute_reply.started":"2024-07-02T23:53:37.249282Z","shell.execute_reply":"2024-07-02T23:53:37.818211Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"## LOADING THE DATASET AND TRAINING THE NETWORK\n# Thanks to a class used for handling the argument of the network training (still room for modifying it !)","metadata":{}},{"cell_type":"code","source":"### import torch\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import CLIPProcessor, CLIPModel, CLIPTokenizer\nfrom PIL import Image # per debug\nimport os\nimport json\nfrom tqdm import tqdm\nfrom PIL import Image\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport wandb\n\n\n\nclass Trainer():\n    def __init__(self, images_dir, \n                       json_train_path,\n                       json_test_path,\n                       train_data_split=0.8,\n                       batch_size=256, \n                       lr=0.001, \n                       num_epochs=15,\n                       threshold=0.5,\n                       weight_taskA=0.7,\n                       weight_taskB=0.3):\n        \n        # Check if CUDA is available\n        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n        print(f\"Training on: {self.device}\")\n        \n        # Loading the Dataset\n        self.images_dir = images_dir\n        self.num_epochs = num_epochs\n        self.threshold = threshold\n        self.weight_taskA = weight_taskA\n        self.weight_taskB = weight_taskB\n        \n        train_data = MultimodalDataset(images_dir, json_train_path)\n        test_data = MultimodalDataset(images_dir, json_test_path)\n\n        self.train_dataloader = DataLoader(train_data, batch_size, shuffle=True, pin_memory=True)\n        self.test_dataloader = DataLoader(test_data, batch_size, shuffle=True, pin_memory=True)\n\n        # Defining the Model\n        self.classifier = MisogynyCls(5).to(self.device)\n        self.optimizer = optim.Adam(self.classifier.parameters(), lr)\n        self.loss_taskA = F.binary_cross_entropy \n        self.loss_taskB = F.binary_cross_entropy\n\n        # Pretrained CLIP loading...\n        self.clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\", device_map='cuda')\n        self.clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n    \n        '''\n        wandb.init(  # set the wandb project where this run will be logged\n                    project=\"Multimodal_xai\", \n                    # track hyperparameters and run metadata   \n                    config={\n                        #\"architecture\": args.model,    \"optimizer\": \"SGD\" if args.use_sgd else \"ADAM\",\n                        #\"scheduler\" : args.scheduler,    \"learning_rate\": args.lr * 100 if args.use_sgd else args.lr,\n                        #\"min_learning_rate\": args.lrmin,    \"epochs\": args.epochs,\n                        #\"filename\": args.filename,    \"batch\": args.batch_size,\n                        #\"momentum\": args.momentum,    \"weightdecay\": args.wd,\n                        #\"tanh\": args.tanh,    \"invert_lr\": args.invert_lr,\n                        #\"max_learning_rate\": args.lrmax\n                    })\n        '''\n        \n    def train_model(self):\n        for epoch in range(self.num_epochs):\n            print(f'Epoch [{epoch+1}/{self.num_epochs}]')\n            train_loss_list, test_loss_list, train_acc_taskA_list, train_acc_taskB_list,  test_acc_taskA_list, test_acc_taskB_list= self.train_epoch()\n            \n            print(f'Average Train Loss: {sum(train_loss_list) / len(train_loss_list): .4f}, Average Test Loss: {sum(test_loss_list) / len(test_loss_list): .4f}')\n            print(f'Average Accuracy Train (task A): {sum(train_acc_taskA_list) / len(train_acc_taskA_list): .4f}%, Average Accuracy Test (task A): {sum(test_acc_taskA_list) / len(test_acc_taskA_list): .4f}%')\n            print(f'Average Accuracy Train (task B): {sum(train_acc_taskB_list) / len(train_acc_taskB_list): .4f}%, Average Accuracy Test (task B): {sum(test_acc_taskB_list) / len(test_acc_taskB_list): .4f}%')\n\n            model_path = '/kaggle/working/model_' + str(epoch+1) + '.pth'\n            torch.save(self.classifier.state_dict(), model_path);\n            \n            \n\n    def train_epoch(self):\n        train_loss_list = []\n        test_loss_list = []\n        train_acc_taskA_list = []\n        train_acc_taskB_list = []\n        test_acc_taskA_list = []\n        test_acc_taskB_list = []\n        \n        for batch in tqdm(self.train_dataloader):\n            self.optimizer.zero_grad() # ZEROING OUT THE GRADIENTS\n            self.classifier.train() # TRAINING MODE\n            \n            # CREATING THE CLIP EMBEDDINGS\n            image_list, text_list, labels_misogyny, shaming_labels, stereotype_labels, objectification_labels, violence_labels = batch\n            \n            image_list = [Image.open(f\"{os.path.join(self.images_dir, img)}\") for img in image_list] # per poterlo usare poi con CLIP\n            labels_misogyny = labels_misogyny.to(self.device)\n            labels_taskB = torch.stack([shaming_labels, stereotype_labels, objectification_labels, violence_labels],  dim=1).to(self.device)\n\n            clip_inputs = self.clip_processor(text=text_list, images=image_list, return_tensors=\"pt\", padding=True, truncation=True)\n            clip_inputs['input_ids'] = clip_inputs['input_ids'].to(self.device)\n            clip_inputs['attention_mask'] = clip_inputs['attention_mask'].to(self.device)\n            clip_inputs['pixel_values'] = clip_inputs['pixel_values'].to(self.device)\n    \n            clip_outputs = self.clip_model(**clip_inputs)\n            model_input = torch.cat([clip_outputs['text_embeds'], clip_outputs['image_embeds']], dim=1).to(self.device)\n    \n            pred_taskA, pred_taskB = self.classifier(model_input)\n\n            loss_A = self.loss_taskA(pred_taskA, labels_misogyny)\n            loss_B = self.loss_taskB(pred_taskB, labels_taskB, reduction='mean')\n            loss = (self.weight_taskA * loss_A) + (self.weight_taskB * loss_B)\n            train_loss_list.append(loss)\n            \n            loss.backward()\n            self.optimizer.step()\n\n            accuracy_taskA = accuracy(pred_taskA, labels_misogyny, self.threshold)\n            accuracy_taskB = accuracy(pred_taskB, labels_taskB, self.threshold)\n            \n            train_acc_taskA_list.append(accuracy_taskA)\n            train_acc_taskB_list.append(accuracy_taskB)\n        \n        \n        with torch.no_grad():\n            self.classifier.eval()\n\n            for batch in tqdm(self.test_dataloader):\n                # CREATING THE CLIP EMBEDDINGS\n                image_list, text_list, labels_misogyny, shaming_labels, stereotype_labels, objectification_labels, violence_labels = batch\n\n                image_list = [Image.open(f\"{os.path.join(self.images_dir, img)}\") for img in image_list] # per poterlo usare poi con CLIP\n                labels_misogyny = labels_misogyny.to(self.device)\n                labels_taskB = torch.stack([shaming_labels, stereotype_labels, objectification_labels, violence_labels],  dim=1).to(self.device)\n\n                clip_inputs = self.clip_processor(text=text_list, images=image_list, return_tensors=\"pt\", padding=True, truncation=True)\n                clip_inputs['input_ids'] = clip_inputs['input_ids'].to(self.device)\n                clip_inputs['attention_mask'] = clip_inputs['attention_mask'].to(self.device)\n                clip_inputs['pixel_values'] = clip_inputs['pixel_values'].to(self.device)\n\n                clip_outputs = self.clip_model(**clip_inputs)\n                model_input = torch.cat([clip_outputs['text_embeds'], clip_outputs['image_embeds']], dim=1).to(self.device)\n\n                pred_taskA, pred_taskB = self.classifier(model_input)\n\n                loss_A = self.loss_taskA(pred_taskA, labels_misogyny)\n                loss_B = self.loss_taskB(pred_taskB, labels_taskB, reduction='mean')\n                loss = (self.weight_taskA * loss_A) + (self.weight_taskB * loss_B)\n                test_loss_list.append(loss)\n\n                accuracy_taskA = accuracy(pred_taskA, labels_misogyny, self.threshold)\n                accuracy_taskB = accuracy(pred_taskB, labels_taskB, self.threshold)\n\n                test_acc_taskA_list.append(accuracy_taskA)\n                test_acc_taskB_list.append(accuracy_taskB)\n\n            \n            '''\n            wandb.log({\"accuracy_taskA\": accuracy_taskA,\n                       \"accuracy_taskB\": accuracy_taskB,\n                       \"loss_A\": loss_A.item(),\n                       \"loss_B\": loss_B.item(),\n                       \"learning_rate\": self.optimizer.param_groups[0]['lr']})\n            \n            '''\n            \n        return train_loss_list, test_loss_list, train_acc_taskA_list, train_acc_taskB_list, test_acc_taskA_list, test_acc_taskB_list\n\n            ","metadata":{"execution":{"iopub.status.busy":"2024-07-02T23:53:37.821000Z","iopub.execute_input":"2024-07-02T23:53:37.821472Z","iopub.status.idle":"2024-07-02T23:53:39.015124Z","shell.execute_reply.started":"2024-07-02T23:53:37.821437Z","shell.execute_reply":"2024-07-02T23:53:39.014333Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"\nmodel_trainer = Trainer(\"/kaggle/input/dataset-wow/MAMI DATASET/MAMI DATASET/training/TRAINING\", # images_dir\n                        \"/kaggle/working/train_image_text.json\",\n                        \"/kaggle/working/test_image_text.json\",\n                        batch_size=64,\n                        weight_taskA=1,\n                        weight_taskB=1) # json_file as data source\n\n\nmodel_trainer.train_model()","metadata":{"execution":{"iopub.status.busy":"2024-07-02T23:53:39.016207Z","iopub.execute_input":"2024-07-02T23:53:39.016641Z","iopub.status.idle":"2024-07-03T00:35:13.250034Z","shell.execute_reply.started":"2024-07-02T23:53:39.016614Z","shell.execute_reply":"2024-07-03T00:35:13.248929Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Training on: cuda:0\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 7200/7200 [00:00<00:00, 429744.26it/s]\n100%|██████████| 1800/1800 [00:00<00:00, 474976.23it/s]\n/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\n2024-07-02 23:53:40.610680: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-02 23:53:40.610734: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-02 23:53:40.612191: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/15]\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 113/113 [02:20<00:00,  1.24s/it]\n100%|██████████| 29/29 [00:25<00:00,  1.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Train Loss:  0.8864, Average Test Loss:  0.7427\nAverage Accuracy Train (task A):  74.5575%, Average Accuracy Test (task A):  83.7823%\nAverage Accuracy Train (task B):  52.6410%, Average Accuracy Test (task B):  55.1185%\nEpoch [2/15]\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 113/113 [02:18<00:00,  1.23s/it]\n100%|██████████| 29/29 [00:25<00:00,  1.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Train Loss:  0.6723, Average Test Loss:  0.7737\nAverage Accuracy Train (task A):  85.9790%, Average Accuracy Test (task A):  83.1897%\nAverage Accuracy Train (task B):  56.4712%, Average Accuracy Test (task B):  57.9203%\nEpoch [3/15]\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 113/113 [02:19<00:00,  1.24s/it]\n100%|██████████| 29/29 [00:25<00:00,  1.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Train Loss:  0.5991, Average Test Loss:  0.7184\nAverage Accuracy Train (task A):  87.9840%, Average Accuracy Test (task A):  83.8362%\nAverage Accuracy Train (task B):  60.9375%, Average Accuracy Test (task B):  59.7522%\nEpoch [4/15]\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 113/113 [02:19<00:00,  1.24s/it]\n100%|██████████| 29/29 [00:25<00:00,  1.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Train Loss:  0.5209, Average Test Loss:  0.7624\nAverage Accuracy Train (task A):  90.9845%, Average Accuracy Test (task A):  84.6983%\nAverage Accuracy Train (task B):  63.8274%, Average Accuracy Test (task B):  60.7759%\nEpoch [5/15]\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 113/113 [02:19<00:00,  1.24s/it]\n100%|██████████| 29/29 [00:25<00:00,  1.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Train Loss:  0.4491, Average Test Loss:  0.8360\nAverage Accuracy Train (task A):  93.8606%, Average Accuracy Test (task A):  84.5905%\nAverage Accuracy Train (task B):  65.4729%, Average Accuracy Test (task B):  60.3448%\nEpoch [6/15]\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 113/113 [02:19<00:00,  1.24s/it]\n100%|██████████| 29/29 [00:25<00:00,  1.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Train Loss:  0.3994, Average Test Loss:  1.0564\nAverage Accuracy Train (task A):  95.1051%, Average Accuracy Test (task A):  84.2672%\nAverage Accuracy Train (task B):  66.0398%, Average Accuracy Test (task B):  59.3211%\nEpoch [7/15]\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 113/113 [02:20<00:00,  1.24s/it]\n100%|██████████| 29/29 [00:25<00:00,  1.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Train Loss:  0.3634, Average Test Loss:  1.0257\nAverage Accuracy Train (task A):  96.4878%, Average Accuracy Test (task A):  83.4052%\nAverage Accuracy Train (task B):  67.2428%, Average Accuracy Test (task B):  59.6444%\nEpoch [8/15]\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 113/113 [02:20<00:00,  1.24s/it]\n100%|██████████| 29/29 [00:25<00:00,  1.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Train Loss:  0.3366, Average Test Loss:  1.1039\nAverage Accuracy Train (task A):  97.2622%, Average Accuracy Test (task A):  83.4052%\nAverage Accuracy Train (task B):  68.4735%, Average Accuracy Test (task B):  60.5603%\nEpoch [9/15]\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 113/113 [02:20<00:00,  1.24s/it]\n100%|██████████| 29/29 [00:25<00:00,  1.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Train Loss:  0.3113, Average Test Loss:  1.1444\nAverage Accuracy Train (task A):  98.0227%, Average Accuracy Test (task A):  83.5668%\nAverage Accuracy Train (task B):  68.8744%, Average Accuracy Test (task B):  57.9741%\nEpoch [10/15]\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 113/113 [02:20<00:00,  1.24s/it]\n100%|██████████| 29/29 [00:25<00:00,  1.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Train Loss:  0.2996, Average Test Loss:  1.2028\nAverage Accuracy Train (task A):  98.2439%, Average Accuracy Test (task A):  84.5905%\nAverage Accuracy Train (task B):  69.3169%, Average Accuracy Test (task B):  60.1832%\nEpoch [11/15]\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 113/113 [02:20<00:00,  1.24s/it]\n100%|██████████| 29/29 [00:25<00:00,  1.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Train Loss:  0.2804, Average Test Loss:  1.2731\nAverage Accuracy Train (task A):  98.7832%, Average Accuracy Test (task A):  83.8901%\nAverage Accuracy Train (task B):  69.8562%, Average Accuracy Test (task B):  60.6681%\nEpoch [12/15]\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 113/113 [02:20<00:00,  1.24s/it]\n100%|██████████| 29/29 [00:25<00:00,  1.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Train Loss:  0.2693, Average Test Loss:  1.1773\nAverage Accuracy Train (task A):  98.7970%, Average Accuracy Test (task A):  84.6983%\nAverage Accuracy Train (task B):  70.9900%, Average Accuracy Test (task B):  62.3384%\nEpoch [13/15]\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 113/113 [02:21<00:00,  1.25s/it]\n100%|██████████| 29/29 [00:25<00:00,  1.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Train Loss:  0.2649, Average Test Loss:  1.2249\nAverage Accuracy Train (task A):  98.7555%, Average Accuracy Test (task A):  84.7522%\nAverage Accuracy Train (task B):  71.5293%, Average Accuracy Test (task B):  57.9203%\nEpoch [14/15]\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 113/113 [02:22<00:00,  1.26s/it]\n100%|██████████| 29/29 [00:25<00:00,  1.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Train Loss:  0.2570, Average Test Loss:  1.1883\nAverage Accuracy Train (task A):  98.8385%, Average Accuracy Test (task A):  85.2371%\nAverage Accuracy Train (task B):  72.7046%, Average Accuracy Test (task B):  60.5603%\nEpoch [15/15]\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 113/113 [02:20<00:00,  1.24s/it]\n100%|██████████| 29/29 [00:25<00:00,  1.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Train Loss:  0.2438, Average Test Loss:  1.2641\nAverage Accuracy Train (task A):  99.1842%, Average Accuracy Test (task A):  83.5668%\nAverage Accuracy Train (task B):  72.7046%, Average Accuracy Test (task B):  61.2608%\n","output_type":"stream"}]}]}
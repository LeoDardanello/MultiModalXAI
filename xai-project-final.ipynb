{"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","include_colab_link":true,"provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8823503,"sourceType":"datasetVersion","datasetId":5308380},{"sourceId":8853110,"sourceType":"datasetVersion","datasetId":5321593}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#!pip install --upgrade transformers","metadata":{"id":"ClPn9DhYJM8u","outputId":"fabbbf4c-a592-43fa-c00a-9ba78826e74f","execution":{"iopub.status.busy":"2024-07-02T23:53:30.674350Z","iopub.execute_input":"2024-07-02T23:53:30.674712Z","iopub.status.idle":"2024-07-02T23:53:30.679399Z","shell.execute_reply.started":"2024-07-02T23:53:30.674679Z","shell.execute_reply":"2024-07-02T23:53:30.678500Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"#!pip install wandb\n","metadata":{"execution":{"iopub.status.busy":"2024-07-02T23:53:30.680511Z","iopub.execute_input":"2024-07-02T23:53:30.680783Z","iopub.status.idle":"2024-07-02T23:53:30.690960Z","shell.execute_reply.started":"2024-07-02T23:53:30.680752Z","shell.execute_reply":"2024-07-02T23:53:30.690072Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"#!wandb login\n","metadata":{"execution":{"iopub.status.busy":"2024-07-02T23:53:30.692088Z","iopub.execute_input":"2024-07-02T23:53:30.692351Z","iopub.status.idle":"2024-07-02T23:53:30.700520Z","shell.execute_reply.started":"2024-07-02T23:53:30.692328Z","shell.execute_reply":"2024-07-02T23:53:30.699680Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"!rm -rf /kaggle/working/*","metadata":{"execution":{"iopub.status.busy":"2024-07-17T07:51:50.338017Z","iopub.execute_input":"2024-07-17T07:51:50.338275Z","iopub.status.idle":"2024-07-17T07:51:51.326324Z","shell.execute_reply.started":"2024-07-17T07:51:50.338250Z","shell.execute_reply":"2024-07-17T07:51:51.325294Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"from torch import nn\n\n\nclass MisogynyCls(nn.Module):\n    def __init__(self, num_linear_layers, single_task_mode=True, task_a_out=1, task_b_out=4, input_dim=1024, hidden_dim=512, drop_value=0.2):\n        super().__init__()\n        self.single_task_mode=single_task_mode\n        self.head_task_a = nn.Linear(hidden_dim, task_a_out)\n        if not self.single_task_mode:\n            self.head_task_b = nn.Linear(hidden_dim, task_b_out)\n        self.sigmoid = nn.Sigmoid()\n\n\n        self.layers = nn.ModuleList()\n\n        for i in range(num_linear_layers):\n            if i == 0:\n                self.layers.append(nn.Linear(input_dim, hidden_dim))\n            else:\n                self.layers.append(nn.Linear(hidden_dim, hidden_dim))\n                \n            self.layers.append(nn.Dropout(drop_value))\n            self.layers.append(nn.ReLU())\n\n    def forward(self, x):\n        for layer in self.layers:\n            x = layer(x)\n        pred_taskA = self.sigmoid(self.head_task_a(x))\n        if not self.single_task_mode:\n            pred_taskB = self.sigmoid(self.head_task_b(x))\n            return pred_taskA.squeeze(1), pred_taskB\n        else:\n            return pred_taskA.squeeze(1)","metadata":{"id":"ZRBdpGpDJqv0","execution":{"iopub.status.busy":"2024-07-17T08:32:03.508480Z","iopub.execute_input":"2024-07-17T08:32:03.509154Z","iopub.status.idle":"2024-07-17T08:32:03.518571Z","shell.execute_reply.started":"2024-07-17T08:32:03.509123Z","shell.execute_reply":"2024-07-17T08:32:03.517687Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":"## tsv/csv to json","metadata":{}},{"cell_type":"code","source":"import csv\nimport json\nimport os\n\nimages_path = \"/kaggle/input/dataset-wow/MAMI DATASET/MAMI DATASET/training/TRAINING\"\n\nfile_and_dest = [('/kaggle/input/dataset-wow/train_image_text.tsv','/kaggle/working/train_image_text.json'),\n                    ('/kaggle/input/dataset-wow/test_image_text.tsv','/kaggle/working/test_image_text.json')]\n\n\nfor file in file_and_dest: \n    data = []\n\n    with open(file[0], newline='', encoding='utf-8') as tsvfile:\n        reader = csv.DictReader(tsvfile, delimiter='\\t')\n        \n        for row in reader:\n            data.append(row)\n                \n    if not os.path.exists(file[1]):\n        with open(file[1], 'w', encoding='utf-8') as jsonfile:\n            json.dump([], jsonfile, ensure_ascii=False, indent=4)\n        print(f\"File JSON vuoto creato come {file[1]}\")\n\n        with open(file[1], 'w', encoding='utf-8') as jsonfile:\n            json.dump(data, jsonfile, ensure_ascii=False, indent=4)\n\n        print(f\"File JSON salvato come {file[1]}\")\n\n","metadata":{"execution":{"iopub.status.busy":"2024-07-17T08:32:12.628335Z","iopub.execute_input":"2024-07-17T08:32:12.629109Z","iopub.status.idle":"2024-07-17T08:32:12.686788Z","shell.execute_reply.started":"2024-07-17T08:32:12.629078Z","shell.execute_reply":"2024-07-17T08:32:12.686061Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"'''\nimport csv\nimport json\nimport os\n\nimages_path = \"/kaggle/input/dataset-wow/MAMI DATASET/MAMI DATASET/training/TRAINING\"\n\nfile_source = '/kaggle/input/dataset-wow/train_image_text.tsv'\nfile_train = '/kaggle/working/train_image_text.json'\nfile_test = '/kaggle/working/test_image_text.json'\ndata = []\n\nwith open(file_source, newline='', encoding='utf-8') as tsvfile:\n    reader = csv.DictReader(tsvfile, delimiter='\\t')\n    \n\n    for row in reader:\n        file_path = os.path.join(images_path, row[\"file_name\"])\n\n        if os.path.exists(file_path):\n            data.append(row)\n    \n    split = int(len(data)*0.8)\n\n    with open(file_train, 'w', encoding='utf-8') as jsonfile:\n        json.dump([], jsonfile, ensure_ascii=False, indent=4)\n    print(f\"File JSON vuoto creato come {file_train}\")\n\n    with open(file_test, 'w', encoding='utf-8') as jsonfile:\n        json.dump([], jsonfile, ensure_ascii=False, indent=4)\n    print(f\"File JSON vuoto creato come {file_test}\")\n    \n    with open(file_train, 'w', encoding='utf-8') as jsonfile:\n        json.dump(data[:split], jsonfile, ensure_ascii=False, indent=4)\n    print(f\"File JSON salvato come {file_train}\")\n        \n    with open(file_test, 'w', encoding='utf-8') as jsonfile:\n        json.dump(data[split:], jsonfile, ensure_ascii=False, indent=4)\n    print(f\"File JSON salvato come {file_test}\")\n'''","metadata":{"execution":{"iopub.status.busy":"2024-07-17T07:52:07.335311Z","iopub.execute_input":"2024-07-17T07:52:07.336185Z","iopub.status.idle":"2024-07-17T07:52:07.344347Z","shell.execute_reply.started":"2024-07-17T07:52:07.336151Z","shell.execute_reply":"2024-07-17T07:52:07.343431Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"'\\nimport csv\\nimport json\\nimport os\\n\\nimages_path = \"/kaggle/input/dataset-wow/MAMI DATASET/MAMI DATASET/training/TRAINING\"\\n\\nfile_source = \\'/kaggle/input/dataset-wow/train_image_text.tsv\\'\\nfile_train = \\'/kaggle/working/train_image_text.json\\'\\nfile_test = \\'/kaggle/working/test_image_text.json\\'\\ndata = []\\n\\nwith open(file_source, newline=\\'\\', encoding=\\'utf-8\\') as tsvfile:\\n    reader = csv.DictReader(tsvfile, delimiter=\\'\\t\\')\\n    \\n\\n    for row in reader:\\n        file_path = os.path.join(images_path, row[\"file_name\"])\\n\\n        if os.path.exists(file_path):\\n            data.append(row)\\n    \\n    split = int(len(data)*0.8)\\n\\n    with open(file_train, \\'w\\', encoding=\\'utf-8\\') as jsonfile:\\n        json.dump([], jsonfile, ensure_ascii=False, indent=4)\\n    print(f\"File JSON vuoto creato come {file_train}\")\\n\\n    with open(file_test, \\'w\\', encoding=\\'utf-8\\') as jsonfile:\\n        json.dump([], jsonfile, ensure_ascii=False, indent=4)\\n    print(f\"File JSON vuoto creato come {file_test}\")\\n    \\n    with open(file_train, \\'w\\', encoding=\\'utf-8\\') as jsonfile:\\n        json.dump(data[:split], jsonfile, ensure_ascii=False, indent=4)\\n    print(f\"File JSON salvato come {file_train}\")\\n        \\n    with open(file_test, \\'w\\', encoding=\\'utf-8\\') as jsonfile:\\n        json.dump(data[split:], jsonfile, ensure_ascii=False, indent=4)\\n    print(f\"File JSON salvato come {file_test}\")\\n'"},"metadata":{}}]},{"cell_type":"markdown","source":"## Multimodal Dataset definition","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import Dataset\nimport os\nimport wandb\n\nclass MultimodalDataset(Dataset): # Dataset for handling multimodal data\n    def __init__(self, images_dir, json_file_path): # dir_path -> directory path where images are stored / json_file_path -> file path for metadata (including labels)   \n        file_paths, text_list, labels_misogyny, shaming_label_list, stereotype_label_list, objectification_label_list, violence_label_list = load_json_file(json_file_path)\n   \n        self.file_paths = file_paths\n        self.images_dir = images_dir\n        self.text_list = text_list\n        self.labels_misogyny = labels_misogyny\n        self.shaming_label_list = shaming_label_list\n        self.stereotype_label_list = stereotype_label_list\n        self.objectification_label_list = objectification_label_list\n        self.violence_label_list = violence_label_list\n        \n    def __len__(self):\n        return len(self.file_paths)\n    \n    def __getitem__(self, idx):\n        return self.file_paths[idx], self.text_list[idx], self.labels_misogyny[idx], self.shaming_label_list[idx], self.stereotype_label_list[idx], self.objectification_label_list[idx], self.violence_label_list[idx]\n    \ndef load_json_file(json_file_path):\n    with open(json_file_path,\"r\") as f:\n        data = json.load(f)\n        text_list = [] # list of the text related to each image\n        image_list = [] # list of images path\n        labels_misogyny = [] # list of TASK A labels (misogyny classification)\n\n        ### list of TASK B labels ###\n        shaming_label_list = [] \n        stereotype_label_list = []\n        objectification_label_list = []\n        violence_label_list = []\n\n\n        for item in tqdm(data): \n            image_list.append(item['file_name'])\n            text_list.append(item[\"text\"])\n            labels_misogyny.append(float(item[\"label\"]))\n            shaming_label_list.append(float(item[\"shaming\"]))\n            stereotype_label_list.append(float(item[\"stereotype\"]))\n            objectification_label_list.append(float(item[\"objectification\"]))\n            violence_label_list.append(float(item[\"violence\"]))\n\n        #print(f\"{type(labels_misogyny)}\")\n        return image_list, text_list, torch.tensor(labels_misogyny, dtype=torch.float32), torch.tensor(shaming_label_list, dtype=torch.float32), torch.tensor(stereotype_label_list, dtype=torch.float32), torch.tensor(objectification_label_list, dtype=torch.float32), torch.tensor(violence_label_list, dtype=torch.float32)\n    \ndef accuracy(preds, labels, thresh):\n    num_samples = labels.shape[0]\n    preds = preds > thresh\n    matching_rows = torch.eq(labels.bool(), preds)\n    \n    # in case we're dealing with the prediction of task B/task A (they've different number of dimensions)\n    num_correct = matching_rows.all(dim=1).sum().item() if preds.ndim!=1 else matching_rows.sum().item()\n    return 100*(num_correct/num_samples)","metadata":{"execution":{"iopub.status.busy":"2024-07-17T08:32:14.229514Z","iopub.execute_input":"2024-07-17T08:32:14.229839Z","iopub.status.idle":"2024-07-17T08:32:14.244362Z","shell.execute_reply.started":"2024-07-17T08:32:14.229816Z","shell.execute_reply":"2024-07-17T08:32:14.243336Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"markdown","source":"## LOADING THE DATASET AND TRAINING THE NETWORK","metadata":{}},{"cell_type":"code","source":"### import torch\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import CLIPProcessor, CLIPModel, CLIPTokenizer\nfrom PIL import Image # per debug\nimport os\nimport json\nfrom tqdm import tqdm\nfrom PIL import Image\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport wandb\n\n\n\nclass Trainer():\n    def __init__(self, \n                model,     \n                train_images_dir,\n                test_image_dir,\n                json_train_path,\n                json_test_path,\n                single_task_mode=True,\n                train_data_split=0.8,\n                batch_size=256, \n                lr=0.001, \n                num_epochs=15,\n                threshold=0.5,\n                weight_taskA=0.7,\n                weight_taskB=0.3):\n        \n        # Check if CUDA is available\n        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n        print(f\"Training on: {self.device}\")\n        \n        # Loading the Dataset\n        self.train_images_dir = train_images_dir\n        self.test_image_dir = test_image_dir\n        self.num_epochs = num_epochs\n        self.threshold = threshold\n        self.weight_taskA = weight_taskA\n        self.single_task_mode=single_task_mode\n        if not self.single_task_mode:\n            self.weight_taskB = weight_taskB\n        \n        train_data = MultimodalDataset(train_images_dir, json_train_path)\n        test_data = MultimodalDataset(test_image_dir, json_test_path)\n        \n        print(f\"training on samples:{test_data.__len__()}\")\n        print(f\"testing on samples:{test_data.__len__()}\")\n        if not self.single_task_mode:\n            print(f\"training model in single task mode: Misoginy Identification\")\n        else:\n            print(f\"training model in dual task mode: Misoginy Identification and Misoginy Classification\")\n\n    \n        self.train_dataloader = DataLoader(train_data, batch_size, shuffle=True, pin_memory=True)\n        self.test_dataloader = DataLoader(test_data, batch_size, shuffle=True, pin_memory=True)\n\n        # Defining the Model\n        self.classifier = model.to(self.device)\n        self.optimizer = optim.Adam(self.classifier.parameters(), lr)\n        self.loss_taskA = F.binary_cross_entropy \n        if not self.single_task_mode:\n            self.loss_taskB = F.binary_cross_entropy\n\n        # Pretrained CLIP loading...\n        self.clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\", device_map='cuda')\n        self.clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n        \n        \n    def get_random_test_sample:\n        if not self.single_task_mode:\n            batch = next(iter(test_loader))\n            _, ,_,_,_,_,_ = batch\n            return images\n        \n    def train_model(self):\n        for epoch in range(self.num_epochs):\n            print(f'Epoch [{epoch+1}/{self.num_epochs}]')\n            if not self.single_task_mode:\n                train_loss_list, test_loss_list, train_acc_taskA_list, train_acc_taskB_list,  test_acc_taskA_list, test_acc_taskB_list= self.train_epoch()\n            else:\n                train_loss_list, test_loss_list, train_acc_taskA_list, test_acc_taskA_list= self.train_epoch()\n            print(f'Average Train Loss: {sum(train_loss_list) / len(train_loss_list): .4f}, Average Test Loss: {sum(test_loss_list) / len(test_loss_list): .4f}')\n            print(f'Average Accuracy Train (task A): {sum(train_acc_taskA_list) / len(train_acc_taskA_list): .4f}%, Average Accuracy Test (task A): {sum(test_acc_taskA_list) / len(test_acc_taskA_list): .4f}%')\n            if not self.single_task_mode:\n                print(f'Average Accuracy Train (task B): {sum(train_acc_taskB_list) / len(train_acc_taskB_list): .4f}%, Average Accuracy Test (task B): {sum(test_acc_taskB_list) / len(test_acc_taskB_list): .4f}%')\n\n            model_path = '/kaggle/working/model_' + str(epoch+1) + '.pth'\n            torch.save(self.classifier.state_dict(), model_path);\n            \n            \n\n    def train_epoch(self):\n        train_loss_list = []\n        test_loss_list = []\n        train_acc_taskA_list = []\n        train_acc_taskB_list = []\n        test_acc_taskA_list = []\n        test_acc_taskB_list = []\n        \n        for batch in tqdm(self.train_dataloader):\n            self.optimizer.zero_grad() # ZEROING OUT THE GRADIENTS\n            self.classifier.train() # TRAINING MODE\n            \n            # CREATING THE CLIP EMBEDDINGS\n            image_list, text_list, labels_misogyny, shaming_labels, stereotype_labels, objectification_labels, violence_labels = batch\n            \n            image_list = [Image.open(f\"{os.path.join(self.train_images_dir, img)}\") for img in image_list] # per poterlo usare poi con CLIP\n            labels_misogyny = labels_misogyny.to(self.device)\n            labels_taskB = torch.stack([shaming_labels, stereotype_labels, objectification_labels, violence_labels],  dim=1).to(self.device)\n\n            clip_inputs = self.clip_processor(text=text_list, images=image_list, return_tensors=\"pt\", padding=True, truncation=True)\n            clip_inputs['input_ids'] = clip_inputs['input_ids'].to(self.device)\n            clip_inputs['attention_mask'] = clip_inputs['attention_mask'].to(self.device)\n            clip_inputs['pixel_values'] = clip_inputs['pixel_values'].to(self.device)\n    \n            clip_outputs = self.clip_model(**clip_inputs)\n            model_input = torch.cat([clip_outputs['text_embeds'], clip_outputs['image_embeds']], dim=1).to(self.device)\n    \n    \n            if not self.single_task_mode:\n                pred_taskA, pred_taskB = self.classifier(model_input)\n            else:\n                pred_taskA = self.classifier(model_input)\n\n            loss_A = self.loss_taskA(pred_taskA, labels_misogyny)\n            if not self.single_task_mode:\n                loss_B = self.loss_taskB(pred_taskB, labels_taskB, reduction='mean')\n                loss = (self.weight_taskA * loss_A) + (self.weight_taskB * loss_B)\n            else:\n                loss= loss_A\n            train_loss_list.append(loss)\n            \n            loss.backward()\n            self.optimizer.step()\n\n            accuracy_taskA = accuracy(pred_taskA, labels_misogyny, self.threshold)\n            if not self.single_task_mode:\n                accuracy_taskB = accuracy(pred_taskB, labels_taskB, self.threshold)\n            \n            train_acc_taskA_list.append(accuracy_taskA)\n            if not self.single_task_mode:\n                train_acc_taskB_list.append(accuracy_taskB)\n        \n        \n        with torch.no_grad():\n            self.classifier.eval()\n\n            for batch in tqdm(self.test_dataloader):\n                # CREATING THE CLIP EMBEDDINGS\n                image_list, text_list, labels_misogyny, shaming_labels, stereotype_labels, objectification_labels, violence_labels = batch\n\n                image_list = [Image.open(f\"{os.path.join(self.test_image_dir, img)}\") for img in image_list] # per poterlo usare poi con CLIP\n                labels_misogyny = labels_misogyny.to(self.device)\n                labels_taskB = torch.stack([shaming_labels, stereotype_labels, objectification_labels, violence_labels],  dim=1).to(self.device)\n\n                clip_inputs = self.clip_processor(text=text_list, images=image_list, return_tensors=\"pt\", padding=True, truncation=True)\n                clip_inputs['input_ids'] = clip_inputs['input_ids'].to(self.device)\n                clip_inputs['attention_mask'] = clip_inputs['attention_mask'].to(self.device)\n                clip_inputs['pixel_values'] = clip_inputs['pixel_values'].to(self.device)\n\n                clip_outputs = self.clip_model(**clip_inputs)\n                model_input = torch.cat([clip_outputs['text_embeds'], clip_outputs['image_embeds']], dim=1).to(self.device)\n                \n                if not self.single_task_mode:\n                    pred_taskA, pred_taskB = self.classifier(model_input)\n                else:\n                    pred_taskA = self.classifier(model_input)\n\n                loss_A = self.loss_taskA(pred_taskA, labels_misogyny)\n                if not self.single_task_mode:\n                    loss_B = self.loss_taskB(pred_taskB, labels_taskB, reduction='mean')\n                    loss = (self.weight_taskA * loss_A) + (self.weight_taskB * loss_B)\n                else:\n                    loss=loss_A\n                test_loss_list.append(loss)\n\n                accuracy_taskA = accuracy(pred_taskA, labels_misogyny, self.threshold)\n                if not self.single_task_mode:\n                    accuracy_taskB = accuracy(pred_taskB, labels_taskB, self.threshold)\n\n                test_acc_taskA_list.append(accuracy_taskA)\n                \n                if not self.single_task_mode:\n                    test_acc_taskB_list.append(accuracy_taskB)\n\n        if not self.single_task_mode:\n            return train_loss_list, test_loss_list, train_acc_taskA_list, train_acc_taskB_list, test_acc_taskA_list, test_acc_taskB_list\n        else:\n            return train_loss_list, test_loss_list, train_acc_taskA_list,  test_acc_taskA_list, \n            ","metadata":{"execution":{"iopub.status.busy":"2024-07-17T09:51:47.597903Z","iopub.execute_input":"2024-07-17T09:51:47.598546Z","iopub.status.idle":"2024-07-17T09:51:47.631681Z","shell.execute_reply.started":"2024-07-17T09:51:47.598511Z","shell.execute_reply":"2024-07-17T09:51:47.630431Z"},"trusted":true},"execution_count":60,"outputs":[{"traceback":["\u001b[0;36m  Cell \u001b[0;32mIn[60], line 73\u001b[0;36m\u001b[0m\n\u001b[0;31m    def get_random_test_sample:\u001b[0m\n\u001b[0m                              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"],"ename":"SyntaxError","evalue":"invalid syntax (2901871143.py, line 73)","output_type":"error"}]},{"cell_type":"code","source":"model = MisogynyCls(5)","metadata":{"execution":{"iopub.status.busy":"2024-07-17T09:43:25.168810Z","iopub.execute_input":"2024-07-17T09:43:25.169644Z","iopub.status.idle":"2024-07-17T09:43:25.188381Z","shell.execute_reply.started":"2024-07-17T09:43:25.169611Z","shell.execute_reply":"2024-07-17T09:43:25.187624Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"\nmodel_trainer = Trainer(model, # model \n                        \"/kaggle/input/dataset-wow/MAMI DATASET/MAMI DATASET/training/TRAINING\",# train_images_dir\n                        \"/kaggle/input/dataset-wow/MAMI DATASET/MAMI DATASET/test\", #test_images_dir\n                        \"/kaggle/working/train_image_text.json\",\n                        \"/kaggle/working/test_image_text.json\",\n                        batch_size=64,\n                        weight_taskA=1,\n                        weight_taskB=1) # json_file as data source\n\n\nmodel_trainer.train_model()","metadata":{"execution":{"iopub.status.busy":"2024-07-17T09:43:27.929605Z","iopub.execute_input":"2024-07-17T09:43:27.930449Z","iopub.status.idle":"2024-07-17T09:47:33.223602Z","shell.execute_reply.started":"2024-07-17T09:43:27.930417Z","shell.execute_reply":"2024-07-17T09:47:33.222361Z"},"trusted":true},"execution_count":59,"outputs":[{"name":"stdout","text":"Training on: cuda:0\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 9000/9000 [00:00<00:00, 459890.55it/s]\n100%|██████████| 1000/1000 [00:00<00:00, 401791.74it/s]","output_type":"stream"},{"name":"stdout","text":"training on samples:1000\ntesting on samples:1000\ntraining model in single task mode: Misoginy Identification\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/15]\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 141/141 [03:46<00:00,  1.61s/it]\n100%|██████████| 16/16 [00:16<00:00,  1.01s/it]","output_type":"stream"},{"name":"stdout","text":"Average Train Loss:  0.4132, Average Test Loss:  0.7005\nAverage Accuracy Train (task A):  80.6206%, Average Accuracy Test (task A):  70.9961%\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[59], line 11\u001b[0m\n\u001b[1;32m      1\u001b[0m model_trainer \u001b[38;5;241m=\u001b[39m Trainer(model, \u001b[38;5;66;03m# model \u001b[39;00m\n\u001b[1;32m      2\u001b[0m                         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/kaggle/input/dataset-wow/MAMI DATASET/MAMI DATASET/training/TRAINING\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;66;03m# train_images_dir\u001b[39;00m\n\u001b[1;32m      3\u001b[0m                         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/kaggle/input/dataset-wow/MAMI DATASET/MAMI DATASET/test\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;66;03m#test_images_dir\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      7\u001b[0m                         weight_taskA\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m      8\u001b[0m                         weight_taskB\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# json_file as data source\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[43mmodel_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[51], line 77\u001b[0m, in \u001b[0;36mTrainer.train_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAverage Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msum\u001b[39m(train_loss_list)\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mlen\u001b[39m(train_loss_list)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m .4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Average Test Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msum\u001b[39m(test_loss_list)\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mlen\u001b[39m(test_loss_list)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m .4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAverage Accuracy Train (task A): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msum\u001b[39m(train_acc_taskA_list)\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mlen\u001b[39m(train_acc_taskA_list)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m .4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%, Average Accuracy Test (task A): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msum\u001b[39m(test_acc_taskA_list)\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mlen\u001b[39m(test_acc_taskA_list)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m .4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 77\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAverage Accuracy Train (task B): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msum\u001b[39m(\u001b[43mtrain_acc_taskB_list\u001b[49m)\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mlen\u001b[39m(train_acc_taskB_list)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m .4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%, Average Accuracy Test (task B): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msum\u001b[39m(test_acc_taskB_list)\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mlen\u001b[39m(test_acc_taskB_list)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m .4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     79\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/kaggle/working/model_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(epoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.pth\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     80\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier\u001b[38;5;241m.\u001b[39mstate_dict(), model_path)\n","\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'train_acc_taskB_list' referenced before assignment"],"ename":"UnboundLocalError","evalue":"local variable 'train_acc_taskB_list' referenced before assignment","output_type":"error"}]},{"cell_type":"markdown","source":"Shap Single Modality","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
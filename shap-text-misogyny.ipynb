{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8853110,"sourceType":"datasetVersion","datasetId":5321593},{"sourceId":8982809,"sourceType":"datasetVersion","datasetId":5409488}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install shap\n!pip install torch\n!pip install transformers","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom PIL import Image\nfrom transformers import CLIPProcessor, CLIPModel, CLIPTokenizer\n\nclass MisogynyCls(nn.Module):\n    def __init__(self, num_linear_layers, task_a_out=1, task_b_out=4, input_dim=1024, hidden_dim=512, drop_value=0.2):\n        super().__init__()\n        self.head_task_a = nn.Linear(hidden_dim, task_a_out)\n        self.head_task_b = nn.Linear(hidden_dim, task_b_out)\n        self.sigmoid = nn.Sigmoid()\n        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # Check if CUDA is available\n        \n        # Pretrained CLIP loading...\n        self.clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n        self.clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n\n\n        self.layers = nn.ModuleList()\n\n        for i in range(num_linear_layers):\n            if i == 0:\n                self.layers.append(nn.Linear(input_dim, hidden_dim))\n            else:\n                self.layers.append(nn.Linear(hidden_dim, hidden_dim))\n                \n            self.layers.append(nn.BatchNorm1d(hidden_dim))\n            self.layers.append(nn.Dropout(drop_value))\n            self.layers.append(nn.ReLU())\n\n    def forward(self, text_list, image_list):\n        clip_inputs = self.clip_processor(text=text_list, images=image_list, return_tensors=\"pt\", padding=True, truncation=True)\n        clip_outputs = self.clip_model(**clip_inputs)\n        \n        x = torch.cat([clip_outputs['text_embeds'], clip_outputs['image_embeds']], dim=1).to(self.device) # model input is the concatenation of the two modalities !\n        \n        for layer in self.layers:\n            x = layer(x)\n            #print(x.shape)\n        pred_taskA = self.sigmoid(self.head_task_a(x))\n        pred_taskB = self.sigmoid(self.head_task_b(x))\n        \n        return pred_taskA, pred_taskB\n\nclass OnlyTextCls(nn.Module):\n    def __init__(self, cls):\n        super().__init__() # da aggiustare\n        self.classifier = cls\n    \n    def forward(self, text_list):\n        text_list = [el.item() for el in text_list]\n        null_images = [Image.new('RGB', (100, 100), color=(0, 0, 0)) for _ in range(len(text_list))]\n        prediction, _ = self.classifier(text_list, null_images) # for now we return only the prediction about the main task (binary one)\n        print(prediction)\n    \n        return prediction\n    \n    \nclass OnlyImageCls(nn.Module):\n    def __init__(self, cls):\n        super().__init__() # da aggiustare\n        self.classifier = cls\n    \n    def forward(self, image_list):\n        null_text = [\" \"  for _ in range(len(image_list))] # Da cambiare e parametrizzare il token nullo ('[UNK]')\n        prediction, _ = self.classifier(null_text, image_list) # for now we return only the prediction about the main task (binary one)\n        print(prediction)\n        return prediction","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Explaining text\n","metadata":{}},{"cell_type":"code","source":"from transformers import CLIPProcessor, CLIPModel, CLIPTokenizer, AutoTokenizer\nimport shap\nimport numpy as np\n\n#### IMPLEMENT ALSO PUNCTUATION PREPROCESSING OVER THE INPUT PROMPT FOR THE MODEL !!! ####\n\n#clip_tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n\n\n\nmask_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n\n# 3. Create an instance of the model and load the state dictionary\ncheckpoint = torch.load('/kaggle/input/model-params/model_3.pth', map_location=torch.device('cpu'))\nclassifier = MisogynyCls(5)\nclassifier.load_state_dict(checkpoint)\nonly_text_classifier = OnlyTextCls(classifier)\n\nclassifier.eval()\n\n### Testing the masker for the explainer ###\nclip_masker = shap.maskers.Text(mask_tokenizer, mask_token=\"...\", collapse_mask_token=False) # cambiare mask token\nexplainer = shap.Explainer(only_text_classifier, masker=clip_masker)\n\nsample_text = [\"crazy slut bitch yoo shut up\"]\n# Compute SHAP values\nshap_values = explainer(sample_text) # dentro l'explainer l'input verrÃ  \n\nprint(shap_values)\n#shap.plots.waterfall(shap_values[0], max_display=14)\nshap.plots.text(shap_values[0])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Loading json file","metadata":{}},{"cell_type":"code","source":"import csv\nimport json\nimport os\n\nimages_path = \"/kaggle/input/dataset-wow/MAMI DATASET/MAMI DATASET/training/TRAINING\"\n\nfile_and_dest = [('/kaggle/input/dataset-wow/train_image_text.tsv','/kaggle/working/train_image_text.json'),\n                    ('/kaggle/input/dataset-wow/test_image_text.tsv','/kaggle/working/test_image_text.json')]\n\n\nfor file in file_and_dest: \n    data = []\n\n    with open(file[0], newline='', encoding='utf-8') as tsvfile:\n        reader = csv.DictReader(tsvfile, delimiter='\\t')\n        \n        for row in reader:\n            data.append(row)\n                \n    if not os.path.exists(file[1]):\n        with open(file[1], 'w', encoding='utf-8') as jsonfile:\n            json.dump([], jsonfile, ensure_ascii=False, indent=4)\n        print(f\"File JSON vuoto creato come {file[1]}\")\n\n        with open(file[1], 'w', encoding='utf-8') as jsonfile:\n            json.dump(data, jsonfile, ensure_ascii=False, indent=4)\n\n        print(f\"File JSON salvato come {file[1]}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Multimodal Dataset","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import Dataset\nimport os\nfrom tqdm import tqdm\nimport wandb\nimport json\n\nclass MultimodalDataset(Dataset): # Dataset for handling multimodal data\n    def __init__(self, images_dir, json_file_path): # dir_path -> directory path where images are stored / json_file_path -> file path for metadata (including labels)   \n        file_paths, text_list, labels_misogyny, shaming_label_list, stereotype_label_list, objectification_label_list, violence_label_list = load_json_file(json_file_path)\n   \n        self.file_paths = file_paths\n        self.images_dir = images_dir\n        self.text_list = text_list\n        self.labels_misogyny = labels_misogyny\n        self.shaming_label_list = shaming_label_list\n        self.stereotype_label_list = stereotype_label_list\n        self.objectification_label_list = objectification_label_list\n        self.violence_label_list = violence_label_list\n        \n    def __len__(self):\n        return len(self.file_paths)\n    \n    def __getitem__(self, idx):\n        return self.file_paths[idx], self.text_list[idx], self.labels_misogyny[idx], self.shaming_label_list[idx], self.stereotype_label_list[idx], self.objectification_label_list[idx], self.violence_label_list[idx]\n    \ndef load_json_file(json_file_path):\n    with open(json_file_path,\"r\") as f:\n        data = json.load(f)\n        text_list = [] # list of the text related to each image\n        image_list = [] # list of images path\n        labels_misogyny = [] # list of TASK A labels (misogyny classification)\n\n        ### list of TASK B labels ###\n        shaming_label_list = [] \n        stereotype_label_list = []\n        objectification_label_list = []\n        violence_label_list = []\n\n\n        for item in tqdm(data): \n            image_list.append(item['file_name'])\n            text_list.append(item[\"text\"])\n            labels_misogyny.append(float(item[\"label\"]))\n            shaming_label_list.append(float(item[\"shaming\"]))\n            stereotype_label_list.append(float(item[\"stereotype\"]))\n            objectification_label_list.append(float(item[\"objectification\"]))\n            violence_label_list.append(float(item[\"violence\"]))\n\n        #print(f\"{type(labels_misogyny)}\")                                \n        return image_list, text_list, torch.tensor(labels_misogyny, dtype=torch.float32), torch.tensor(shaming_label_list, dtype=torch.float32), torch.tensor(stereotype_label_list, dtype=torch.float32), torch.tensor(objectification_label_list, dtype=torch.float32), torch.tensor(violence_label_list, dtype=torch.float32)\n    \ndef accuracy(preds, labels, thresh):\n    num_samples = labels.shape[0]\n    preds = preds > thresh\n    matching_rows = torch.eq(labels.bool(), preds)\n    \n    # in case we're dealing with the prediction of task B/task A (they've different number of dimensions)\n    num_correct = matching_rows.all(dim=1).sum().item() if preds.ndim!=1 else matching_rows.sum().item()\n    return 100*(num_correct/num_samples)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Explaining Image","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader\nfrom torchvision.transforms import ToTensor\nfrom PIL import Image\nimport os\nfrom tqdm import tqdm\n\n\ntrain_data = MultimodalDataset(\"/kaggle/input/dataset-wow/MAMI DATASET/MAMI DATASET/training/TRAINING\", \"/kaggle/working/train_image_text.json\")\ntest_data = MultimodalDataset(\"/kaggle/input/dataset-wow/MAMI DATASET/MAMI DATASET/test\", \"/kaggle/working/test_image_text.json\")\ntrain_dataloader = DataLoader(train_data, 100, shuffle=True, pin_memory=True)\ntest_dataloader = DataLoader(test_data, 16, shuffle=True, pin_memory=True)\n\nbatch_train, _, _, _, _, _, _ = next(iter(train_dataloader))\nbatch_test, _, _, _, _, _, _  = next(iter(test_dataloader))\n\nbatch_train = [ToTensor()(Image.open(f\"{os.path.join('/kaggle/input/dataset-wow/MAMI DATASET/MAMI DATASET/training/TRAINING', img)}\")) for img in batch_train]\nbatch_test = [ToTensor()(Image.open(f\"{os.path.join('/kaggle/input/dataset-wow/MAMI DATASET/MAMI DATASET/test', img)}\")) for img in batch_test]\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import shap\nimport torch\nfrom torchvision import transforms\nimport numpy as np\n\nbatch_size = 50\nn_evals = 100\nresize = transforms.Resize((440, 440))\n\ncheckpoint = torch.load('/kaggle/input/model-params/model_3.pth', map_location=torch.device('cpu'))\nclassifier = MisogynyCls(5)\nclassifier.load_state_dict(checkpoint)\nonly_image_classifier = OnlyImageCls(classifier)\nclassifier.eval()\n\ndata = [resize(img) for img in batch_train]\n\ndata_to_test = data[8].permute(1, 2, 0)\nprint(data_to_test.shape)\n\nmasker_blur = shap.maskers.Image(\"blur(128,128)\", data_to_test[0].shape)\nexplainer = shap.Explainer(only_image_classifier, masker_blur)\n\nshap_values = explainer(\n    data_to_test.unsqueeze(0),\n    max_evals=n_evals,\n    batch_size=batch_size,\n)\n\nprint(shap_values.data.shape)\nprint(shap_values.values.shape)\n\nprint(shap_values.values.shape)\n\nshap.image_plot(\n    shap_values=shap_values.values.squeeze(4),\n    pixel_values=shap_values.data.numpy(),\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}